
=== ./baseline_flat.py ===

#!/usr/bin/env python3
import argparse, tempfile, os, sys, subprocess

ap = argparse.ArgumentParser()
ap.add_argument("--states", default="states.csv")
ap.add_argument("--H", type=int, default=40)
ap.add_argument("--fee-bps", type=float, default=5.0)
ap.add_argument("--hold-bars", type=int, default=8)
args = ap.parse_args()

tmp = tempfile.NamedTemporaryFile("w", delete=False, suffix=".csv")
tmp.write("row_idx,agree,action,details\n")
with open(args.states) as f:
    n = sum(1 for _ in f) - 1
for i in range(n):
    tmp.write(f"{i},1,0.0,{{}}\n")
tmp.close()

cmd = [sys.executable, "backtest_gate_horizon_v3.py",
       "--states", args.states, "--gate", tmp.name,
       "--h", str(args.H), "--mode", "sign",
       "--action_thresh", "0.05",
       "--hold_bars", str(args.hold_bars),
       "--fee_bps", str(args.fee_bps)]
out = subprocess.check_output(cmd, text=True)
print(out)
os.unlink(tmp.name)

=== ./baseline_random_sign_many.py ===

#!/usr/bin/env python3
import argparse, subprocess, statistics, re

ap = argparse.ArgumentParser()
ap.add_argument("--runs", type=int, default=50)
ap.add_argument("--states", default="states.csv")
ap.add_argument("--H", type=int, default=40)
ap.add_argument("--hold_bars", type=int, default=8)
ap.add_argument("--fee_bps", type=float, default=5.0)
args = ap.parse_args()

equities, sharpes, trades = [], [], []

for i in range(args.runs):
    cmd = [
        "./baseline_random_sign.py",
        "--states", args.states,
        "--H", str(args.H),
        "--hold-bars", str(args.hold_bars),
        "--fee-bps", str(args.fee_bps),
    ]
    out = subprocess.check_output(cmd, text=True, stderr=subprocess.STDOUT)

    m_eq = re.search(r"equity_end=([0-9.]+)", out)
    m_sh = re.search(r"Sharpe_like=([\-0-9.]+)", out)
    m_tr = re.search(r"trades=(\d+)", out)
    if m_eq and m_sh and m_tr:
        equities.append(float(m_eq.group(1)))
        sharpes.append(float(m_sh.group(1)))
        trades.append(int(m_tr.group(1)))

def q(xs, p):
    if not xs: return float("nan")
    s = sorted(xs)
    i = max(0, min(len(s)-1, int(p*(len(s)-1))))
    return s[i]

def mean(xs): return (sum(xs)/len(xs)) if xs else float("nan")

print(f"[random baseline x{args.runs}]")
print(f"trades avg/med: {mean(trades):.2f}/{q(trades,0.50):.0f}")
print(
    "equity_end mean={:.4f}  median={:.4f}  p05={:.4f}  p95={:.4f}".format(
        mean(equities), q(equities,0.50), q(equities,0.05), q(equities,0.95)
    )
)
print(
    "Sharpe mean={:.2f}  median={:.2f}  p05={:.2f}  p95={:.2f}".format(
        mean(sharpes), q(sharpes,0.50), q(sharpes,0.05), q(sharpes,0.95)
    )
)

=== ./patch_auto_tune_gate.py ===

import re, sys, pathlib

p = pathlib.Path("auto_tune_gate.py")
src = p.read_text()

# Find the broken footer area and replace with a safe printer.
# We look for the block that mentions "Recommended Params (no live execution)".
pat = re.compile(
    r"(?s)\n\s*#\s*Recommended Params.*?print\(.+?\)\s*$"
)

replacement = r'''
# --- Recommended summary (safe prints) ---
if best is not None:
    gate_csv = best.get('gate_csv','')
    tag      = best.get('tag','')
    win      = best.get('trade_winrate','')
    hit      = best.get('per_bar_hitrate','')
    eq       = best.get('equity_end','')
    sh       = best.get('Sharpe_like','')
    dd       = best.get('MaxDD','')
    hold     = best.get('hold_bars','')
    th       = best.get('action_thresh','')
    ls       = best.get('linear_scale','')
    mode     = args.mode
    sweep_dir = out_dir  # where we wrote results

    print("\\n=== Recommended Params (no live execution) ===")
    print(f"Gate CSV: {gate_csv}")
    print(f"Tag     : {tag}")
    print(f"KPI     : Win {win} | Hit {hit} | Eq {eq} | Sharpe {sh} | MaxDD {dd}")

    # Backtest command
    print("\\n# When you want to backtest this exact gate again:")
    print("python3 backtest_gate_horizon_v3.py \\")
    print(f"  --states {args.states} --gate {sweep_dir}/gates/{gate_csv} \\")
    if th:
        print(f"  --h {args.H} --mode {mode} --action_thresh {th} --hold_bars {hold} --fee_bps {args.fee_bps}")
    else:
        print(f"  --h {args.H} --mode {mode} --linear_scale {ls} --hold_bars {hold} --fee_bps {args.fee_bps}")
else:
    print("[warn] no best row computed; results.csv may be empty.")
'''.strip()

new = pat.sub("\n"+replacement, src)
if new == src:
    print("[patch] pattern not found. No change made (maybe already fixed?).")
else:
    p.write_text(new)
    print("[patch] auto_tune_gate.py footer replaced OK.")

=== ./fix_autotune_footer.py ===

import os, csv, pathlib, sys

p = pathlib.Path("auto_tune_gate.py")
src = p.read_text()

# We'll locate the start of the old footer by an anchor string
anchor = "Recommended Params (no live execution)"
idx = src.find(anchor)

if idx == -1:
    # fall back: chop last ~120 lines and rebuild a footer anyway
    lines = src.splitlines()
    keep = "\n".join(lines[:-120]) if len(lines) > 120 else src
else:
    # keep everything up to the line BEFORE the anchor’s line
    head = src[:idx]
    # also backtrack to the start of that line
    head = head[:head.rfind("\n")] if "\n" in head else head
    keep = head

safe_footer = r'''
# === Recommended Params (no live execution) ===
# We re-read results.csv we just wrote into out_dir and pick a "best" row to print clean commands.

import csv, os

def _num(x, default=0.0):
    try:
        return float(x)
    except: 
        return default

def _pick_best(results_csv):
    if not os.path.exists(results_csv):
        return None
    with open(results_csv, newline='') as f:
        rdr = csv.DictReader(f)
        rows = list(rdr)
    if not rows:
        return None
    # Prefer higher equity_end, then higher Sharpe_like
    rows.sort(key=lambda r: (_num(r.get('equity_end',0)), _num(r.get('Sharpe_like',0))), reverse=True)
    return rows[0]

try:
    results_csv = os.path.join(out_dir, "results.csv")  # out_dir is defined earlier in the script
    best = _pick_best(results_csv)
    if best is None:
        print("[warn] no best row to print (results.csv empty?).")
    else:
        gate_csv = best.get('gate_csv','')
        tag      = best.get('tag','')
        win      = best.get('trade_winrate','')
        hit      = best.get('per_bar_hitrate','')
        eq       = best.get('equity_end','')
        sh       = best.get('Sharpe_like','')
        dd       = best.get('MaxDD','')
        hold     = best.get('hold_bars','')
        th       = best.get('action_thresh','')
        ls       = best.get('linear_scale','')
        mode     = args.mode

        print("\\n=== Recommended Params (no live execution) ===")
        print(f"Gate CSV: {gate_csv}")
        print(f"Tag     : {tag}")
        print(f"KPI     : Win {win} | Hit {hit} | Eq {eq} | Sharpe {sh} | MaxDD {dd}")

        print("\\n# When you want to backtest this exact gate again:")
        print("python3 backtest_gate_horizon_v3.py \\")
        print(f"  --states {args.states} --gate {os.path.join(out_dir,'gates',gate_csv)} \\")
        if th:
            print(f"  --h {args.H} --mode {mode} --action_thresh {th} --hold_bars {hold} --fee_bps {args.fee_bps}")
        else:
            # linear mode case
            print(f"  --h {args.H} --mode {mode} --linear_scale {ls} --hold_bars {hold} --fee_bps {args.fee_bps}")

except Exception as e:
    print(f"[warn] footer print failed: {e}")
'''.lstrip()

p.write_text(keep + "\n" + safe_footer)
print("[patch] auto_tune_gate.py footer replaced OK.")

=== ./rl_duckdb_data_layer.py ===

# rl_duckdb_data_layer.py (v1.5 — reward scaled by 1000x)
# - Timestamp: supports 'ts' or 'timestamp' (TIMESTAMP)
# - L1 fields: bid_px_1 / ask_px_1 / bid_sz_1 / ask_sz_1
# - Features: mid, spread, imbalance, mom1, mom3, vol3
# - Next-state (H bars ahead): mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p
# - Reward: fwd_ret_raw = (mid_p - mid)/mid ; fwd_ret = 1000 * fwd_ret_raw
# - union_by_name=true to tolerate schema drift across days/files

def build_sql(parquet_glob: str,
              start: str | None,
              end: str | None,
              h: int,
              th: float,
              drop_noise: bool) -> str:
    # Optional time filter
    time_filter = ""
    if start and end:
        time_filter = f"WHERE ts >= TIMESTAMP '{start}' AND ts <= TIMESTAMP '{end}'"
    elif start:
        time_filter = f"WHERE ts >= TIMESTAMP '{start}'"
    elif end:
        time_filter = f"WHERE ts <= TIMESTAMP '{end}'"

    return f"""
WITH raw AS (
  SELECT
    COALESCE(TRY_CAST("ts" AS TIMESTAMP), TRY_CAST("timestamp" AS TIMESTAMP)) AS ts,
    CAST("bid_px_1" AS DOUBLE) AS bid_px_1,
    CAST("ask_px_1" AS DOUBLE) AS ask_px_1,
    CAST("bid_sz_1" AS DOUBLE) AS bid_sz_1,
    CAST("ask_sz_1" AS DOUBLE) AS ask_sz_1
  FROM read_parquet('{parquet_glob}', union_by_name=true)
),
lvl AS (
  SELECT *
  FROM raw
  WHERE ts IS NOT NULL
    AND bid_px_1 IS NOT NULL AND ask_px_1 IS NOT NULL
),
feat AS (
  SELECT
    ts,
    (ask_px_1 + bid_px_1)/2.0 AS mid,
    (ask_px_1 - bid_px_1)     AS spread,
    CASE
      WHEN (bid_sz_1 + ask_sz_1) > 0
      THEN (bid_sz_1 - ask_sz_1) / NULLIF(bid_sz_1 + ask_sz_1, 0)
      ELSE NULL
    END AS imbalance
  FROM lvl
),
lags AS (
  SELECT
    ts, mid, spread, imbalance,
    LAG(mid, 1) OVER (ORDER BY ts) AS mid_l1,
    LAG(mid, 3) OVER (ORDER BY ts) AS mid_l3
  FROM feat
),
rets AS (
  SELECT
    ts, mid, spread, imbalance, mid_l1, mid_l3,
    CASE WHEN mid_l1 IS NULL OR mid_l1 = 0 THEN NULL ELSE (mid - mid_l1)/mid_l1 END AS ret1,
    CASE WHEN mid_l3 IS NULL OR mid_l3 = 0 THEN NULL ELSE (mid - mid_l3)/mid_l3 END AS ret3
  FROM lags
),
vol AS (
  SELECT
    ts, mid, spread, imbalance,
    ret1, ret3,
    /* 3-bar rolling stddev of 1-bar returns */
    STDDEV_SAMP(ret1) OVER (ORDER BY ts ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS vol3
  FROM rets
),
lead_h AS (
  SELECT
    ts, mid, spread, imbalance, ret1 AS mom1, ret3 AS mom3, vol3,
    /* H-step lookahead */
    LEAD(mid,        {h}) OVER (ORDER BY ts) AS mid_p,
    LEAD(spread,     {h}) OVER (ORDER BY ts) AS spread_p,
    LEAD(imbalance,  {h}) OVER (ORDER BY ts) AS imbalance_p,
    LEAD(ret1,       {h}) OVER (ORDER BY ts) AS mom1_p,
    LEAD(ret3,       {h}) OVER (ORDER BY ts) AS mom3_p,
    LEAD(vol3,       {h}) OVER (ORDER BY ts) AS vol3_p
  FROM vol
),
aug AS (
  SELECT
    ts, mid, spread, imbalance, mom1, mom3, vol3,
    mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
    CASE WHEN mid IS NULL OR mid = 0 OR mid_p IS NULL THEN NULL
         ELSE (mid_p - mid) / mid
    END AS fwd_ret_raw
  FROM lead_h
),
scale AS (
  SELECT
    ts, mid, spread, imbalance, mom1, mom3, vol3,
    mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
    /* Scale reward to strengthen gradients (does not change optimal policy) */
    1000.0 * fwd_ret_raw AS fwd_ret
  FROM aug
),
flt AS (
  SELECT * FROM scale
  {time_filter}
)
SELECT
  ts, mid, spread, imbalance, mom1, mom3, vol3,
  mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
  fwd_ret
FROM flt
ORDER BY ts
"""

=== ./backtest_gate_horizon_v3.py ===

#!/usr/bin/env python3
# backtest_gate_horizon_v3.py
# - H-step ahead PnL
# - Correct per-trade winrate (segment-based), per-bar hitrate
# - Exposure, turnover, avg trade PnL, slippage/fee model
# - Verbose sanity prints

import argparse, csv, statistics as st, sys
from math import isfinite

def read_states(path):
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        hdr = [h.strip().lower() for h in (r.fieldnames or [])]
        need = ["mid","spread","imbalance","mom1","mom3","vol3"]
        if hdr != need:
            raise SystemExit(f"[error] states header mismatch: got {r.fieldnames}, want {need}")
        mids=[]; ok=0; bad=0
        for row in r:
            try:
                x = float(row["mid"])
                if isfinite(x): mids.append(x); ok+=1
                else: bad+=1
            except: bad+=1
        if ok < 3: raise SystemExit(f"[error] states too short: ok={ok}, bad={bad}")
        return mids, ok, bad

def read_gate(path):
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        if not r.fieldnames: raise SystemExit("[error] gate has no header")
        hdr = [h.strip().lower() for h in r.fieldnames]
        if "agree" not in hdr or "action" not in hdr:
            raise SystemExit(f"[error] gate header missing agree/action: {r.fieldnames}")
        acts=[]; ok=0; bad=0
        for row in r:
            try:
                agree = int(row.get("agree","0"))
                a = float(row.get("action","0"))
                acts.append(a if agree==1 else 0.0); ok+=1
            except: bad+=1
        if ok == 0: raise SystemExit("[error] gate has zero usable rows")
        return acts, ok, bad

def segment_trades(pos):
    """Return trade segments as (t_open, t_close, pos_sign). pos_sign is -1,0,+1."""
    segs=[]
    if not pos: return segs
    cur = pos[0]; t0 = 0
    for t in range(1, len(pos)):
        if pos[t] != cur:
            segs.append((t0, t-1, cur))
            t0 = t; cur = pos[t]
    segs.append((t0, len(pos)-1, cur))
    return segs

def metrics_time_series(equity):
    rets=[equity[i]/equity[i-1]-1.0 for i in range(1,len(equity))]
    if not rets: return {"Sharpe_like": 0.0, "MaxDD": 0.0, "CAGR_like": 0.0, "Ret_mean":0.0, "Ret_std":0.0}
    mu = st.mean(rets)
    sd = st.pstdev(rets) if len(rets)>1 else 0.0
    sharpe = mu/sd if sd>1e-12 else 0.0
    peak = equity[0]; maxdd=0.0
    for x in equity:
        peak = max(peak, x)
        dd = peak/x - 1.0
        maxdd = max(maxdd, dd)
    return {"CAGR_like": equity[-1]/equity[0]-1.0, "Sharpe_like": sharpe, "MaxDD": maxdd, "Ret_mean": mu, "Ret_std": sd}

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--action_thresh", type=float, default=0.08)
    ap.add_argument("--linear_scale", type=float, default=1.0)
    ap.add_argument("--hold_bars", type=int, default=8)
    ap.add_argument("--fee_bps", type=float, default=5.0, help="bps applied on position changes")
    ap.add_argument("--slip_bps", type=float, default=0.0, help="optional slip bps per trade entry/exit")
    ap.add_argument("--verbose", action="store_true")
    args=ap.parse_args()

    mids, s_ok, s_bad = read_states(args.states)
    acts, g_ok, g_bad = read_gate(args.gate)

    n = min(len(acts), len(mids) - args.h)
    if n <= 0:
        print(f"[error] insufficient overlap: len(acts)={len(acts)} len(mids)={len(mids)} H={args.h}", flush=True)
        sys.exit(0)

    # H-step forward returns
    rets = [ (mids[t+args.h]-mids[t]) / mids[t] for t in range(n) ]

    # Intended positions
    raw=[0.0]*n
    if args.mode=="sign":
        th=args.action_thresh
        for t,a in enumerate(acts[:n]):
            raw[t] = 1.0 if a>=th else (-1.0 if a<=-th else 0.0)
    else:
        sc=args.linear_scale
        for t,a in enumerate(acts[:n]):
            raw[t] = max(-1.0, min(1.0, a*sc))

    # Enforce minimum holding
    pos=[0.0]*n
    if n>0:
        pos[0]=raw[0]
        last_change=0
        for t in range(1,n):
            if raw[t] != pos[t-1] and (t - last_change) < args.hold_bars:
                pos[t] = pos[t-1]
            else:
                if raw[t] != pos[t-1]:
                    last_change = t
                pos[t] = raw[t]

    # Fees/slippage
    fee_rate = args.fee_bps/10000.0
    slip_rate = args.slip_bps/10000.0

    # Per-bar PnL and equity
    equity=[1.0]
    exposure_bars = sum(1 for p in pos if abs(p)>0.0)
    turnover = 0.0
    prev_pos = 0.0
    bar_pnl=[]
    for t in range(n):
        trn = abs(pos[t] - prev_pos)
        turnover += trn
        fee = trn * fee_rate
        r = pos[t]*rets[t] - fee
        equity.append(equity[-1]*(1.0 + r))
        bar_pnl.append(r)
        prev_pos = pos[t]

    # Per-trade (segment) stats
    segs = segment_trades(pos)
    trade_pnls=[]; entries=0
    for (lo, hi, sign) in segs:
        if sign == 0.0: continue
        # entry slip + exit slip (charge once per boundary)
        seg_turn = (1.0 if (lo==0 or pos[lo-1]==0.0) else 0.0) + (1.0 if (hi==n-1 or pos[hi+1]==0.0) else 0.0)
        slip = seg_turn * slip_rate
        seg_ret = sum(bar_pnl[lo:hi+1]) - slip
        trade_pnls.append(seg_ret)
        entries += 1

    m = metrics_time_series(equity)
    per_bar_hitrate = sum(1 for r,p in zip(bar_pnl,pos) if p!=0.0 and r>0) / max(1, sum(1 for p in pos if p!=0.0))
    trade_wins = sum(1 for x in trade_pnls if x>0)
    trade_winrate = trade_wins / max(1, len(trade_pnls))
    avg_trade = (st.mean(trade_pnls) if trade_pnls else 0.0)
    med_trade = (st.median(trade_pnls) if trade_pnls else 0.0)
    exposure = exposure_bars / n
    avg_turnover = turnover / n

    if args.verbose:
        print(f"[debug] states ok/bad={s_ok}/{s_bad} gate ok/bad={g_ok}/{g_bad} n={n}", flush=True)
        print(f"[debug] sample acts={ [round(x,4) for x in acts[:5]] } rets={ [round(x,6) for x in rets[:5]] }", flush=True)

    print("[backtest H]", flush=True)
    print(f" rows={n}  trades={len(trade_pnls)}  trade_winrate={trade_winrate:.2%}  per_bar_hitrate={per_bar_hitrate:.2%}", flush=True)
    print(f" H={args.h} hold_bars={args.hold_bars} mode={args.mode} th={args.action_thresh if args.mode=='sign' else None} scale={args.linear_scale if args.mode=='linear' else None}", flush=True)
    print(f" equity_end={equity[-1]:.4f} Sharpe_like={m['Sharpe_like']:.2f} MaxDD={m['MaxDD']:.2%} CAGR_like={m['CAGR_like']:.2%}", flush=True)
    print(f" exposure={exposure:.2%} avg_turnover={avg_turnover:.4f}/bar avg_trade_pnl={avg_trade:.6f} median_trade_pnl={med_trade:.6f}", flush=True)

if __name__=="__main__":
    main()

=== ./backtest_gate_from_states.py ===

#!/usr/bin/env python3
# backtest_gate_from_states.py
# Aligns gate_out.csv with states.csv by row index and simulates PnL on next-step mid returns.
# Position can be "sign" (±1/0 using threshold) or "linear" (position = clipped action).
# Fees charged on position changes (turnover * fee_bps).
#
# Usage:
#   python3 backtest_gate_from_states.py --states states.csv --gate gate_out.csv \
#       --mode sign --action_thresh 0.05 --fee_bps 1.0
#
import argparse, csv, math, statistics as st

def read_states(path):
    mids=[]
    with open(path, newline="") as f:
        r=csv.DictReader(f)
        need=["mid","spread","imbalance","mom1","mom3","vol3"]
        hdr=[h.strip().lower() for h in (r.fieldnames or [])]
        if hdr!=need:
            raise SystemExit(f"states header mismatch: got {r.fieldnames}, want {need}")
        for row in r:
            try: mids.append(float(row["mid"]))
            except: pass
    if len(mids)<3: raise SystemExit("not enough rows in states.csv")
    return mids

def read_gate(path):
    acts=[]
    with open(path, newline="") as f:
        r=csv.DictReader(f)
        for row in r:
            try:
                agree=int(row["agree"])
                action=float(row["action"])
                acts.append(action if agree==1 else 0.0)
            except: pass
    if not acts: raise SystemExit("no rows in gate csv")
    return acts

def metrics(equity):
    rets=[equity[i]/equity[i-1]-1.0 for i in range(1,len(equity))]
    if not rets: return {}
    mean = st.mean(rets)
    sd   = st.pstdev(rets) if len(rets)>1 else 0.0
    sharpe = mean/sd if sd>1e-12 else 0.0
    peak=equity[0]; dd=0.0; maxdd=0.0
    for x in equity:
        peak=max(peak,x); dd=peak/x-1.0; maxdd=max(maxdd,dd)
    return {"CAGR_like": (equity[-1]/equity[0]-1.0), "Sharpe_like": sharpe, "MaxDD": maxdd,
            "Ret_mean": mean, "Ret_std": sd}

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--action_thresh", type=float, default=0.05, help="sign mode: trade if |a|>=th")
    ap.add_argument("--linear_scale", type=float, default=1.0, help="linear mode: pos = clip(a*scale, -1, 1)")
    ap.add_argument("--fee_bps", type=float, default=1.0, help="per-change bps")
    args=ap.parse_args()

    mids=read_states(args.states)
    acts=read_gate(args.gate)
    n=min(len(acts), len(mids)-1)  # need t and t+1
    acts=acts[:n]; mids=mids[:n+1]

    # returns on mid
    rets=[(mids[t+1]-mids[t])/mids[t] for t in range(n)]

    # build positions
    pos=[0.0]*n
    if args.mode=="sign":
        for t,a in enumerate(acts):
            pos[t]= 1.0 if a>=args.action_thresh else (-1.0 if a<=-args.action_thresh else 0.0)
    else:
        for t,a in enumerate(acts):
            v = max(-1.0, min(1.0, a*args.linear_scale))
            pos[t]=v

    # fee on position changes
    fee_rate=args.fee_bps/10000.0
    pnl=[]; equity=[1.0]
    prev_pos=0.0
    wins=0; trades=0
    for t in range(n):
        # turnover fee when position changes
        turn=abs(pos[t]-prev_pos)
        fee = turn*fee_rate
        r   = pos[t]*rets[t] - fee
        pnl.append(r)
        equity.append(equity[-1]*(1.0+r))
        if pos[t]!=0.0:
            trades+=1
            if r>0: wins+=1
        prev_pos=pos[t]

    m=metrics(equity)
    winrate = (wins/trades) if trades>0 else 0.0
    print("[backtest]")
    print(f" rows={n}  trades={trades}  winrate={winrate:.2%}")
    print(f" mode={args.mode}  th={args.action_thresh if args.mode=='sign' else None}  scale={args.linear_scale if args.mode=='linear' else None}  fee_bps={args.fee_bps}")
    print(f" equity_end={equity[-1]:.4f}  Sharpe_like={m.get('Sharpe_like',0):.2f}  MaxDD={m.get('MaxDD',0):.2%}  CAGR_like={m.get('CAGR_like',0):.2%}")

if __name__=="__main__":
    main()

=== ./decide_from_latest.py ===

#!/usr/bin/env python3
# decide_from_latest.py
# Pull the latest state from parquet, normalize with scaler.json, run consensus gate, print action.
import argparse, subprocess, json, numpy as np, sys, os

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        print(p.stderr.strip(), file=sys.stderr)
        sys.exit(p.returncode)
    return p.stdout.strip()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    ap.add_argument("--device", default="cpu")
    args = ap.parse_args()

    # 1) get latest state (single CSV line)
    state_row = run([
        sys.executable, "latest_state_from_parquet.py",
        "--parquet", args.parquet
    ])
    # 2) run live gate with --state_row
    out = run([
        sys.executable, "consensus_gate_live.py",
        "--gate", args.gate,
        "--scaler", args.scaler,
        "--state_row", state_row,
        "--device", args.device
    ])
    # Print only the last line with the decision
    lines = [ln for ln in out.splitlines() if ln and ln[0].isdigit()]
    if not lines:
        print(out)
    else:
        print(lines[-1])

if __name__ == "__main__":
    main()

=== ./iql_agent.py ===

#!/usr/bin/env python3
# iql_agent.py
# Implicit Q-Learning (offline RL): V via expectile regression, twin Q, AWR-style actor

from typing import Dict
import torch
import torch.nn as nn
import torch.nn.functional as F

from rl_utils import MLP, polyak_update

class VNet(nn.Module):
    def __init__(self, s_dim: int, hidden=(128,128)):
        super().__init__()
        self.v = MLP(s_dim, 1, hidden, nn.ReLU, None)
    def forward(self, s):
        return self.v(s)

class QNet(nn.Module):
    def __init__(self, s_dim: int, a_dim: int, hidden=(128,128)):
        super().__init__()
        self.q = MLP(s_dim + a_dim, 1, hidden, nn.ReLU, None)
    def forward(self, s, a):
        return self.q(torch.cat([s,a], dim=-1))

class GaussianPolicy(nn.Module):
    def __init__(self, s_dim: int, a_dim: int, hidden=(128,128), log_std_min=-5, log_std_max=2, act_limit=1.0):
        super().__init__()
        self.mu = MLP(s_dim, a_dim, hidden, nn.ReLU, None)
        self.log_std = nn.Parameter(torch.zeros(a_dim))  # state-independent std (simple, CPU-friendly)
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.act_limit = float(act_limit)

    def forward(self, s):
        mu = self.mu(s)
        log_std = torch.clamp(self.log_std, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std)
        # reparam sample
        eps = torch.randn_like(mu)
        a = torch.tanh(mu + eps * std) * self.act_limit
        return a, mu, std

    def deterministic(self, s):
        mu = self.mu(s)
        return torch.tanh(mu) * self.act_limit

class IQL:
    def __init__(
        self,
        s_dim: int, a_dim: int,
        gamma: float = 0.99,
        tau: float = 0.005,
        expectile: float = 0.7,
        beta: float = 3.0,          # advantage temperature for AWR
        actor_lr: float = 3e-4,
        q_lr: float = 3e-4,
        v_lr: float = 3e-4,
        device: str = "cpu",
        act_limit: float = 1.0
    ):
        self.device = device
        self.gamma = gamma
        self.tau = tau
        self.expectile = expectile
        self.beta = beta

        self.v = VNet(s_dim).to(device)
        self.q1 = QNet(s_dim, a_dim).to(device)
        self.q2 = QNet(s_dim, a_dim).to(device)
        self.pi = GaussianPolicy(s_dim, a_dim, act_limit=act_limit).to(device)

        self.v_opt  = torch.optim.Adam(self.v.parameters(), lr=v_lr)
        self.q1_opt = torch.optim.Adam(self.q1.parameters(), lr=q_lr)
        self.q2_opt = torch.optim.Adam(self.q2.parameters(), lr=q_lr)
        self.pi_opt = torch.optim.Adam(self.pi.parameters(), lr=actor_lr)

        self.v_targ = VNet(s_dim).to(device)
        self.v_targ.load_state_dict(self.v.state_dict())

    def _expectile_loss(self, diff):
        # expectile regression loss: weight errors asymmetrically
        w = torch.where(diff > 0, self.expectile, 1 - self.expectile)
        return (w * (diff**2)).mean()

    def update(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        s  = batch["s"].to(self.device)
        a  = batch["a"].to(self.device)
        r  = batch["r"].to(self.device)
        sp = batch["sp"].to(self.device)
        d  = batch["d"].to(self.device)

        with torch.no_grad():
            v_sp = self.v_targ(sp)
            y = r + (1.0 - d) * self.gamma * v_sp

        # ----- Q update (twin) -----
        q1_pred = self.q1(s, a)
        q2_pred = self.q2(s, a)
        q1_loss = F.mse_loss(q1_pred, y)
        q2_loss = F.mse_loss(q2_pred, y)

        self.q1_opt.zero_grad(set_to_none=True)
        q1_loss.backward()
        self.q1_opt.step()

        self.q2_opt.zero_grad(set_to_none=True)
        q2_loss.backward()
        self.q2_opt.step()

        # ----- V update via expectile regression -----
        with torch.no_grad():
            q_min = torch.minimum(q1_pred, q2_pred)
        v_pred = self.v(s)
        v_loss = self._expectile_loss(q_min - v_pred)

        self.v_opt.zero_grad(set_to_none=True)
        v_loss.backward()
        self.v_opt.step()
        polyak_update(self.v_targ, self.v, self.tau)

        # ----- Actor update via advantage-weighted regression -----
        with torch.no_grad():
            adv = torch.minimum(self.q1(s, a), self.q2(s, a)) - self.v(s)
            weights = torch.clamp(torch.exp(adv * self.beta), max=100.0)  # stabilize
        a_samp, mu, std = self.pi(s)  # sample but we’ll supervise toward dataset a
        # Weighted MSE to dataset action
        pi_loss = (weights * ((self.pi.deterministic(s) - a) ** 2).sum(dim=1, keepdim=True)).mean()

        self.pi_opt.zero_grad(set_to_none=True)
        pi_loss.backward()
        self.pi_opt.step()

        return {
            "q1_loss": float(q1_loss.item()),
            "q2_loss": float(q2_loss.item()),
            "v_loss":  float(v_loss.item()),
            "pi_loss": float(pi_loss.item()),
        }

=== ./peek_parquet_summary.py ===

#!/usr/bin/env python3
# peek_parquet_summary.py
# Minimal, concise Parquet inspector using DuckDB.
# - Prints: file count, a few example paths, compact schema summary (first 30 cols),
#   detected timestamp column + format (ISO vs epoch_ms) with a couple sample values,
#   detected bid/ask price & size columns (prefers *_0), and short recommended DuckDB expressions.
# - Optional: --json writes full details to a JSON file; console stays concise.

import argparse, json, re, sys
from typing import List, Tuple, Optional
import duckdb

def sql_lit(s: str) -> str:
    return "'" + s.replace("'", "''") + "'"

def qident(name: str) -> str:
    return '"' + name.replace('"','""') + '"'

def describe_schema(con, glob: str) -> List[Tuple[str,str]]:
    rows = con.execute(f"DESCRIBE SELECT * FROM read_parquet({sql_lit(glob)})").fetchall()
    return [(r[0], r[1]) for r in rows]

def list_files(con, glob: str) -> List[str]:
    try:
        q = f"SELECT DISTINCT filename FROM read_parquet({sql_lit(glob)}, filename=true)"
        return [r[0] for r in con.execute(q).fetchall()]
    except Exception:
        return []

def sample_values(con, glob: str, col: str, n: int = 8) -> List[Optional[str]]:
    try:
        q = f"SELECT {qident(col)} FROM read_parquet({sql_lit(glob)}) LIMIT {n}"
        return [r[0] for r in con.execute(q).fetchall()]
    except Exception:
        return []

def to_level(name: str) -> Optional[int]:
    m = re.search(r'_(\d+)$', name)
    return int(m.group(1)) if m else None

def ts_candidates(cols: List[Tuple[str,str]]) -> List[str]:
    cands = []
    for c,_t in cols:
        lc = c.lower()
        if "ts" in lc or "time" in lc or "timestamp" in lc or "datetime" in lc:
            cands.append(c)
    seen=set(); out=[]
    for c in cands:
        if c not in seen:
            seen.add(c); out.append(c)
    return out or ["ts"]

def classify_ts_samples(samples: List[Optional[str]]) -> str:
    if not samples: return "unknown"
    # If most values are numeric-ish with 13 digits → epoch ms
    num_like=0; ms_like=0; iso_like=0; total=0
    for v in samples:
        if v is None: continue
        s = str(v)
        total += 1
        if re.fullmatch(r'\d{10,16}', s or ""):
            num_like += 1
            if len(s) >= 13: ms_like += 1
        if re.search(r'\d{4}-\d{2}-\d{2}[ T]', s):
            iso_like += 1
    if total==0: return "unknown"
    if ms_like >= max(3, total//2): return "epoch_ms"
    if iso_like >= max(3, total//2): return "iso"
    if num_like >= max(3, total//2): return "epoch_like"
    return "unknown"

def pick_candidates(cols: List[Tuple[str,str]], side: str, kind: str) -> List[str]:
    names=[c for c,_ in cols]
    cands=[]
    for n in names:
        ln=n.lower()
        if side not in ln: continue
        if kind=="px":
            if ("px" in ln) or ("price" in ln) or re.search(r'(^|_)(p|prc|price)($|_)', ln):
                cands.append(n)
        else:
            if ("sz" in ln) or ("size" in ln) or ("qty" in ln) or ("quantity" in ln) or re.search(r'(^|_)(q|qty|size|quantity)($|_)', ln):
                cands.append(n)
    cands=list(set(cands))
    cands.sort(key=lambda x: (999999 if to_level(x) is None else to_level(x), x))
    return cands

def coalesce_expr(cols: List[str], default: Optional[str]=None, limit:int=6) -> str:
    parts = [f"CAST({qident(c)} AS DOUBLE)" for c in cols[:limit]]
    if default is not None:
        parts.append(default)
    if not parts:
        return "NULL"
    return "COALESCE(" + ", ".join(parts) + ")"

def build_ts_expr(ts_cols: List[str]) -> str:
    parts=[]
    for c in ts_cols[:4]:
        parts.append(f"TRY_CAST({qident(c)} AS TIMESTAMP)")
    for c in ts_cols[:4]:
        parts.append(f"to_timestamp(CAST(TRY_CAST({qident(c)} AS BIGINT) AS DOUBLE)/1000.0)")
    # minimal common fallbacks
    for c in ["ts_ms","timestamp_ms","time_ms"]:
        parts.append(f"to_timestamp(CAST(TRY_CAST({c} AS BIGINT) AS DOUBLE)/1000.0)")
    return "COALESCE(" + ", ".join(parts) + ")"

def main():
    ap=argparse.ArgumentParser(description="Concise Parquet summary (schema + detected OB fields).")
    ap.add_argument("--parquet", required=True, help="Recursive glob, e.g. /path/**/*.parquet")
    ap.add_argument("--json", default=None, help="Optional: write full report JSON here")
    args=ap.parse_args()

    con=duckdb.connect(database=":memory:")

    files=list_files(con, args.parquet)
    if not files:
        print(f"[info] No files matched: {args.parquet}")
        sys.exit(0)

    cols=describe_schema(con, args.parquet)
    # Detect fields
    ts_cols = ts_candidates(cols)
    # choose best ts candidate by which classifies cleanly
    ts_choice = ts_cols[0]
    ts_samples = sample_values(con, args.parquet, ts_choice, 8)
    ts_format = classify_ts_samples(ts_samples)

    bid_px = pick_candidates(cols, "bid", "px")
    ask_px = pick_candidates(cols, "ask", "px")
    bid_sz = pick_candidates(cols, "bid", "sz")
    ask_sz = pick_candidates(cols, "ask", "sz")

    # Short recommended expressions (trimmed to be readable)
    TS_EXPR   = build_ts_expr(ts_cols)
    BID_PX_EX = coalesce_expr(bid_px, default=None, limit=6)
    ASK_PX_EX = coalesce_expr(ask_px, default=None, limit=6)
    BID_SZ_EX = coalesce_expr(bid_sz, default="1.0", limit=6)
    ASK_SZ_EX = coalesce_expr(ask_sz, default="1.0", limit=6)

    # -------- Console summary (concise) --------
    print(f"[files] matched: {len(files)}")
    for p in files[:5]:
        print("  -", p)
    if len(files) > 5:
        print(f"  ... (+{len(files)-5} more)")

    print("\n[schema] (first 30 columns)")
    for i,(n,t) in enumerate(cols[:30],1):
        print(f"  {i:2d}. {n}: {t}")
    if len(cols) > 30:
        print(f"  ... (+{len(cols)-30} more columns)")

    print("\n[timestamp]")
    print(f"  candidates: {ts_cols[:6]}{' ...' if len(ts_cols)>6 else ''}")
    print(f"  chosen    : {ts_choice}")
    print(f"  format    : {ts_format}")
    if ts_samples:
        show = [str(s) for s in ts_samples[:3]]
        print(f"  samples   : {show}")

    print("\n[top-of-book candidates]")
    print(f"  bid_px: {bid_px[:6]}{' ...' if len(bid_px)>6 else ''}")
    print(f"  ask_px: {ask_px[:6]}{' ...' if len(ask_px)>6 else ''}")
    print(f"  bid_sz: {bid_sz[:6]}{' ...' if len(bid_sz)>6 else ''}")
    print(f"  ask_sz: {ask_sz[:6]}{' ...' if len(ask_sz)>6 else ''}")

    print("\n[recommended DuckDB expressions] (truncated to keep readable)")
    print(f"  TS_EXPR   = {TS_EXPR}")
    print(f"  BID_PX_EX = {BID_PX_EX}")
    print(f"  ASK_PX_EX = {ASK_PX_EX}")
    print(f"  BID_SZ_EX = {BID_SZ_EX}")
    print(f"  ASK_SZ_EX = {ASK_SZ_EX}")

    # -------- Optional JSON with fuller detail --------
    if args.json:
        report = {
            "parquet_glob": args.parquet,
            "file_count": len(files),
            "example_files": files[:10],
            "schema_first30": cols[:30],
            "all_columns_count": len(cols),
            "timestamp": {
                "candidates": ts_cols,
                "chosen": ts_choice,
                "format": ts_format,
                "samples_first3": [str(x) for x in ts_samples[:3]] if ts_samples else []
            },
            "candidates": {
                "bid_px": bid_px,
                "ask_px": ask_px,
                "bid_sz": bid_sz,
                "ask_sz": ask_sz
            },
            "expressions": {
                "TS_EXPR": TS_EXPR,
                "BID_PX_EX": BID_PX_EX,
                "ASK_PX_EX": ASK_PX_EX,
                "BID_SZ_EX": BID_SZ_EX,
                "ASK_SZ_EX": ASK_SZ_EX
            }
        }
        with open(args.json, "w") as f:
            json.dump(report, f, indent=2)
        print(f"\n[ok] wrote JSON: {args.json}")

if __name__ == "__main__":
    main()

=== ./consensus_gate_td3bc_flexible.py ===

#!/usr/bin/env python3
# consensus_gate_td3bc_flexible.py
# Works with TorchScript, full nn.Module, or raw state_dict (*.pt)
import argparse, json, csv, os, sys, re
from typing import List, Tuple, Optional
import numpy as np

try:
    import torch
    import torch.nn as nn
except ImportError:
    print("[error] Requires torch. Try: pip install torch --extra-index-url https://download.pytorch.org/whl/cpu", file=sys.stderr)
    sys.exit(2)

REQUIRED_COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

# ----- Helpers to support state_dict-only checkpoints -----

def _extract_layers_from_state_dict(sd: "OrderedDict") -> List[Tuple[torch.Tensor, torch.Tensor, int]]:
    """
    Return list of (W, b, layer_index) sorted by layer_index.
    Tries to infer layer order from numeric hints in keys; falls back to lexical order.
    """
    weights = []
    # collect pairs that look like *.weight / *.bias
    pairs = {}
    for k in sd.keys():
        if k.endswith(".weight"):
            prefix = k[:-7]
            bkey = prefix + ".bias"
            if bkey in sd:
                pairs[prefix] = (k, bkey)

    def get_idx(name: str) -> int:
        # pull last integer in the name, else 1e9 to send to the end
        m = list(re.finditer(r'(\d+)', name))
        return int(m[-1].group(1)) if m else 10**9

    items = []
    for prefix, (wk, bk) in pairs.items():
        idx = get_idx(prefix)
        items.append((idx, prefix, wk, bk))
    # sort by index then by name for stability
    items.sort(key=lambda x: (x[0], x[1]))

    for idx, prefix, wk, bk in items:
        W = sd[wk].detach().clone()
        b = sd[bk].detach().clone()
        weights.append((W, b, idx))
    return weights

class StateDictMLPRunner(nn.Module):
    """
    Minimal runner that applies a list of Linear layers defined by (W, b).
    Hidden layers = ReLU; Last layer = Tanh if output dim == 1 else Identity.
    """
    def __init__(self, layers: List[Tuple[torch.Tensor, torch.Tensor, int]]):
        super().__init__()
        # store as Parameters so we can .to(device)
        self.W = nn.ParameterList([nn.Parameter(W) for (W, b, _) in layers])
        self.b = nn.ParameterList([nn.Parameter(b) for (W, b, _) in layers])
        self.activ_last_tanh = (self.W[-1].shape[0] == 1)  # out_features
        self.eval()  # no-op but matches Module interface

    def forward(self, x: torch.Tensor):
        h = x
        for i in range(len(self.W)):
            # x shape: [B, in], W shape: [out, in], b: [out]
            h = torch.addmm(self.b[i], h, self.W[i].t())
            if i < len(self.W) - 1:
                h = torch.relu(h)
            else:
                if self.activ_last_tanh:
                    h = torch.tanh(h)
        # we don't have Q; return only action tensor
        return h

# ----- Model loader that handles 3 formats -----

def load_flexible_model(path: str, device: str):
    # 1) TorchScript
    try:
        m = torch.jit.load(path, map_location=device)
        m.eval()
        return m, "jit"
    except Exception:
        pass

    obj = torch.load(path, map_location=device)
    # 2) Full nn.Module
    if isinstance(obj, nn.Module):
        obj.eval()
        return obj, "module"

    # 3) Raw state_dict (OrderedDict)
    if isinstance(obj, dict):
        sd = obj.get("state_dict", None)
        if sd is None:
            # could be already a state_dict-like mapping
            sd = obj
        if isinstance(sd, dict):
            layers = _extract_layers_from_state_dict(sd)
            if not layers:
                raise RuntimeError("Could not infer Linear layers from state_dict keys.")
            runner = StateDictMLPRunner(layers).to(device)
            return runner, "state_dict"
    raise RuntimeError(f"Unsupported checkpoint format for {path}")

@torch.no_grad()
def model_forward(model, x: torch.Tensor):
    out = model(x)
    if isinstance(out, (tuple, list)) and len(out) >= 2:
        # (a, q) pattern
        return out[0], out[1]
    return out, None  # action only

def consensus_action(actions: np.ndarray, agree_eps: float, act_limit: float) -> Tuple[bool, float]:
    if actions.ndim != 1:
        actions = actions.reshape(-1)
    if actions.size == 0:
        return False, 0.0
    max_diff = float(np.max(np.abs(actions[:, None] - actions[None, :])))
    agree = max_diff <= (agree_eps * act_limit)
    mean_a = float(np.mean(actions))
    mean_a = max(-act_limit, min(act_limit, mean_a))
    return agree, mean_a

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True, help="consensus_gate_td3bc.json")
    ap.add_argument("--state_csv", required=True, help="CSV with: mid,spread,imbalance,mom1,mom3,vol3")
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    ap.add_argument("--out_csv", default=None)
    args = ap.parse_args()

    with open(args.gate, "r") as f:
        gate = json.load(f)
    if gate.get("type") != "td3bc_consensus_gate":
        print(f"[error] gate.type must be 'td3bc_consensus_gate' (got {gate.get('type')})", file=sys.stderr)
        sys.exit(2)

    actors: List[str] = gate["actors"]
    agree_eps: float = float(gate["agree_eps"])
    q_min_thresh: float = float(gate["q_min_thresh"])
    state_dim: int = int(gate["state_dim"])
    act_limit: float = float(gate.get("act_limit", 1.0))

    # Load states
    with open(args.state_csv, newline="") as f:
        rdr = csv.DictReader(f)
        hdr = [h.strip().lower() for h in (rdr.fieldnames or [])]
        if hdr != REQUIRED_COLS:
            print(f"[error] state_csv header mismatch.\n  got : {rdr.fieldnames}\n  want: {REQUIRED_COLS}", file=sys.stderr)
            sys.exit(3)
        X = []
        for row in rdr:
            try:
                X.append([float(row[c]) for c in REQUIRED_COLS])
            except Exception as e:
                print(f"[warn] bad row skipped: {e}", file=sys.stderr)
        X = np.asarray(X, dtype=np.float32)
    if X.size == 0:
        print("[error] no usable rows in state_csv.", file=sys.stderr)
        sys.exit(4)

    # Load models
    device = torch.device(args.device if (args.device != "cuda" or torch.cuda.is_available()) else "cpu")
    models = []
    kinds  = []
    for a in actors:
        if not os.path.isfile(a):
            print(f"[error] actor file not found: {a}", file=sys.stderr)
            sys.exit(5)
        m, kind = load_flexible_model(a, device)
        models.append(m)
        kinds.append(kind)
    print("[info] loaded actors:", list(zip(actors, kinds)))

    # Inference
    T = X.shape[0]
    results = []
    print("row_idx,agree,action,details")
    for i in range(T):
        s = torch.from_numpy(X[i:i+1]).to(device)  # [1, state_dim]
        acts = []
        qs   = []
        for m in models:
            a, q = model_forward(m, s)
            a_np = a.detach().cpu().numpy().reshape(-1)
            acts.append(float(a_np[0]) if a_np.size > 0 else 0.0)
            if q is not None:
                q_np = q.detach().cpu().numpy().reshape(-1)
                if q_np.size > 0: qs.append(float(q_np[0]))

        acts_arr = np.array(acts, dtype=np.float32)
        agree, mean_a = consensus_action(acts_arr, agree_eps, act_limit)

        q_ok = True; min_q=None
        if qs:
            min_q = float(np.min(qs))
            q_ok = (min_q >= q_min_thresh)

        final_ok = (agree and q_ok)
        action_out = mean_a if final_ok else 0.0

        details = {"actors": len(models), "agree_eps": agree_eps, "act_limit": act_limit,
                   "max_pair_diff": float(np.max(np.abs(acts_arr[:,None]-acts_arr[None,:]))) if len(acts_arr)>1 else 0.0,
                   "min_q": min_q}
        print(f"{i},{int(final_ok)},{action_out},{details}")
        results.append((i, int(final_ok), action_out, details))

    if args.out_csv:
        with open(args.out_csv, "w", newline="") as f:
            w = csv.writer(f); w.writerow(["row_idx","agree","action","min_q"])
            for i, ok, a, det in results:
                w.writerow([i, ok, a, (det.get("min_q") if isinstance(det, dict) else None)])

if __name__ == "__main__":
    main()

=== ./backtest_linear_deadband.py ===

#!/usr/bin/env python3
# backtest_linear_deadband.py — linear sizing with deadband on actions (|a|<db -> pos=0)
import argparse, csv, statistics as st

def read_states(p):
    with open(p, newline="") as f:
        r=csv.DictReader(f); mids=[float(row["mid"]) for row in r]
    return mids

def read_gate(p):
    acts=[]
    with open(p, newline="") as f:
        r=csv.DictReader(f)
        for row in r:
            agree=int(row["agree"]); a=float(row["action"])
            acts.append(a if agree==1 else 0.0)
    return acts

def metrics(eq):
    rets=[eq[i]/eq[i-1]-1 for i in range(1,len(eq))]
    if not rets: return 0,0,0
    m=st.mean(rets); s=st.pstdev(rets) or 1e-12
    sharpe=m/s
    peak=eq[0]; maxdd=0
    for x in eq:
        if x>peak: peak=x
        dd=peak/x-1
        if dd>maxdd: maxdd=dd
    return sharpe,maxdd,eq[-1]

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--linear_scale", type=float, default=0.75)
    ap.add_argument("--deadband", type=float, default=0.05)
    ap.add_argument("--hold_bars", type=int, default=8)
    ap.add_argument("--fee_bps", type=float, default=2.0)
    args=ap.parse_args()

    mids=read_states(args.states)
    acts=read_gate(args.gate)
    n=min(len(acts), len(mids)-args.h)
    mids=mids[:n+args.h]; acts=acts[:n]
    rets=[(mids[t+args.h]-mids[t])/mids[t] for t in range(n)]

    # deadbanded linear position
    raw=[0.0]*n
    for t,a in enumerate(acts):
        v = a if abs(a)>=args.deadband else 0.0
        v = max(-1.0, min(1.0, v*args.linear_scale))
        raw[t]=v

    # min-hold on continuous pos (defer changes until hold satisfied)
    pos=[0.0]*n
    if n>0:
        pos[0]=raw[0]; last_change=0
        for t in range(1,n):
            if raw[t]!=pos[t-1] and (t-last_change)<args.hold_bars:
                pos[t]=pos[t-1]
            else:
                if raw[t]!=pos[t-1]: last_change=t
                pos[t]=raw[t]

    fee = args.fee_bps/10000.0
    eq=[1.0]; prev=0.0; trades=0
    for t in range(n):
        turn=abs(pos[t]-prev); r=pos[t]*rets[t] - turn*fee
        eq.append(eq[-1]*(1+r))
        if pos[t]!=prev: trades+=1
        prev=pos[t]

    sharpe,maxdd,eqend = metrics(eq)
    print("[backtest H-linear-deadband]")
    print(f" rows={n} trades={trades} db={args.deadband} scale={args.linear_scale} hold={args.hold_bars} fee_bps={args.fee_bps}")
    print(f" equity_end={eqend:.4f} Sharpe_like={sharpe:.2f} MaxDD={maxdd:.2%}")

if __name__=="__main__":
    main()

=== ./consensus_gate_live.py ===

#!/usr/bin/env python3
# consensus_gate_live.py
# Live gate with scaler.json; forces float32 everywhere to avoid dtype mismatches.

import argparse, json, csv, os, sys, re
from typing import List, Tuple
import numpy as np

try:
    import torch
    import torch.nn as nn
except ImportError:
    print("[error] Requires torch.", file=sys.stderr); sys.exit(2)

COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

def _extract_layers_from_state_dict(sd: "OrderedDict"):
    pairs = {}
    for k in sd.keys():
        if k.endswith(".weight"):
            pre = k[:-7]; bk = pre + ".bias"
            if bk in sd: pairs[pre] = (k, bk)
    def get_idx(name: str) -> int:
        ms = list(re.finditer(r'(\d+)', name))
        return int(ms[-1].group(1)) if ms else 10**9
    items = [(get_idx(pre), pre, wk, bk) for pre,(wk,bk) in pairs.items()]
    items.sort(key=lambda x: (x[0], x[1]))
    layers=[]
    for _, pre, wk, bk in items:
        W = sd[wk].detach().clone().float()
        B = sd[bk].detach().clone().float()
        layers.append((W, B))
    return layers

class StateDictMLPRunner(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.W = nn.ParameterList([nn.Parameter(W) for (W,B) in layers])
        self.B = nn.ParameterList([nn.Parameter(B) for (W,B) in layers])
        self.activ_last_tanh = (self.W[-1].shape[0] == 1)
        self.eval()
    def forward(self, x):
        h = x
        for i in range(len(self.W)):
            h = torch.addmm(self.B[i], h, self.W[i].t())
            if i < len(self.W)-1: h = torch.relu(h)
            else:
                if self.activ_last_tanh: h = torch.tanh(h)
        return h

def load_flexible_model(path: str, device: str):
    try:
        m = torch.jit.load(path, map_location=device)
        m = m.float(); m.eval(); return m, "jit"
    except Exception:
        pass
    obj = torch.load(path, map_location=device)
    if isinstance(obj, nn.Module):
        obj = obj.float(); obj.eval(); return obj, "module"
    if isinstance(obj, dict):
        sd = obj.get("state_dict", None) or obj
        if isinstance(sd, dict):
            layers = _extract_layers_from_state_dict(sd)
            if not layers: raise RuntimeError("No linear layers parsed from state_dict.")
            runner = StateDictMLPRunner(layers).to(device).float()
            return runner, "state_dict"
    raise RuntimeError(f"Unsupported checkpoint format: {path}")

@torch.no_grad()
def model_forward(model, x):
    out = model(x)
    if isinstance(out, (tuple,list)) and len(out) >= 2:
        return out[0], out[1]
    return out, None

def consensus_action(actions: np.ndarray, agree_eps: float, act_limit: float):
    if actions.ndim != 1: actions = actions.reshape(-1)
    if actions.size == 0: return False, 0.0, 0.0
    max_diff = float(np.max(np.abs(actions[:,None]-actions[None,:])))
    agree = max_diff <= (agree_eps * act_limit)
    mean_a = float(np.mean(actions)); mean_a = max(-act_limit, min(act_limit, mean_a))
    return agree, mean_a, max_diff

def load_scaler(path: str):
    with open(path, "r") as f:
        S = json.load(f)
    if S.get("type") != "zscore": raise SystemExit("scaler.json type must be 'zscore'")
    cols = S["cols"]
    if [c.lower() for c in cols] != COLS: raise SystemExit(f"scaler cols mismatch: {cols}")
    mean = np.asarray(S["mean"], dtype=np.float32)
    std  = np.asarray(S["std"], dtype=np.float32)
    std  = np.where(std < 1e-8, 1.0, std).astype(np.float32)
    return mean, std

def parse_state_row(s: str):
    vals = [float(x.strip()) for x in s.split(",")]
    if len(vals) != 6: raise SystemExit("state_row must have 6 comma-separated values")
    return np.asarray([vals], dtype=np.float32)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--state_csv")
    g.add_argument("--state_row")
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    ap.add_argument("--out_csv", default=None)
    args = ap.parse_args()

    with open(args.gate,"r") as f: gate = json.load(f)
    if gate.get("type") != "td3bc_consensus_gate":
        raise SystemExit("gate.type must be td3bc_consensus_gate")
    actors: List[str] = gate["actors"]
    agree_eps: float = float(gate["agree_eps"])
    q_min_thresh: float = float(gate["q_min_thresh"])
    act_limit: float = float(gate.get("act_limit", 1.0))

    mu, sd = load_scaler(args.scaler)  # float32

    if args.state_csv:
        X=[]
        with open(args.state_csv, newline="") as f:
            rdr = csv.DictReader(f)
            hdr = [h.strip().lower() for h in (rdr.fieldnames or [])]
            if hdr != COLS: raise SystemExit(f"state_csv header mismatch: got {rdr.fieldnames}, want {COLS}")
            for row in rdr:
                try:
                    X.append([float(row[c]) for c in COLS])
                except: pass
        if not X: raise SystemExit("no usable rows in state_csv")
        X = np.asarray(X, dtype=np.float32)
    else:
        X = parse_state_row(args.state_row)  # float32

    # normalize (stay float32)
    Xn = ((X - mu) / sd).astype(np.float32)

    device = torch.device(args.device if (args.device != "cuda" or torch.cuda.is_available()) else "cpu")
    models=[]; kinds=[]
    for a in actors:
        if not os.path.isfile(a): raise SystemExit(f"actor not found: {a}")
        m, k = load_flexible_model(a, device); models.append(m); kinds.append(k)
    print("[info] loaded actors:", list(zip(actors, kinds)))
    print("[info] agree_eps=", agree_eps, " act_limit=", act_limit, " q_min_thresh=", q_min_thresh)

    T = Xn.shape[0]; out=[]
    print("row_idx,agree,action,max_pair_diff,min_q")
    for i in range(T):
        s = torch.from_numpy(Xn[i:i+1].astype(np.float32)).to(device).to(torch.float32)
        acts=[]; qs=[]
        for m in models:
            a, q = model_forward(m, s)
            a_np = a.detach().cpu().numpy().astype(np.float32).reshape(-1)
            acts.append(float(a_np[0]) if a_np.size>0 else 0.0)
            if q is not None:
                q_np = q.detach().cpu().numpy().astype(np.float32).reshape(-1)
                if q_np.size>0: qs.append(float(q_np[0]))
        aa = np.array(acts, dtype=np.float32)
        agree, mean_a, maxdiff = consensus_action(aa, agree_eps, act_limit)
        min_q = float(np.min(qs)) if qs else None
        ok = agree and ((min_q is None) or (min_q >= q_min_thresh))
        action_out = mean_a if ok else 0.0
        print(f"{i},{int(ok)},{action_out},{maxdiff},{min_q}")
        out.append((i,int(ok),action_out,min_q))

    if args.out_csv:
        with open(args.out_csv, "w", newline="") as f:
            w = csv.writer(f); w.writerow(["row_idx","agree","action","min_q"])
            for i,ok,a,mq in out: w.writerow([i,ok,a,mq])

if __name__ == "__main__":
    main()

=== ./rank_sweep.py ===

#!/usr/bin/env python3
import argparse, csv, os, sys, glob
from collections import namedtuple

Row = namedtuple("Row", [
    "mode","H","fee_bps","min_agree_k","agree_eps","min_mag",
    "action_thresh","linear_scale","hold_bars","rows","trades",
    "trade_winrate","per_bar_hitrate","equity_end","Sharpe_like",
    "MaxDD","CAGR_like","exposure","avg_turnover","avg_trade_pnl",
    "median_trade_pnl","saturation_neg1","saturation_pos1","saturation_mid","gate_csv","tag"
])

def parse_float(x):
    try:
        return float(x)
    except:
        return None

def pct_to_frac(v):
    """
    Accepts:
      - '57.14%'         -> 0.5714
      - '57.14' (0..100) -> 0.5714
      - 0.5714 (0..1)    -> 0.5714
      - 57.14 (0..100)   -> 0.5714
    Returns fraction in [0,1] or None.
    """
    if v is None or v == "":
        return None
    if isinstance(v, str):
        s = v.strip()
        if s.endswith("%"):
            num = parse_float(s[:-1])
            return None if num is None else num/100.0
        num = parse_float(s)
        if num is None:
            return None
    else:
        num = v
    # If it's obviously 0..1 already
    if 0.0 <= num <= 1.0:
        return num
    # If it looks like a percent (0..100], scale down
    if 0.0 <= num <= 100.0:
        return num/100.0
    # Out of range -> None
    return None

def load_results(sweep_dir):
    res = []
    path = os.path.join(sweep_dir, "results.csv")
    if not os.path.exists(path):
        sys.exit(f"[error] results.csv not found in {sweep_dir}")
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        rows = list(r)
        if not rows:
            sys.exit("[error] results.csv is empty")
        for d in rows:
            tag = d.get("tag") or (d.get("gate_csv","").replace("gate_","").replace(".csv",""))
            # numeric fields
            H = parse_float(d.get("H"))
            fee_bps = parse_float(d.get("fee_bps"))
            min_agree_k = parse_float(d.get("min_agree_k"))
            agree_eps = parse_float(d.get("agree_eps"))
            min_mag = parse_float(d.get("min_mag"))
            action_thresh = parse_float(d.get("action_thresh"))
            linear_scale = parse_float(d.get("linear_scale"))
            hold_bars = parse_float(d.get("hold_bars"))
            rows_cnt = parse_float(d.get("rows"))
            trades = parse_float(d.get("trades"))
            equity_end = parse_float(d.get("equity_end"))
            Sharpe_like = parse_float(d.get("Sharpe_like"))
            MaxDD = pct_to_frac(d.get("MaxDD"))
            CAGR_like = pct_to_frac(d.get("CAGR_like"))
            exposure = pct_to_frac(d.get("exposure"))
            avg_turnover = parse_float(d.get("avg_turnover"))
            avg_trade_pnl = parse_float(d.get("avg_trade_pnl"))
            median_trade_pnl = parse_float(d.get("median_trade_pnl"))
            sat_n1 = parse_float(d.get("saturation_neg1") or 0)
            sat_p1 = parse_float(d.get("saturation_pos1") or 0)
            sat_mid = parse_float(d.get("saturation_mid") or 0)
            trade_winrate = pct_to_frac(d.get("trade_winrate"))
            per_bar_hitrate = pct_to_frac(d.get("per_bar_hitrate"))
            # coerce ints where appropriate
            H = int(H) if H is not None else None
            min_agree_k = int(min_agree_k) if min_agree_k is not None else None
            hold_bars = int(hold_bars) if hold_bars is not None else None
            rows_cnt = int(rows_cnt) if rows_cnt is not None else None
            trades = int(trades) if trades is not None else 0
            sat_n1 = int(sat_n1) if sat_n1 is not None else 0
            sat_p1 = int(sat_p1) if sat_p1 is not None else 0
            sat_mid = int(sat_mid) if sat_mid is not None else 0
            res.append(Row(
                mode=d.get("mode","sign"),
                H=H,
                fee_bps=fee_bps or 0.0,
                min_agree_k=min_agree_k or 0,
                agree_eps=agree_eps or 0.0,
                min_mag=min_mag,
                action_thresh=action_thresh,
                linear_scale=linear_scale,
                hold_bars=hold_bars,
                rows=rows_cnt,
                trades=trades,
                trade_winrate=trade_winrate,
                per_bar_hitrate=per_bar_hitrate,
                equity_end=equity_end or 1.0,
                Sharpe_like=Sharpe_like or 0.0,
                MaxDD=MaxDD or 0.0,
                CAGR_like=CAGR_like or 0.0,
                exposure=exposure or 0.0,
                avg_turnover=avg_turnover or 0.0,
                avg_trade_pnl=avg_trade_pnl or 0.0,
                median_trade_pnl=median_trade_pnl or 0.0,
                saturation_neg1=sat_n1,
                saturation_pos1=sat_p1,
                saturation_mid=sat_mid,
                gate_csv=d.get("gate_csv",""),
                tag=tag,
            ))
    return res

def color_enabled(force):
    return True if force else sys.stdout.isatty()

def cfmt(enabled):
    class C:
        def __init__(self, on):
            if not on:
                self.end=self.green=self.red=self.yellow=self.blue=self.cyan=self.magenta=""
                return
            self.end="\033[0m"
            self.green="\033[38;5;82m"
            self.red="\033[38;5;196m"
            self.yellow="\033[38;5;220m"
            self.blue="\033[38;5;39m"
            self.cyan="\033[38;5;51m"
            self.magenta="\033[38;5;207m"
    return C(enabled)

def fmt_pct(frac):
    return "-" if frac is None else f"{frac*100:.2f}%"

def score_row(r, w_eq=100.0, w_sh=2.0, w_dd=50.0, w_exp=0.5):
    # Higher is better: equity↑, Sharpe↑, drawdown↓, exposure↓ (small)
    return (w_eq*(r.equity_end-1.0)) + (w_sh*r.Sharpe_like) - (w_dd*r.MaxDD) - (w_exp*r.exposure)

def parse_best_flags(r):
    k = r.min_agree_k
    eps = r.agree_eps
    mag = r.min_mag
    th  = r.action_thresh
    hold = r.hold_bars
    flags = ["--consensus-mode","sign","--min-agree-k",str(k),"--agree-eps",str(eps)]
    if mag is not None:
        flags += ["--min-mag", str(mag)]
    if th is not None:
        flags += ["--buy_th", str(th), "--sell_th", str(th)]
    if hold is not None:
        flags += ["--hold_bars", str(hold)]
    return flags

def explain_filter_fail(rows, args):
    reasons = {
        "min_trades": 0,
        "min_equity": 0,
        "min_sharpe": 0,
        "max_dd": 0,
    }
    for r in rows:
        if r.trades is None or r.trades < args.min_trades: reasons["min_trades"] += 1
        if r.equity_end is None or r.equity_end < args.min_equity: reasons["min_equity"] += 1
        if r.Sharpe_like is None or r.Sharpe_like < args.min_sharpe: reasons["min_sharpe"] += 1
        if r.MaxDD is None or (r.MaxDD*100.0) > args.max_dd: reasons["max_dd"] += 1
    return reasons

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--sweep", required=True, help="runs/sweep_* or quoted glob")
    ap.add_argument("--top", type=int, default=12)
    ap.add_argument("--min-trades", type=int, default=6)
    ap.add_argument("--min-equity", type=float, default=1.0)
    ap.add_argument("--min-sharpe", type=float, default=0.0)
    ap.add_argument("--max-dd", type=float, default=1.5, help="max drawdown in percent")
    ap.add_argument("--force-color", action="store_true")
    args = ap.parse_args()

    # Expand globs, pick latest non-empty
    dirs = sorted([d for g in glob.glob(args.sweep) for d in ([g] if os.path.isdir(g) else [])])
    if not dirs:
        sys.exit(f"[error] no sweep dirs match: {args.sweep}")
    chosen = None; rows = None
    for d in reversed(dirs):
        try:
            rows = load_results(d)
            chosen = d
            break
        except SystemExit:
            continue
    if chosen is None:
        sys.exit("[error] found no non-empty results.csv in provided sweep(s)")

    C = cfmt(color_enabled(args.force_color))
    print(f"[sweep] {chosen}  (rows={len(rows)})")

    # Filter with guardrails
    filt = []
    for r in rows:
        if r.trades is None or r.trades < args.min_trades: continue
        if r.equity_end is None or r.equity_end < args.min_equity: continue
        if r.Sharpe_like is None or r.Sharpe_like < args.min_sharpe: continue
        if r.MaxDD is None or (r.MaxDD*100.0) > args.max_dd: continue
        filt.append(r)

    if not filt:
        why = explain_filter_fail(rows, args)
        print(f"{C.yellow}[warn]{C.end} no rows passed filters "
              f"(min_trades={args.min_trades}, min_equity={args.min_equity}, "
              f"min_sharpe={args.min_sharpe}, max_dd={args.max_dd}%)")
        print(f" breakdown: trades<{args.min_trades}: {why['min_trades']}, "
              f"equity<{args.min_equity}: {why['min_equity']}, "
              f"sharpe<{args.min_sharpe}: {why['min_sharpe']}, "
              f"dd>{args.max_dd}%: {why['max_dd']}")
        sys.exit(0)

    ranked = sorted(filt, key=lambda r: (-score_row(r), -r.Sharpe_like, r.MaxDD, -r.equity_end))[:args.top]

    # Pretty table with *only* the important KPIs
    print("=== Top Results (filtered & ranked) ===")
    print("Tag | Trades | "
          f"{C.cyan}Winrate{C.end} | {C.cyan}HitRate{C.end} | "
          f"{C.green}EquityEnd{C.end} | {C.green}Sharpe{C.end} | {C.red}MaxDD{C.end} | "
          "H | k | eps | mag | th | hold")
    print("-"*98)

    for r in ranked:
        print(
            f"{r.tag} | {r.trades} | "
            f"{C.cyan}{fmt_pct(r.trade_winrate)}{C.end} | "
            f"{C.cyan}{fmt_pct(r.per_bar_hitrate)}{C.end} | "
            f"{C.green}{r.equity_end:.4f}{C.end} | "
            f"{C.green}{r.Sharpe_like:.2f}{C.end} | "
            f"{C.red}{r.MaxDD*100:.2f}%{C.end} | "
            f"{r.H} | {r.min_agree_k} | {r.agree_eps} | {r.min_mag} | {r.action_thresh} | {r.hold_bars}"
        )

    best = ranked[0]
    print("\n=== Recommended Params (no live execution) ===")
    print(f"Gate CSV: {best.gate_csv}")
    print(f"Tag     : {best.tag}")
    print(f"KPI     : Win {fmt_pct(best.trade_winrate)} | Hit {fmt_pct(best.per_bar_hitrate)} | "
          f"Eq {best.equity_end:.4f} | Sharpe {best.Sharpe_like:.2f} | MaxDD {best.MaxDD*100:.2f}%")

    flags = " ".join(parse_best_flags(best))
    print("\n# When you LATER want to test with these thresholds (no live now):")
    print("BACKTEST (sign mode):")
    print(f"  python3 backtest_gate_horizon_v3.py \\")
    print(f"    --states states.csv --gate {os.path.join('runs', os.path.basename(chosen), 'gates', best.gate_csv)} \\")
    print(f"    --h {best.H} --mode sign --action_thresh {best.action_thresh} --hold_bars {best.hold_bars} --fee_bps {best.fee_bps}")

    print("\n# If/when you want consensus flags for later (again, do not run live yet):")
    print(f"  {flags}")

if __name__ == "__main__":
    main()

=== ./retest_sweep_on_subset.py ===

#!/usr/bin/env python3
import argparse, os, re, shutil, subprocess, sys, csv
from pathlib import Path
from datetime import datetime, timezone

RE_NUM  = r'[-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?'
RE_INT  = r'\d+'
def rx(p): return re.compile(p)

# Robust patterns
R = {
  "rows":     rx(rf'rows=({RE_INT})'),
  "trades":   rx(rf'trades=({RE_INT})'),
  "win":      rx(rf'trade_winrate=({RE_NUM})%'),
  "hit":      rx(rf'per_bar_hitrate=({RE_NUM})%'),
  "eq":       rx(rf'equity_end=({RE_NUM})'),
  "sharpe":   rx(rf'Sharpe_like=({RE_NUM})'),
  "maxdd":    rx(rf'MaxDD=({RE_NUM})%'),
  "cagr":     rx(rf'CAGR_like=({RE_NUM})%'),
  "expo":     rx(rf'exposure=({RE_NUM})%'),
  "turn":     rx(rf'avg_turnover=({RE_NUM})/bar'),
  "avgp":     rx(rf'avg_trade_pnl=({RE_NUM})'),
  "medp":     rx(rf'median_trade_pnl=({RE_NUM})'),
}

FN_RX = re.compile(
  r'^gate_k(?P<k>\d+)_eps(?P<eps>[0-9.]+)_mag(?P<mag>[0-9.]+)_th(?P<th>[0-9.]+)_hold(?P<hold>\d+)\.csv$'
)

def g1(rx, s):
    m = rx.search(s)
    return m.group(1) if m else None

def f_or(x, default):
    try: 
        return float(x)
    except:
        return default

def sat_counts(csv_path: Path):
    neg = pos = mid = 0
    with csv_path.open() as f:
        reader = csv.reader(f)
        header = next(reader, None)
        for row in reader:
            if not row or len(row) < 3: 
                continue
            try:
                a = float(row[2])
            except:
                continue
            if a <= -0.99: neg += 1
            elif a >= 0.99: pos += 1
            else: mid += 1
    return neg, pos, mid

def run_backtest(states, gate, H, fee_bps):
    mm = FN_RX.match(gate.name)
    th   = mm.group("th")
    hold = mm.group("hold")
    cmd = [
        "python3", "backtest_gate_horizon_v3.py",
        "--states", states, "--gate", str(gate),
        "--h", str(H), "--mode", "sign",
        "--action_thresh", th, "--hold_bars", hold,
        "--fee_bps", str(fee_bps), "--verbose"
    ]
    out = subprocess.check_output(cmd, text=True, stderr=subprocess.STDOUT)
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--sweep-src", required=True)
    ap.add_argument("--states", required=True)
    ap.add_argument("--H", type=int, default=40)
    ap.add_argument("--fee-bps", type=float, default=5.0)
    ap.add_argument("--out", default=None)
    args = ap.parse_args()

    root = Path.cwd()
    src = Path(args.sweep_src).resolve()
    gates_src = src / "gates"
    if not gates_src.exists():
        print(f"[error] no gates dir in {src}", file=sys.stderr); return 2

    if args.out:
        out_dir = Path(args.out).resolve()
    else:
        ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        out_dir = root / "runs" / f"sweep_sign_retest_{ts}"
    gates_out = out_dir / "gates"
    logs_out  = out_dir / "logs"
    gates_out.mkdir(parents=True, exist_ok=True)
    logs_out.mkdir(parents=True, exist_ok=True)

    # copy gates
    copied = 0
    for g in sorted(gates_src.glob("gate_*.csv")):
        shutil.copy2(g, gates_out / g.name); copied += 1
    if copied == 0:
        print(f"[error] no gate_*.csv in {gates_src}", file=sys.stderr); return 2

    results_path = out_dir / "results.csv"
    with results_path.open("w", newline="") as f:
        w = csv.writer(f)
        w.writerow([
            "mode","H","fee_bps","min_agree_k","agree_eps","min_mag","action_thresh",
            "linear_scale","hold_bars","rows","trades","trade_winrate","per_bar_hitrate",
            "equity_end","Sharpe_like","MaxDD","CAGR_like","exposure","avg_turnover",
            "avg_trade_pnl","median_trade_pnl","saturation_neg1","saturation_pos1",
            "saturation_mid","gate_csv","tag"
        ])

        for g in sorted(gates_out.glob("gate_*.csv")):
            base = g.name
            mm = FN_RX.match(base)
            if not mm:
                print(f"[skip] cannot parse params from {base}", file=sys.stderr)
                continue

            K    = int(mm.group("k"))
            EPS  = float(mm.group("eps"))
            MAG  = float(mm.group("mag"))
            TH   = float(mm.group("th"))
            HOLD = int(mm.group("hold"))

            # saturation
            sneg, spos, smid = sat_counts(g)

            # backtest + raw log
            try:
                out = run_backtest(args.states, g, args.H, args.fee_bps)
            except subprocess.CalledProcessError as e:
                out = e.output

            (logs_out / f"{base}.log").write_text(out)

            rows    = int(f_or(g1(R["rows"],   out), 0))
            trades  = int(f_or(g1(R["trades"], out), 0))
            win     = f_or(g1(R["win"],   out), 0.0)   # already stripped %
            hit     = f_or(g1(R["hit"],   out), 0.0)
            eq      = f_or(g1(R["eq"],    out), 0.0)
            sharpe  = f_or(g1(R["sharpe"],out), 0.0)
            maxdd   = f_or(g1(R["maxdd"], out), 0.0)
            cagr    = f_or(g1(R["cagr"],  out), 0.0)
            expo    = f_or(g1(R["expo"],  out), 0.0)
            turn    = f_or(g1(R["turn"],  out), 0.0)
            avgp    = f_or(g1(R["avgp"],  out), 0.0)
            medp    = f_or(g1(R["medp"],  out), 0.0)

            tag = f"k{K}_eps{EPS}_mag{MAG}_th{TH}_hold{HOLD}"
            w.writerow([
                "sign", args.H, args.fee_bps, K, EPS, MAG, TH, "",
                HOLD, rows, trades, win, hit, eq, sharpe, maxdd, cagr,
                expo, turn, avgp, medp, sneg, spos, smid, base, tag
            ])
            print(f"[ok] {base:40s} eq={eq:7.4f} sharpe={sharpe:5.2f} trades={trades}  sat: (-1){sneg} mid{smid} (+1){spos}", file=sys.stderr)

    print(f"[done] retested to {out_dir}")
    return 0

if __name__ == "__main__":
    sys.exit(main())

=== ./build_gate_from_states.py ===

#!/usr/bin/env python3
import csv, subprocess, sys, os, argparse

GATE_JSON   = "consensus_gate_td3bc.json"
SCALER_JSON = "scaler.json"
LIVE_GATE   = "consensus_gate_live_plus.py"  # must be in CWD

def run_live_gate(state_row, device, mode, k, eps, minmag):
    cmd = [
        sys.executable, LIVE_GATE,
        "--gate", GATE_JSON,
        "--scaler", SCALER_JSON,
        "--state_row", state_row,
        "--device", device,
        "--consensus-mode", mode,
        "--min-agree-k", str(k),
        "--agree-eps", f"{eps}",
        "--min-mag", f"{minmag}",
    ]
    res = subprocess.run(cmd, capture_output=True, text=True)
    if res.returncode != 0:
        raise RuntimeError(f"live gate failed: {res.stdout}\n{res.stderr}")
    # pull the CSV data line: "row_idx,agree,action,max_pair_diff,min_q" or "0,...."
    last = None
    for line in res.stdout.strip().splitlines():
        t = line.strip()
        if t.startswith("row_idx,") or (t and t[0].isdigit() and "," in t):
            last = t
    if not last:
        raise RuntimeError(f"could not parse gate output:\n{res.stdout}")
    return last

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("states_csv")
    ap.add_argument("out_csv")
    ap.add_argument("--device", choices=["cpu","cuda","mps"], default="cpu")
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--min-agree-k", type=int, default=2)
    ap.add_argument("--agree-eps", type=float, default=0.10)
    ap.add_argument("--min-mag", type=float, default=0.10)
    args = ap.parse_args()

    for f in (GATE_JSON, SCALER_JSON, LIVE_GATE):
        if not os.path.exists(f):
            sys.exit(f"[error] required file not found: {f}")

    with open(args.states_csv, newline="") as f:
        rdr = csv.reader(f)
        header = next(rdr, None)
        rows = [r for r in rdr]

    out_lines = ["row_idx,agree,action,max_pair_diff,min_q"]
    for i, r in enumerate(rows):
        state_row = ",".join(r)
        line = run_live_gate(state_row, args.device, args.mode,
                             args["min_agree_k"] if isinstance(args, dict) else args.min_agree_k,
                             args["agree_eps"] if isinstance(args, dict) else args.agree_eps,
                             args["min_mag"] if isinstance(args, dict) else args.min_mag)
        # replace leading "0," with actual row index for cleanliness
        if line.startswith("row_idx,"):
            # header, keep once at file top
            pass
        elif line.startswith("0,"):
            line = f"{i}" + line[1:]
        out_lines.append(line)

    with open(args.out_csv, "w") as f:
        f.write("\n".join(out_lines) + "\n")
    print(f"[ok] wrote {args.out_csv} with {len(rows)} rows")
if __name__ == "__main__":
    main()

=== ./consensus_gate_td3bc_flexible_norm.py ===

#!/usr/bin/env python3
# consensus_gate_td3bc_flexible_norm.py
# Flexible gate with optional z-score normalization; forces float32 everywhere to avoid dtype mismatches.

import argparse, json, csv, os, sys, re
from typing import List, Tuple, Optional
import numpy as np

try:
    import torch
    import torch.nn as nn
except ImportError:
    print("[error] Requires torch.", file=sys.stderr); sys.exit(2)

REQUIRED_COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

# ----- Helpers to support state_dict-only checkpoints -----

def _extract_layers_from_state_dict(sd: "OrderedDict"):
    pairs = {}
    for k in sd.keys():
        if k.endswith(".weight"):
            pre = k[:-7]; bk = pre + ".bias"
            if bk in sd: pairs[pre] = (k, bk)
    def get_idx(name: str) -> int:
        ms = list(re.finditer(r'(\d+)', name))
        return int(ms[-1].group(1)) if ms else 10**9
    items = [(get_idx(pre), pre, wk, bk) for pre,(wk,bk) in pairs.items()]
    items.sort(key=lambda x: (x[0], x[1]))
    layers=[]
    for _, pre, wk, bk in items:
        W = sd[wk].detach().clone().float()
        b = sd[bk].detach().clone().float()
        layers.append((W, b))
    return layers

class StateDictMLPRunner(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.W = nn.ParameterList([nn.Parameter(W) for (W, b) in layers])
        self.B = nn.ParameterList([nn.Parameter(b) for (W, b) in layers])
        self.activ_last_tanh = (self.W[-1].shape[0] == 1)
        self.eval()
    def forward(self, x):
        # x: float32
        h = x
        for i in range(len(self.W)):
            h = torch.addmm(self.B[i], h, self.W[i].t())
            if i < len(self.W) - 1:
                h = torch.relu(h)
            else:
                if self.activ_last_tanh:
                    h = torch.tanh(h)
        return h

def load_flexible_model(path: str, device: str):
    # 1) TorchScript
    try:
        m = torch.jit.load(path, map_location=device)
        m = m.float()  # force float32
        m.eval()
        return m, "jit"
    except Exception:
        pass
    # 2) torch.load
    obj = torch.load(path, map_location=device)
    if isinstance(obj, nn.Module):
        obj = obj.float()  # force float32
        obj.eval()
        return obj, "module"
    # 3) state_dict / mapping
    if isinstance(obj, dict):
        sd = obj.get("state_dict", None) or obj
        if isinstance(sd, dict):
            layers = _extract_layers_from_state_dict(sd)
            if not layers:
                raise RuntimeError("No linear layers parsed from state_dict.")
            runner = StateDictMLPRunner(layers).to(device).float()
            return runner, "state_dict"
    raise RuntimeError(f"Unsupported checkpoint format for {path}")

@torch.no_grad()
def model_forward(model, x):
    out = model(x)
    if isinstance(out, (tuple, list)) and len(out) >= 2:
        return out[0], out[1]
    return out, None

def consensus_action(actions: np.ndarray, agree_eps: float, act_limit: float):
    if actions.ndim != 1: actions = actions.reshape(-1)
    if actions.size == 0: return False, 0.0
    max_diff = float(np.max(np.abs(actions[:,None]-actions[None,:])))
    agree = max_diff <= (agree_eps * act_limit)
    mean_a = float(np.mean(actions))
    mean_a = max(-act_limit, min(act_limit, mean_a))
    return agree, mean_a

def compute_zscore_stats(X: np.ndarray):
    mu = X.mean(axis=0, dtype=np.float64)
    sd = X.std(axis=0, dtype=np.float64)
    sd = np.where(sd < 1e-8, 1.0, sd)
    return mu.astype(np.float32), sd.astype(np.float32)

def apply_zscore(X: np.ndarray, mu: np.ndarray, sd: np.ndarray):
    # Ensure float32
    return ((X.astype(np.float32) - mu.astype(np.float32)) / sd.astype(np.float32)).astype(np.float32)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True)
    ap.add_argument("--state_csv", required=True)
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    ap.add_argument("--out_csv", default=None)
    ap.add_argument("--zscore-from-states", action="store_true",
                    help="Compute mean/std from states.csv and z-score inputs before inference.")
    args = ap.parse_args()

    with open(args.gate, "r") as f:
        gate = json.load(f)
    if gate.get("type") != "td3bc_consensus_gate":
        print(f"[error] gate.type must be 'td3bc_consensus_gate' (got {gate.get('type')})", file=sys.stderr)
        sys.exit(2)

    actors: List[str] = gate["actors"]
    agree_eps: float = float(gate["agree_eps"])
    q_min_thresh: float = float(gate["q_min_thresh"])
    state_dim: int = int(gate["state_dim"])
    act_limit: float = float(gate.get("act_limit", 1.0))

    # Load states (float32)
    with open(args.state_csv, newline="") as f:
        rdr = csv.DictReader(f)
        hdr = [h.strip().lower() for h in (rdr.fieldnames or [])]
        if hdr != REQUIRED_COLS:
            print(f"[error] state_csv header mismatch.\n  got : {rdr.fieldnames}\n  want: {REQUIRED_COLS}", file=sys.stderr)
            sys.exit(3)
        X = []
        for row in rdr:
            try:
                X.append([float(row[c]) for c in REQUIRED_COLS])
            except Exception:
                pass
        X = np.asarray(X, dtype=np.float32)
    if X.size == 0:
        print("[error] no usable rows in state_csv.", file=sys.stderr)
        sys.exit(4)

    mu = sd = None
    if args.zscore_from_states:
        mu, sd = compute_zscore_stats(X)
        X = apply_zscore(X, mu, sd)
        print(f"[norm] z-score applied (float32).")
        print(f"[norm] mean  : {[round(float(x),4) for x in mu.tolist()]}")
        print(f"[norm] std   : {[round(float(x),4) for x in sd.tolist()]}")

    # Load models
    device = torch.device(args.device if (args.device != "cuda" or torch.cuda.is_available()) else "cpu")
    models=[]; kinds=[]
    for a in actors:
        if not os.path.isfile(a):
            print(f"[error] actor not found: {a}", file=sys.stderr); sys.exit(5)
        m, k = load_flexible_model(a, device)
        models.append(m)
        kinds.append(k)
    print("[info] loaded actors:", list(zip(actors, kinds)))

    # Inference (ensure float32 input tensors)
    T = X.shape[0]; results=[]
    print("row_idx,agree,action,details")
    for i in range(T):
        s = torch.from_numpy(X[i:i+1].astype(np.float32)).to(device).to(torch.float32)
        acts=[]; qs=[]
        for m in models:
            a, q = model_forward(m, s)
            a_np = a.detach().cpu().numpy().astype(np.float32).reshape(-1)
            acts.append(float(a_np[0]) if a_np.size>0 else 0.0)
            if q is not None:
                q_np = q.detach().cpu().numpy().astype(np.float32).reshape(-1)
                if q_np.size>0: qs.append(float(q_np[0]))

        aa = np.array(acts, dtype=np.float32)
        agree, mean_a = consensus_action(aa, agree_eps, act_limit)

        q_ok = True; min_q=None
        if qs:
            min_q = float(np.min(qs))
            q_ok = (min_q >= q_min_thresh)

        final_ok = (agree and q_ok)
        action_out = mean_a if final_ok else 0.0

        details = {"actors": len(models), "agree_eps": agree_eps, "act_limit": act_limit,
                   "max_pair_diff": float(np.max(np.abs(aa[:,None]-aa[None,:]))) if len(aa)>1 else 0.0,
                   "min_q": min_q}
        print(f"{i},{int(final_ok)},{action_out},{details}")
        results.append((i, int(final_ok), action_out, details))

    if args.out_csv:
        with open(args.out_csv, "w", newline="") as f:
            w = csv.writer(f); w.writerow(["row_idx","agree","action","min_q"])
            for i, ok, a, det in results:
                w.writerow([i, ok, a, (det.get("min_q") if isinstance(det, dict) else None)])

if __name__ == "__main__":
    main()

=== ./gen_states_from_parquet.py ===

#!/usr/bin/env python3
# gen_states_from_parquet.py — makes states.csv from your book snaps
# Usage:
#   python3 gen_states_from_parquet.py --parquet "/path/**/*.parquet" --out states.csv --limit 200
import argparse, duckdb, pandas as pd

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--out", default="states.csv")
    ap.add_argument("--limit", type=int, default=200, help="recent rows to export")
    args = ap.parse_args()

    con = duckdb.connect(database=":memory:")
    # robust: union_by_name handles mixed schemas; ts/timestamp both supported
    sql = f"""
WITH raw AS (
  SELECT * FROM read_parquet('{args.parquet}', filename=true, union_by_name=true)
),
proj AS (
  SELECT
    COALESCE(TRY_CAST("ts" AS TIMESTAMP), TRY_CAST("timestamp" AS TIMESTAMP))   AS ts_parsed,
    CAST(bid_px_1 AS DOUBLE) AS bid_px, CAST(ask_px_1 AS DOUBLE) AS ask_px,
    CAST(bid_sz_1 AS DOUBLE) AS bid_sz, CAST(ask_sz_1 AS DOUBLE) AS ask_sz
  FROM raw
),
ord AS (
  SELECT * FROM proj WHERE ts_parsed IS NOT NULL ORDER BY ts_parsed
),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px + ask_px)*0.5 AS mid,
    (ask_px - bid_px)     AS spread,
    CASE WHEN (bid_sz + ask_sz) > 0
         THEN (bid_sz - ask_sz)/NULLIF(bid_sz + ask_sz,0)
         ELSE 0 END       AS imbalance,
    ((bid_px + ask_px)*0.5 - LAG((bid_px + ask_px)*0.5, 1) OVER ()) AS mom1,
    ((bid_px + ask_px)*0.5 - LAG((bid_px + ask_px)*0.5, 3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px + ask_px)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
)
SELECT mid, spread, imbalance, mom1, mom3, vol3
FROM feat
WHERE mid IS NOT NULL
ORDER BY ts DESC
LIMIT {args.limit}
"""
    df = con.execute(sql).fetchdf()
    # ensure correct order & types
    cols = ["mid","spread","imbalance","mom1","mom3","vol3"]
    df = df[cols].astype("float32")
    df.to_csv(args.out, index=False)
    print(f"[ok] wrote {args.out} ({len(df)} rows)")

if __name__ == "__main__":
    main()

=== ./rebuild_results_from_gates.py ===

#!/usr/bin/env python3
import argparse, os, re, csv, subprocess, sys
ap = argparse.ArgumentParser()
ap.add_argument("--sweep", required=True)
ap.add_argument("--states", default="states.csv")
ap.add_argument("--H", type=int, default=40)
ap.add_argument("--mode", choices=["sign","linear"], default="sign")
ap.add_argument("--fee_bps", type=float, default=5.0)
args = ap.parse_args()

gates_dir = os.path.join(args.sweep, "gates")
if not os.path.isdir(gates_dir):
    print(f"[error] gates dir not found: {gates_dir}", file=sys.stderr); sys.exit(2)

hdr = ["mode","H","fee_bps","min_agree_k","agree_eps","min_mag","action_thresh","linear_scale",
       "hold_bars","rows","trades","trade_winrate","per_bar_hitrate","equity_end","Sharpe_like",
       "MaxDD","CAGR_like","exposure","avg_turnover","avg_trade_pnl","median_trade_pnl",
       "saturation_neg1","saturation_pos1","saturation_mid","gate_csv","tag"]

pat = re.compile(r"^gate_k(?P<k>\d+)_eps(?P<eps>[0-9.]+)_mag(?P<mag>[0-9.]+)_th(?P<th>[0-9.]+)_hold(?P<hold>\d+)\.csv$")
def parse_gate_name(fn):
    m = pat.match(fn)
    return (dict(min_agree_k=int(m["k"]), agree_eps=float(m["eps"]), min_mag=float(m["mag"]),
                 action_thresh=float(m["th"]), hold_bars=int(m["hold"])) if m else None)

def parse_kpis(text):
    import re
    g=lambda rx,default=None,cast=float:(lambda m: default if not m else cast(m.group(1)))(re.search(rx,text))
    return dict(
        rows=g(r"rows=(\d+)",cast=int), trades=g(r"trades=(\d+)",cast=int),
        trade_winrate=g(r"trade_winrate=([0-9.]+)%"), per_bar_hitrate=g(r"per_bar_hitrate=([0-9.]+)%"),
        equity_end=g(r"equity_end=([0-9.]+)"), Sharpe_like=g(r"Sharpe_like=([\-0-9.]+)"),
        MaxDD=g(r"MaxDD=([0-9.]+)%"), CAGR_like=g(r"CAGR_like=([\-0-9.]+)%"),
        exposure=g(r"exposure=([0-9.]+)%"), avg_turnover=g(r"avg_turnover=([0-9.]+)"),
        avg_trade_pnl=g(r"avg_trade_pnl=([\-0-9.eE]+)"), median_trade_pnl=g(r"median_trade_pnl=([\-0-9.eE]+)")
    )

out_csv = os.path.join(args.sweep, "results.csv")
with open(out_csv, "w", newline="") as f:
    w = csv.DictWriter(f, fieldnames=hdr); w.writeheader()
    for fn in sorted(os.listdir(gates_dir)):
        if not (fn.startswith("gate_") and fn.endswith(".csv")): continue
        meta = parse_gate_name(fn)
        if not meta: print(f"[skip] {fn}"); continue
        gate_path = os.path.join(gates_dir, fn)
        cmd = ["python3","backtest_gate_horizon_v3.py","--states",args.states,"--gate",gate_path,
               "--h",str(args.H),"--mode",args.mode,"--hold_bars",str(meta["hold_bars"]),
               "--fee_bps",str(args.fee_bps)]
        if args.mode=="sign": cmd += ["--action_thresh", str(meta["action_thresh"])]
        try:
            out = subprocess.check_output(cmd, text=True, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            print(f"[error] backtest failed for {fn}\n{e.output}", file=sys.stderr); continue
        k = parse_kpis(out)
        row = {"mode":args.mode,"H":args.H,"fee_bps":args.fee_bps,
               "min_agree_k":meta["min_agree_k"],"agree_eps":meta["agree_eps"],"min_mag":meta["min_mag"],
               "action_thresh":meta["action_thresh"],"linear_scale":"","hold_bars":meta["hold_bars"],
               "gate_csv":fn,"tag":f"k{meta['min_agree_k']}_eps{meta['agree_eps']}_mag{meta['min_mag']}_th{meta['action_thresh']}_hold{meta['hold_bars']}",
               "saturation_neg1":"","saturation_pos1":"","saturation_mid":""}
        row.update(k); w.writerow(row)
        sys.stdout.write(f"[ok] {fn} -> eq={row['equity_end']} sharpe={row['Sharpe_like']} trades={row['trades']}\n")
print(f"[done] wrote {out_csv}")

=== ./live_executor_hold_v2_3.py ===

#!/usr/bin/env python3
# live_executor_hold_v2_3.py
# - Same rules as v2.2 (sign consensus, min-hold/min-mag, guards)
# - NEW: backfills entry_mid/entry_ts if in position but entry fields are missing.
# - Prints/records mid; consistent CSV with uPnL bps.

import argparse, json, os, subprocess, sys
import datetime as dt

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        raise RuntimeError(p.stderr.strip())
    return p.stdout.strip()

def parse_state_row(row_csv):
    vals = [float(x.strip()) for x in row_csv.split(",")]
    if len(vals) != 6:
        raise ValueError("state_row must have 6 numeric fields")
    return dict(mid=vals[0], spread=vals[1], imbalance=vals[2], mom1=vals[3], mom3=vals[4], vol3=vals[5])

def now_utc_iso(timespec: str = "milliseconds") -> str:
    return dt.datetime.now(dt.timezone.utc).isoformat(timespec=timespec)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--buy_th", type=float, default=0.10)
    ap.add_argument("--sell_th", type=float, default=0.10)
    ap.add_argument("--hold_bars", type=int, default=12)
    ap.add_argument("--max_hold_bars", type=int, default=160)
    ap.add_argument("--spread_cap", type=float, default=5.0)
    ap.add_argument("--min_vol3", type=float, default=0.0)
    ap.add_argument("--statefile", default="live_state.json")
    ap.add_argument("--log", default="live_signals.csv")
    ap.add_argument("--ts_timespec", choices=["seconds","milliseconds"], default="milliseconds")
    args = ap.parse_args()

    # 1) Latest features
    row = run([sys.executable, "latest_state_from_parquet.py", "--parquet", args.parquet])
    feat = parse_state_row(row)

    # 2) Gate (sign consensus)
    min_mag = min(args.buy_th, args.sell_th)
    out = run([
        sys.executable, "consensus_gate_live_plus.py",
        "--gate", args.gate, "--scaler", args.scaler,
        "--state_row", row, "--device", args.device,
        "--consensus-mode", "sign", "--min-agree-k", "2",
        "--min-mag", f"{min_mag}"
    ])
    last = [ln for ln in out.splitlines() if ln and ln[0].isdigit()][-1]
    parts = last.split(",")
    raw_action = float(parts[2])

    # 3) intent (asymmetric thresholds)
    intent = 0
    if raw_action >= args.buy_th: intent = +1
    elif raw_action <= -args.sell_th: intent = -1

    # 4) load state + backfill if needed
    st = {"prev_pos": 0, "bars_held": 0, "bars_held_total": 0, "entry_mid": None, "entry_ts": None}
    if os.path.isfile(args.statefile):
        try:
            with open(args.statefile, "r") as f: st.update(json.load(f))
        except: pass
    prev_pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))
    held_total = int(st.get("bars_held_total", 0))
    entry_mid = st.get("entry_mid", None)
    entry_ts  = st.get("entry_ts", None)

    # Backfill entry if we're already in a position but entry is missing
    if prev_pos != 0 and entry_mid is None:
        entry_mid = feat["mid"]; entry_ts = now_utc_iso(args.ts_timespec)

    # 5) guards for NEW entries
    skipped_reason = ""
    if prev_pos == 0 and intent != 0:
        if feat["spread"] > args.spread_cap:
            skipped_reason = f"spread>{args.spread_cap}"
            intent = 0
        elif feat["vol3"] < args.min_vol3:
            skipped_reason = f"vol3<{args.min_vol3}"
            intent = 0

    # 6) Min-hold / max-hold and entry tracking
    next_pos = intent
    if next_pos != prev_pos:
        if prev_pos != 0 and next_pos != 0 and held < args.hold_bars:
            next_pos = prev_pos
            held += 1; held_total += 1
        else:
            if next_pos == 0:
                held = 0; held_total = 0
                entry_mid = None; entry_ts = None
            else:
                held = 0; held_total = 0
                entry_mid = feat["mid"]; entry_ts = now_utc_iso(args.ts_timespec)
    else:
        if next_pos != 0:
            held = min(held + 1, args.hold_bars)
            held_total += 1
            if held_total >= args.max_hold_bars:
                next_pos = 0
                skipped_reason = (skipped_reason + "; " if skipped_reason else "") + "max_hold_cap"
                held = 0; held_total = 0
                entry_mid = None; entry_ts = None

    # 7) unrealized PnL (bps)
    upnl_bps = ""
    if entry_mid is not None and next_pos != 0:
        ret = (feat["mid"] - entry_mid) / entry_mid
        upnl = ret * (1 if next_pos>0 else -1)
        upnl_bps = f"{upnl*10000:.2f}"

    # 8) persist
    with open(args.statefile, "w") as f:
        json.dump({
            "prev_pos": int(next_pos),
            "bars_held": int(held),
            "bars_held_total": int(held_total),
            "entry_mid": entry_mid,
            "entry_ts": entry_ts
        }, f)

    # 9) log (consistent columns)
    ts = now_utc_iso(args.ts_timespec)
    header = ("ts,prev_pos,next_pos,raw_action,buy_th,sell_th,hold_bars,hold_counter,"
              "mid,spread,vol3,entry_mid,entry_ts,upnl_bps,skipped_reason\n")
    line = (f"{ts},{prev_pos},{next_pos},{raw_action:.6f},{args.buy_th:.2f},{args.sell_th:.2f},"
            f"{args.hold_bars},{held},{feat['mid']:.2f},{feat['spread']:.6f},{feat['vol3']:.6f},"
            f"{'' if entry_mid is None else f'{entry_mid:.2f}'},"
            f"{'' if entry_ts is None else entry_ts},"
            f"{upnl_bps},{skipped_reason}")
    print(line)
    if not os.path.isfile(args.log):
        with open(args.log, "w") as f: f.write(header)
    with open(args.log, "a") as f: f.write(line + "\n")

if __name__ == "__main__":
    main()

=== ./validate_gate_and_state.py ===

#!/usr/bin/env python3
# validate_gate_and_state.py
# Checks: gate JSON loads, actor files exist, states.csv has correct header, device is valid.
import argparse, json, os, sys, csv

REQUIRED_COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True)
    ap.add_argument("--state_csv", required=True)
    ap.add_argument("--actors_dir", default=".")
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    args = ap.parse_args()

    # Load gate json
    try:
        with open(args.gate, "r") as f:
            gate = json.load(f)
    except Exception as e:
        print(f"[error] failed to load gate JSON: {e}", file=sys.stderr)
        sys.exit(2)

    # Required keys
    for k in ["type","actors","agree_eps","q_min_thresh","state_dim","act_limit"]:
        if k not in gate:
            print(f"[error] missing key in gate: {k}", file=sys.stderr)
            sys.exit(3)
    if gate["type"] != "td3bc_consensus_gate":
        print(f"[error] unexpected gate type: {gate['type']}", file=sys.stderr)
        sys.exit(3)
    if gate["state_dim"] != 6:
        print(f"[warn] state_dim in gate is {gate['state_dim']} but code expects 6 features")

    # Actor files present?
    missing = []
    for a in gate["actors"]:
        p = a if os.path.isabs(a) else os.path.join(args.actors_dir, a)
        if not os.path.isfile(p):
            missing.append(p)
    if missing:
        print("[error] missing actor file(s):")
        for m in missing: print("   ", m)
        sys.exit(4)

    # states.csv header
    try:
        with open(args.state_csv, newline="") as f:
            reader = csv.reader(f)
            header = next(reader)
    except Exception as e:
        print(f"[error] cannot read state_csv: {e}", file=sys.stderr)
        sys.exit(5)

    header_norm = [h.strip().lower() for h in header]
    if header_norm != REQUIRED_COLS:
        print(f"[error] state_csv header mismatch.\n"
              f"  got : {header}\n"
              f"  want: {REQUIRED_COLS}")
        sys.exit(6)

    print("[ok] gate JSON, actors, and states.csv look good.")
    print("Run gate like:\n"
          f"  python3 consensus_gate_td3bc.py --gate {args.gate} --state_csv {args.state_csv} --device {args.device}")

if __name__ == "__main__":
    main()

=== ./live_dashboard.py ===

#!/usr/bin/env python3
import argparse, csv, os, sys, time, shutil, glob, re
from collections import namedtuple

# ---------- ANSI Palette ----------
def make_palette(bright=True, enabled=True):
    def c(code): 
        return f"\033[{code}m" if enabled else ""
    N = c(0)
    # base colors (dim/bright pairs)
    base = dict(
        R=("31", "91"),
        G=("32", "92"),
        Y=("33", "93"),
        B=("34", "94"),
        M=("35", "95"),
        C=("36", "96"),
        W=("37", "97"),
        K=("30", "90"),
    )
    def pick(key): 
        lo, hi = base[key]
        return c(hi if bright else lo)
    class C:
        reset = N
        bold  = c("1")
        dim   = c("2")
        inv   = c("7")
        R = pick("R"); G = pick("G"); Y = pick("Y")
        B = pick("B"); M = pick("M"); Cx = pick("C")
        W = pick("W"); K = pick("K")
        hdr = pick("Y")  # headers you liked (green boxes) => yellow
        ok  = pick("G"); warn = pick("Y"); bad = pick("R")
        cold = pick("C")
    return C

# ---------- Helpers ----------
def read_results_csv(path):
    rows = []
    if not os.path.exists(path): 
        return rows
    with open(path, newline="") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            # normalize numeric fields
            def ffloat(k, default=0.0):
                v = r.get(k, "")
                try: return float(v)
                except: return default
            r["_score"]  = ffloat("score")
            r["_eq_end"] = ffloat("equity_end")
            r["_sharpe"] = ffloat("Sharpe_like")
            r["_maxdd"]  = ffloat("MaxDD")
            r["_trades"] = int(ffloat("trades", 0))
            # optional fields may or may not exist
            r["_winp"]   = ffloat("win%") if "win%" in r else None
            r["_hitp"]   = ffloat("hit%") if "hit%" in r else None
            rows.append(r)
    rows.sort(key=lambda x: x["_score"], reverse=True)
    return rows

def latest_file(globpat):
    files = glob.glob(globpat)
    if not files: return None
    files.sort(key=lambda p: os.path.getmtime(p))
    return files[-1]

def compute_saturation(csv_path):
    neg1 = pos1 = mid = 0
    if not csv_path or not os.path.exists(csv_path): 
        return (0,0,0)
    with open(csv_path, newline="") as f:
        rdr = csv.reader(f)
        header = next(rdr, None)
        # expect: row_idx,agree,action,details  (action is col 2)
        for row in rdr:
            try:
                a = float(row[2])
            except:
                continue
            if a <= -0.99: neg1 += 1
            elif a >= 0.99: pos1 += 1
            else: mid += 1
    return (neg1, pos1, mid)

def term_width():
    try:
        return shutil.get_terminal_size().columns
    except:
        return 120

def bar():
    return "-" * min(120, term_width())

# --- Color thresholds for the KPIs you care about ---
def color_value(C, name, val):
    """Return (colored_text, plain_val_str)."""
    # where "val" is numeric, pick color by thresholds
    label = f"{C.W}{name}{C.reset}"
    try:
        x = float(val)
    except:
        return f"{label}={val}", str(val)

    if name in ("trade_winrate", "per_bar_hitrate"):
        # percent in [0,100]
        col = C.bad
        if x >= 55: col = C.ok
        elif x >= 50: col = C.warn
        return f"{label}={col}{x:.2f}%{C.reset}", f"{x:.2f}%"

    if name == "equity_end":
        # >1 good
        if x >= 1.01: col = C.ok
        elif x >= 1.0: col = C.warn
        else: col = C.bad
        return f"{label}={col}{x:.4f}{C.reset}", f"{x:.4f}"

    if name == "Sharpe_like":
        if x >= 0.3: col = C.ok
        elif x >= 0.1: col = C.warn
        else: col = C.bad
        return f"{label}={col}{x:.2f}{C.reset}", f"{x:.2f}"

    if name == "MaxDD":
        # lower is better (input as percent)
        if x <= 0.5: col = C.ok
        elif x <= 1.0: col = C.warn
        else: col = C.bad
        return f"{label}={col}{x:.2f}%{C.reset}", f"{x:.2f}%"

    if name == "exposure":
        # informational
        return f"{label}={C.cold}{x:.2f}%{C.reset}", f"{x:.2f}%"

    if name in ("avg_turnover",):
        return f"{label}={C.cold}{x:.4f}/bar{C.reset}", f"{x:.4f}/bar"

    if name in ("avg_trade_pnl","median_trade_pnl"):
        # faint color
        col = C.cold
        return f"{label}={col}{x:.6f}{C.reset}", f"{x:.6f}"

    # default
    return f"{label}={x}", str(x)

def color_pct(C, x):
    # used for top table mild coloring on win%/hit%
    if x is None: return "-"
    if x >= 55: col = C.ok
    elif x >= 50: col = C.warn
    else: col = C.bad
    return f"{col}{x:.2f}%{C.reset}"

# ---------- Renderers ----------
def print_header(C, txt):
    print(f"{C.hdr}{txt}{C.reset}")

def print_top_table(C, results, topn=12):
    print_header(C, f"-- TOP {topn} (composite score) from results.csv  |  total={len(results)} --")
    hdr = f" #   score   eq_end  Sharpe  MaxDD   trades   win%    hit%   k  eps  mag  th/ls  hold   gate_csv"
    print(hdr)
    print("-"*len(hdr))
    for i, r in enumerate(results[:topn], 1):
        winp = color_pct(C, r.get("_winp"))
        hitp = color_pct(C, r.get("_hitp"))
        k    = r.get("min_agree_k","")
        eps  = r.get("agree_eps","")
        mag  = r.get("min_mag","")
        thls = r.get("action_thresh","") or r.get("th/ls","")
        hold = r.get("hold_bars","")
        gate = r.get("gate_csv","")
        line = (
            f"{i:>2}  "
            f"{r['_score']:>5.3f}  "
            f"{r['_eq_end']:>6.4f}  "
            f"{r['_sharpe']:>6.2f}  "
            f"{r['_maxdd']:>5.2f}%  "
            f"{r['_trades']:>6d}  "
            f"{winp:>7}  {hitp:>7}  "
            f"{k:>2}  {eps:>4}  {mag:>4}  {str(thls):>5}  {str(hold):>4}  {gate}"
        )
        print(line)
    print()

def print_latest_gate_saturation(C, gates_dir):
    latest_gate = latest_file(os.path.join(gates_dir, "*.csv"))
    base = os.path.basename(latest_gate) if latest_gate else "N/A"
    print_header(C, f"-- latest gate saturation  --  {base}")
    n1, p1, mid = compute_saturation(latest_gate) if latest_gate else (0,0,0)
    print(f"neg=-1: {C.ok}{n1}{C.reset}   pos=+1: {C.ok}{p1}{C.reset}   mid: {C.warn}{mid}{C.reset}")
    print()

def print_log_tail(C, sweep_dir, lines=25):
    latest_log = latest_file(os.path.join(sweep_dir, "log_*.txt"))
    base = os.path.basename(latest_log) if latest_log else "N/A"
    print_header(C, f"-- tail of {base} (last {lines}) --")
    if not latest_log:
        print(f"{C.warn}[no logs yet]{C.reset}\n")
        return
    with open(latest_log, "r", errors="ignore") as f:
        tail = f.readlines()[-lines:]
    # show tail *except* the raw 'rows=' KPI line; we'll render it colored later
    for t in tail:
        if t.strip().startswith("rows="):
            continue
        print(f"{C.dim}{t.rstrip()}{C.reset}")
    print()
    return latest_log

def parse_kpis_from_logs(sweep_dir):
    """Find most recent KPI line 'rows=... trade_winrate=... per_bar_hitrate=...' in any log."""
    logs = glob.glob(os.path.join(sweep_dir, "log_*.txt"))
    if not logs: return None
    logs.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    pat = re.compile(
        r"rows=(?P<rows>\d+)\s+trades=(?P<trades>\d+)\s+trade_winrate=(?P<win>[\d\.]+)%\s+per_bar_hitrate=(?P<hit>[\d\.]+)%"
    )
    # other KPIs appear on next lines in most of your logs
    pat2 = re.compile(
        r"equity_end=(?P<eq>[\d\.]+)\s+Sharpe_like=(?P<sh>[\-\d\.]+)\s+MaxDD=(?P<dd>[\-\d\.]+)%\s+CAGR_like=(?P<cagr>[\-\d\.]+)%"
    )
    pat3 = re.compile(
        r"exposure=(?P<exp>[\-\d\.]+)%\s+avg_turnover=(?P<to>[\-\d\.]+)/bar\s+avg_trade_pnl=(?P<avgp>[\-\d\.]+)\s+median_trade_pnl=(?P<medp>[\-\d\.]+)"
    )
    for path in logs:
        rows = []
        with open(path, "r", errors="ignore") as f:
            rows = f.readlines()
        # scan backwards to find last KPI trio
        rows_rev = list(reversed(rows))
        got1=got2=got3=None
        for ln in rows_rev:
            if got3 is None:
                m3 = pat3.search(ln); 
                if m3: got3 = m3.groupdict()
                continue
        # continue scans separately to keep last matches
        for ln in rows_rev:
            if got2 is None:
                m2 = pat2.search(ln)
                if m2: got2 = m2.groupdict()
            if got1 is None:
                m1 = pat.search(ln)
                if m1: got1 = m1.groupdict()
            if got1 and got2: break
        if got1 and got2 and got3:
            return {
                "rows": int(got1["rows"]),
                "trades": int(got1["trades"]),
                "trade_winrate": float(got1["win"]),
                "per_bar_hitrate": float(got1["hit"]),
                "equity_end": float(got2["eq"]),
                "Sharpe_like": float(got2["sh"]),
                "MaxDD": float(got2["dd"]),
                "exposure": float(got3["exp"]),
                "avg_turnover": float(got3["to"]),
                "avg_trade_pnl": float(got3["avgp"]),
                "median_trade_pnl": float(got3["medp"]),
            }
    return None

def print_kpi_line(C, k):
    print_header(C, "-- backtest KPIs (colored) --")
    # first row (rows/trades/win/hit)
    parts1 = [
        f"rows={C.W}{k['rows']}{C.reset}",
        f"trades={C.W}{k['trades']}{C.reset}",
        color_value(C, "trade_winrate", k["trade_winrate"])[0],
        color_value(C, "per_bar_hitrate", k["per_bar_hitrate"])[0],
    ]
    print("  " + "  ".join(parts1))
    # second row (equity/sharpe/dd/cagr/exposure)
    parts2 = [
        color_value(C, "equity_end", k["equity_end"])[0],
        color_value(C, "Sharpe_like", k["Sharpe_like"])[0],
        color_value(C, "MaxDD", k["MaxDD"])[0],
        f"CAGR_like={C.cold}{k['exposure']*0+0:.2f}%{C.reset}".replace("0.00%","—"),  # placeholder if you don’t log CAGR here
        color_value(C, "exposure", k["exposure"])[0],
    ]
    print("  " + "  ".join(parts2))
    # third row (turnover/pnls)
    parts3 = [
        color_value(C, "avg_turnover", k["avg_turnover"])[0],
        color_value(C, "avg_trade_pnl", k["avg_trade_pnl"])[0],
        color_value(C, "median_trade_pnl", k["median_trade_pnl"])[0],
    ]
    print("  " + "  ".join(parts3))
    print()

# ---------- Main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--sweep", required=True, help="runs/sweep_* dir")
    ap.add_argument("--top", type=int, default=12)
    ap.add_argument("--refresh", type=float, default=2.0)
    ap.add_argument("--force-color", action="store_true")
    ap.add_argument("--dim-colors", action="store_true")
    args = ap.parse_args()

    sweep_dir = os.path.abspath(args.sweep)
    gates_dir = os.path.join(sweep_dir, "gates")
    results_csv = os.path.join(sweep_dir, "results.csv")
    now_csv = os.path.join(sweep_dir, "now.csv")  # optional live snapshot

    term_supports_color = sys.stdout.isatty()
    C = make_palette(bright=not args.dim_colors, enabled=(term_supports_color or args.force_color))

    try:
        while True:
            os.system("clear")
            print(f"{C.hdr}=== LIVE SWEEP DASHBOARD ==={C.reset}")
            print(f"{sweep_dir}")
            print()

            # now.csv (current trial) if present
            if os.path.exists(now_csv):
                print_header(C, "-- now.csv (current trial, last line) --")
                try:
                    with open(now_csv, "r") as f:
                        lines = [ln.strip() for ln in f if ln.strip()]
                        if lines:
                            print(lines[0])
                            if len(lines) > 1:
                                print(lines[-1])
                    print()
                except:
                    print(f"{C.warn}[now.csv unreadable]{C.reset}\n")
            else:
                print_header(C, "-- now.csv (not found) --")
                print()

            # Top table
            results = read_results_csv(results_csv)
            print_top_table(C, results, topn=args.top)

            # Latest gate saturation
            print_latest_gate_saturation(C, gates_dir)

            # Log tail (without raw KPI duplicate)
            latest_log = print_log_tail(C, sweep_dir, lines=25)

            # Colored KPI line from logs
            kpis = parse_kpis_from_logs(sweep_dir)
            if kpis:
                print_kpi_line(C, kpis)
            else:
                print(f"{C.warn}[no KPI line parsed yet]{C.reset}\n")

            print(f"{C.dim}[Ctrl+C to exit]{C.reset}")
            sys.stdout.flush()
            time.sleep(args.refresh)
    except KeyboardInterrupt:
        pass

if __name__ == "__main__":
    main()

=== ./per_nstep_replay.py ===

#!/usr/bin/env python3
# per_nstep_replay.py
# Prioritized Experience Replay with N-step returns (CPU, NumPy only).

from typing import Dict, Optional
import numpy as np

class PERNStepReplay:
    def __init__(
        self,
        capacity: int,
        state_dim: int,
        act_dim: int = 0,
        n_step: int = 1,
        gamma: float = 0.99,
        alpha: float = 0.6,
        beta: float = 0.4,
        eps: float = 1e-6,
        seed: int = 7,
    ):
        self.capacity = int(capacity)
        self.state_dim = int(state_dim)
        self.act_dim = int(act_dim)
        self.n_step = int(n_step)
        self.gamma = float(gamma)
        self.alpha = float(alpha)
        self.beta  = float(beta)
        self.eps   = float(eps)

        self.ptr = 0
        self.full = False
        self.rng = np.random.default_rng(seed)

        self.s  = np.zeros((self.capacity, self.state_dim), dtype=np.float32)
        self.sp = np.zeros((self.capacity, self.state_dim), dtype=np.float32)
        self.r  = np.zeros((self.capacity, 1), dtype=np.float32)
        self.d  = np.zeros((self.capacity, 1), dtype=np.float32)
        self.a  = None
        if self.act_dim > 0:
            self.a = np.zeros((self.capacity, self.act_dim), dtype=np.float32)

        self.prior = np.ones((self.capacity, 1), dtype=np.float32)
        self.max_prio = 1.0

    def size(self) -> int:
        return self.capacity if self.full else self.ptr

    def add(self, s, a, r, sp, d, priority: Optional[float] = None):
        i = self.ptr
        self.s[i]  = s
        self.sp[i] = sp
        self.r[i]  = r
        self.d[i]  = d
        if self.a is not None and a is not None:
            self.a[i] = a

        p = self.max_prio if priority is None else float(priority)
        p = max(self.eps, p)
        self.prior[i, 0] = p

        self.ptr = (self.ptr + 1) % self.capacity
        if self.ptr == 0:
            self.full = True

    def _n_step_target(self, idx: int):
        ret = 0.0
        g = 1.0
        i = idx
        n = self.size()
        for _ in range(self.n_step):
            if i >= n:
                break
            ret += g * float(self.r[i, 0])
            if self.d[i, 0] > 0.5:
                return ret, self.sp[i], True
            g *= self.gamma
            i += 1
        j = min(idx + self.n_step - 1, n - 1)
        return ret, self.sp[j], False

    def sample(self, batch_size: int) -> Dict[str, np.ndarray]:
        n = self.size()
        p = self.prior[:n, 0] ** self.alpha
        p /= np.sum(p)
        idx = self.rng.choice(n, size=batch_size, replace=True, p=p)

        w = (n * p[idx]) ** (-self.beta)
        w = (w / (np.max(w) + 1e-8)).astype(np.float32).reshape(-1, 1)

        S  = self.s[idx]
        A  = self.a[idx] if self.a is not None else None
        Rn = np.zeros((batch_size, 1), dtype=np.float32)
        SPn= np.zeros_like(self.sp[idx])
        Dn = np.zeros((batch_size, 1), dtype=np.float32)

        for t, ii in enumerate(idx):
            Rn[t, 0], SPn[t], done_flag = self._n_step_target(ii)
            Dn[t, 0] = 1.0 if done_flag else 0.0

        out = {"idx": idx, "w": w, "s": S, "r": Rn, "sp": SPn, "d": Dn}
        if A is not None:
            out["a"] = A
        return out

    def update_priorities(self, idx: np.ndarray, td_err: np.ndarray):
        td = np.abs(td_err).reshape(-1)
        for i, e in zip(idx, td):
            p = float(e) + self.eps
            self.prior[i, 0] = p
            if p > self.max_prio:
                self.max_prio = p

=== ./metrics_logger.py ===

import csv, os, time
from dataclasses import dataclass, asdict

@dataclass
class TrainRow:
    ts: float
    seed: int
    step: int
    q1: float
    q2: float
    pi: float

class MetricsLogger:
    def __init__(self, path: str):
        self.path = path
        self._wrote_header = os.path.exists(path) and os.path.getsize(path) > 0
        os.makedirs(os.path.dirname(path), exist_ok=True)

    def log(self, seed: int, step: int, q1: float, q2: float, pi: float):
        row = TrainRow(time.time(), seed, step, float(q1), float(q2), float(pi))
        write_header = not self._wrote_header
        with open(self.path, "a", newline="") as f:
            w = csv.DictWriter(f, fieldnames=[k for k in asdict(row).keys()])
            if write_header:
                w.writeheader()
                self._wrote_header = True
            w.writerow(asdict(row))

=== ./rl_utils.py ===

#!/usr/bin/env python3
import math
import torch
import torch.nn as nn

def fanin_init(m):
    if isinstance(m, nn.Linear):
        bound = 1.0 / math.sqrt(m.weight.size(0))
        nn.init.uniform_(m.weight, -bound, +bound)
        if m.bias is not None:
            nn.init.zeros_(m.bias)

class MLP(nn.Module):
    def __init__(self, in_dim, out_dim, hidden=(128, 128), act=nn.ReLU, out_act=None):
        super().__init__()
        layers = []
        last = in_dim
        for h in hidden:
            layers += [nn.Linear(last, h), act()]
            last = h
        layers.append(nn.Linear(last, out_dim))
        if out_act is not None:
            layers.append(out_act())
        self.net = nn.Sequential(*layers)
        self.apply(fanin_init)

    def forward(self, x):
        return self.net(x)

def polyak_update(target: nn.Module, online: nn.Module, tau: float):
    with torch.no_grad():
        for tp, p in zip(target.parameters(), online.parameters()):
            tp.data.mul_(1.0 - tau).add_(p.data, alpha=tau)

=== ./runs/snap_20251005_041723/rl_duckdb_data_layer.py ===

# rl_duckdb_data_layer.py (v1.5 — reward scaled by 1000x)
# - Timestamp: supports 'ts' or 'timestamp' (TIMESTAMP)
# - L1 fields: bid_px_1 / ask_px_1 / bid_sz_1 / ask_sz_1
# - Features: mid, spread, imbalance, mom1, mom3, vol3
# - Next-state (H bars ahead): mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p
# - Reward: fwd_ret_raw = (mid_p - mid)/mid ; fwd_ret = 1000 * fwd_ret_raw
# - union_by_name=true to tolerate schema drift across days/files

def build_sql(parquet_glob: str,
              start: str | None,
              end: str | None,
              h: int,
              th: float,
              drop_noise: bool) -> str:
    # Optional time filter
    time_filter = ""
    if start and end:
        time_filter = f"WHERE ts >= TIMESTAMP '{start}' AND ts <= TIMESTAMP '{end}'"
    elif start:
        time_filter = f"WHERE ts >= TIMESTAMP '{start}'"
    elif end:
        time_filter = f"WHERE ts <= TIMESTAMP '{end}'"

    return f"""
WITH raw AS (
  SELECT
    COALESCE(TRY_CAST("ts" AS TIMESTAMP), TRY_CAST("timestamp" AS TIMESTAMP)) AS ts,
    CAST("bid_px_1" AS DOUBLE) AS bid_px_1,
    CAST("ask_px_1" AS DOUBLE) AS ask_px_1,
    CAST("bid_sz_1" AS DOUBLE) AS bid_sz_1,
    CAST("ask_sz_1" AS DOUBLE) AS ask_sz_1
  FROM read_parquet('{parquet_glob}', union_by_name=true)
),
lvl AS (
  SELECT *
  FROM raw
  WHERE ts IS NOT NULL
    AND bid_px_1 IS NOT NULL AND ask_px_1 IS NOT NULL
),
feat AS (
  SELECT
    ts,
    (ask_px_1 + bid_px_1)/2.0 AS mid,
    (ask_px_1 - bid_px_1)     AS spread,
    CASE
      WHEN (bid_sz_1 + ask_sz_1) > 0
      THEN (bid_sz_1 - ask_sz_1) / NULLIF(bid_sz_1 + ask_sz_1, 0)
      ELSE NULL
    END AS imbalance
  FROM lvl
),
lags AS (
  SELECT
    ts, mid, spread, imbalance,
    LAG(mid, 1) OVER (ORDER BY ts) AS mid_l1,
    LAG(mid, 3) OVER (ORDER BY ts) AS mid_l3
  FROM feat
),
rets AS (
  SELECT
    ts, mid, spread, imbalance, mid_l1, mid_l3,
    CASE WHEN mid_l1 IS NULL OR mid_l1 = 0 THEN NULL ELSE (mid - mid_l1)/mid_l1 END AS ret1,
    CASE WHEN mid_l3 IS NULL OR mid_l3 = 0 THEN NULL ELSE (mid - mid_l3)/mid_l3 END AS ret3
  FROM lags
),
vol AS (
  SELECT
    ts, mid, spread, imbalance,
    ret1, ret3,
    /* 3-bar rolling stddev of 1-bar returns */
    STDDEV_SAMP(ret1) OVER (ORDER BY ts ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS vol3
  FROM rets
),
lead_h AS (
  SELECT
    ts, mid, spread, imbalance, ret1 AS mom1, ret3 AS mom3, vol3,
    /* H-step lookahead */
    LEAD(mid,        {h}) OVER (ORDER BY ts) AS mid_p,
    LEAD(spread,     {h}) OVER (ORDER BY ts) AS spread_p,
    LEAD(imbalance,  {h}) OVER (ORDER BY ts) AS imbalance_p,
    LEAD(ret1,       {h}) OVER (ORDER BY ts) AS mom1_p,
    LEAD(ret3,       {h}) OVER (ORDER BY ts) AS mom3_p,
    LEAD(vol3,       {h}) OVER (ORDER BY ts) AS vol3_p
  FROM vol
),
aug AS (
  SELECT
    ts, mid, spread, imbalance, mom1, mom3, vol3,
    mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
    CASE WHEN mid IS NULL OR mid = 0 OR mid_p IS NULL THEN NULL
         ELSE (mid_p - mid) / mid
    END AS fwd_ret_raw
  FROM lead_h
),
scale AS (
  SELECT
    ts, mid, spread, imbalance, mom1, mom3, vol3,
    mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
    /* Scale reward to strengthen gradients (does not change optimal policy) */
    1000.0 * fwd_ret_raw AS fwd_ret
  FROM aug
),
flt AS (
  SELECT * FROM scale
  {time_filter}
)
SELECT
  ts, mid, spread, imbalance, mom1, mom3, vol3,
  mid_p, spread_p, imbalance_p, mom1_p, mom3_p, vol3_p,
  fwd_ret
FROM flt
ORDER BY ts
"""

=== ./analyze_live_log.py ===

#!/usr/bin/env python3
import csv, sys, pathlib, statistics as st
from datetime import datetime

def to_float(x, default=None):
    try: return float(x)
    except: return default

def parse_rows(path):
    rows=[]
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        if not r.fieldnames:
            return rows
        for row in r:
            # unify keys across old/new schemas
            ts = row.get("ts") or row.get("time") or ""
            prev = int(float(row.get("prev_pos", "0") or 0))
            nxt  = int(float(row.get("next_pos", "0") or 0))
            raw  = to_float(row.get("raw_action", row.get("action", "0")), 0.0)
            # thresholds: new has buy_th/sell_th; old had th
            buy_th  = to_float(row.get("buy_th", row.get("th", None)), None)
            sell_th = to_float(row.get("sell_th", row.get("th", None)), None)
            hold_bars = int(float(row.get("hold_bars", "0") or 0))
            hold_ctr  = int(float(row.get("hold_counter", row.get("held", "0") or 0)))
            spread = to_float(row.get("spread", None))
            vol3   = to_float(row.get("vol3", None))
            skipped = row.get("skipped_reason", "")
            rows.append(dict(ts=ts, prev=prev, nxt=nxt, raw=raw, buy_th=buy_th,
                             sell_th=sell_th, hold_bars=hold_bars, hold_ctr=hold_ctr,
                             spread=spread, vol3=vol3, skipped=skipped))
    return rows

def summarize(rows, last_n=20):
    if not rows:
        print("[empty] no rows"); return
    # basic counts
    flips = sum(1 for r in rows if r["nxt"] != r["prev"])
    entries = sum(1 for i,r in enumerate(rows) if (r["prev"]==0 and r["nxt"]!=0))
    exits   = sum(1 for i,r in enumerate(rows) if (r["prev"]!=0 and r["nxt"]==0))
    inpos_bars = sum(1 for r in rows if r["nxt"]!=0)
    pos_frac = inpos_bars / len(rows)

    # segment holds
    segs=[]; start=0; cur=rows[0]["nxt"]
    for i in range(1,len(rows)):
        if rows[i]["nxt"] != cur:
            segs.append((start, i-1, cur))
            start = i; cur = rows[i]["nxt"]
    segs.append((start, len(rows)-1, cur))
    holds=[(hi-lo+1) for (lo,hi,sign) in segs if sign!=0]
    avg_hold = st.mean(holds) if holds else 0
    med_hold = st.median(holds) if holds else 0

    # action stats while in position
    acts=[abs(r["raw"]) for r in rows if r["nxt"]!=0]
    act_mean = st.mean(acts) if acts else 0
    act_p95  = (sorted(acts)[int(0.95*(len(acts)-1))] if acts else 0)

    # skips
    skip_counts={}
    for r in rows:
        s=r["skipped"]
        if s:
            skip_counts[s]=skip_counts.get(s,0)+1

    print("[live summary]")
    print(f" rows={len(rows)}  flips={flips}  entries={entries}  exits={exits}")
    print(f" in_position_bars={inpos_bars}  exposure={pos_frac:.2%}")
    print(f" hold_bars(avg/med)={avg_hold:.1f}/{med_hold:.1f}")
    print(f" |action|(mean/p95)={act_mean:.3f}/{act_p95:.3f}")
    if skip_counts:
        print(" skips:", "  ".join(f"{k}={v}" for k,v in skip_counts.items()))
    # last N lines
    print("\n[last {} rows]".format(min(last_n,len(rows))))
    hdr = "ts,prev_pos,next_pos,raw_action,buy_th,sell_th,hold_bars,hold_counter,spread,vol3,skipped_reason"
    print(hdr)
    for r in rows[-last_n:]:
        print("{},{},{},{:.6f},{},{},{},{},{},{}{}".format(
            r["ts"], r["prev"], r["nxt"], r["raw"],
            "" if r["buy_th"] is None else f"{r['buy_th']:.2f}",
            "" if r["sell_th"] is None else f"{r['sell_th']:.2f}",
            r["hold_bars"], r["hold_ctr"],
            "" if r["spread"] is None else f"{r['spread']:.6f}",
            "" if r["vol3"]   is None else f"{r['vol3']:.6f}",
            f",{r['skipped']}" if r["skipped"] else ""
        ))

if __name__ == "__main__":
    p = pathlib.Path("live_signals.csv")
    if not p.exists():
        sys.exit("live_signals.csv not found")
    rows = parse_rows(str(p))
    summarize(rows, last_n=20)

=== ./train_iql_from_duckdb.py ===

#!/usr/bin/env python3
# train_iql_from_duckdb.py
# Offline IQL training directly from Parquet via DuckDB feeder.

import argparse
import time
import numpy as np
import torch

from rl_duckdb_data_layer import build_sql, stream_into_buffer, ReplayBuffer
import duckdb
from iql_agent import IQL

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    ap.add_argument("--buf", type=int, default=1_500_000)
    ap.add_argument("--chunk", type=int, default=200_000)
    ap.add_argument("--drop_noise", action="store_true")
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--steps_per_epoch", type=int, default=6000)
    ap.add_argument("--batch", type=int, default=512)
    ap.add_argument("--seed", type=int, default=123)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--expectile", type=float, default=0.7)
    ap.add_argument("--beta", type=float, default=3.0)
    args = ap.parse_args()

    rng = np.random.default_rng(args.seed)
    torch.manual_seed(args.seed)

    con = duckdb.connect(database=":memory:")
    sql = build_sql(args.parquet, args.start, args.end, args.h, args.th, None)

    # Actions required: derive from labels/sign so IQL sees (s,a,r,s')
    buf = ReplayBuffer(capacity=args.buf, state_dim=6, act_dim=1)
    total = stream_into_buffer(
        con, sql, buf,
        chunk_rows=args.chunk,
        drop_noise_label=args.drop_noise,
        derive_action=True
    )
    print(f"[ingest] transitions loaded: {total}, buffer.size={buf.size()}")
    assert buf.size() > args.batch, "Not enough samples to train."

    agent = IQL(
        s_dim=6, a_dim=1,
        gamma=0.99, tau=0.005,
        expectile=args.expectile, beta=args.beta,
        actor_lr=3e-4, q_lr=3e-4, v_lr=3e-4,
        device=args.device, act_limit=1.0
    )

    for ep in range(1, args.epochs + 1):
        t0 = time.time()
        for t in range(1, args.steps_per_epoch + 1):
            batch_np = buf.sample(args.batch, rng)
            batch = {k: torch.from_numpy(v).to(args.device) for k, v in batch_np.items()}
            stats = agent.update(batch)

            if t % 200 == 0 or t == args.steps_per_epoch:
                print(f"[ep {ep}/{args.epochs}] step {t}/{args.steps_per_epoch} "
                      f"| q1={stats['q1_loss']:.5f} q2={stats['q2_loss']:.5f} "
                      f"v={stats['v_loss']:.5f} pi={stats['pi_loss']:.5f}")

        dur = time.time() - t0
        print(f"[ep {ep}] done in {dur:.1f}s")

    torch.save(agent.pi.state_dict(), "iql_policy.pt")
    torch.save(agent.v.state_dict(),  "iql_value.pt")
    print("Saved: iql_policy.pt  iql_value.pt")

if __name__ == "__main__":
    main()

=== ./train_with_custom_datalayer.py ===

#!/usr/bin/env python3
# Shim runner: use adapters/mexc15s/rl_duckdb_data_layer_custom.py
# Patch it to:
#  - allow both "ts" and "timestamp"
#  - add union_by_name=true so mixed schemas in a glob work

import os, sys, importlib, re

ADAPTER_DIR = os.path.join(os.path.dirname(__file__), "adapters", "mexc15s")
sys.path.insert(0, ADAPTER_DIR)

custom = importlib.import_module("rl_duckdb_data_layer_custom")

# 1) Robust timestamp: handle "ts" OR "timestamp"
custom.TS_EXPR = (
    'COALESCE('
    'TRY_CAST("ts" AS TIMESTAMP), '
    'TRY_CAST("timestamp" AS TIMESTAMP), '
    'to_timestamp(CAST(TRY_CAST("ts" AS BIGINT) AS DOUBLE)/1000.0), '
    'to_timestamp(CAST(TRY_CAST("timestamp" AS BIGINT) AS DOUBLE)/1000.0)'
    ')'
)

# 2) Wrap build_sql to inject union_by_name=true in read_parquet(...)
_old_build_sql = custom.build_sql
def _build_sql_union(*args, **kwargs):
    sql = _old_build_sql(*args, **kwargs)
    # Add union_by_name=true to every read_parquet call; keep filename=true intact
    sql = re.sub(
        r"read_parquet\(\s*'([^']*)'\s*,\s*filename\s*=\s*true\s*\)",
        r"read_parquet('\1', filename=true, union_by_name=true)",
        sql,
        flags=re.IGNORECASE,
    )
    return sql
custom.build_sql = _build_sql_union

# 3) Make trainer import our patched module under the expected name
sys.modules["rl_duckdb_data_layer"] = custom

# 4) Run the trainer with original CLI args
import train_td3bc_per_ensemble as trainer
if hasattr(trainer, "main"):
    trainer.main()
else:
    with open(trainer.__file__, "rb") as f:
        code = compile(f.read(), trainer.__file__, "exec")
        exec(code, {"__name__": "__main__"})

=== ./live_executor_hold_v2_2.py ===

#!/usr/bin/env python3
# live_executor_hold_v2_2.py
# - Same logic as v2.1, plus:
#   * logs mid
#   * persists entry_mid and entry_ts in statefile on entries
#   * prints all fields needed for a human to understand "what's going on"

import argparse, json, os, subprocess, sys, datetime as dt

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        raise RuntimeError(p.stderr.strip())
    return p.stdout.strip()

def parse_state_row(row_csv):
    # "mid,spread,imbalance,mom1,mom3,vol3" (no header)
    vals = [float(x.strip()) for x in row_csv.split(",")]
    if len(vals) != 6:
        raise ValueError("state_row must have 6 numeric fields")
    return dict(mid=vals[0], spread=vals[1], imbalance=vals[2], mom1=vals[3], mom3=vals[4], vol3=vals[5])

def now_utc_iso(timespec: str = "milliseconds") -> str:
    return dt.datetime.now(dt.timezone.utc).isoformat(timespec=timespec)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--buy_th", type=float, default=0.10)
    ap.add_argument("--sell_th", type=float, default=0.10)
    ap.add_argument("--hold_bars", type=int, default=12)
    ap.add_argument("--max_hold_bars", type=int, default=160)
    ap.add_argument("--spread_cap", type=float, default=5.0)
    ap.add_argument("--min_vol3", type=float, default=0.0)
    ap.add_argument("--statefile", default="live_state.json")
    ap.add_argument("--log", default="live_signals.csv")
    ap.add_argument("--ts_timespec", choices=["seconds","milliseconds"], default="milliseconds")
    args = ap.parse_args()

    # 1) Latest features
    row = run([sys.executable, "latest_state_from_parquet.py", "--parquet", args.parquet])
    feat = parse_state_row(row)  # has mid, spread, vol3, ...

    # 2) Gate (sign consensus)
    min_mag = min(args.buy_th, args.sell_th)
    out = run([
        sys.executable, "consensus_gate_live_plus.py",
        "--gate", args.gate, "--scaler", args.scaler,
        "--state_row", row, "--device", args.device,
        "--consensus-mode", "sign", "--min-agree-k", "2",
        "--min-mag", f"{min_mag}"
    ])
    last = [ln for ln in out.splitlines() if ln and ln[0].isdigit()][-1]
    parts = last.split(",")
    agree = int(parts[1]); raw_action = float(parts[2])

    # 3) intent (asymmetric thresholds)
    intent = 0
    if raw_action >= args.buy_th: intent = +1
    elif raw_action <= -args.sell_th: intent = -1

    # 4) load state; now includes entry tracking
    st = {"prev_pos": 0, "bars_held": 0, "bars_held_total": 0,
          "entry_mid": None, "entry_ts": None}
    if os.path.isfile(args.statefile):
        try:
            with open(args.statefile, "r") as f: st.update(json.load(f))
        except: pass
    prev_pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))
    held_total = int(st.get("bars_held_total", 0))
    entry_mid = st.get("entry_mid", None)
    entry_ts  = st.get("entry_ts", None)

    # 5) guards for NEW entries
    skipped_reason = ""
    if prev_pos == 0 and intent != 0:
        if feat["spread"] > args.spread_cap:
            skipped_reason = f"spread>{args.spread_cap}"
            intent = 0
        elif feat["vol3"] < args.min_vol3:
            skipped_reason = f"vol3<{args.min_vol3}"
            intent = 0

    # 6) Enforce hold rules and manage entry tracking
    next_pos = intent
    if next_pos != prev_pos:
        # flip or enter/exit
        if prev_pos != 0 and next_pos != 0 and held < args.hold_bars:
            # too soon to flip
            next_pos = prev_pos
            held += 1; held_total += 1
        else:
            # change allowed
            if next_pos == 0:
                # exit
                held = 0; held_total = 0
                entry_mid = None; entry_ts = None
            else:
                # new entry
                held = 0; held_total = 0
                entry_mid = feat["mid"]; entry_ts = now_utc_iso(args.ts_timespec)
    else:
        # no change
        if next_pos != 0:
            held = min(held + 1, args.hold_bars)
            held_total += 1
            if held_total >= args.max_hold_bars:
                # force flatten
                next_pos = 0
                skipped_reason = (skipped_reason + "; " if skipped_reason else "") + "max_hold_cap"
                held = 0; held_total = 0
                entry_mid = None; entry_ts = None

    # 7) unrealized PnL estimate (in bps) using mids, if in position
    upnl_bps = ""
    if entry_mid is not None and next_pos != 0:
        ret = (feat["mid"] - entry_mid) / entry_mid
        upnl = ret * (1 if next_pos>0 else -1)
        upnl_bps = f"{upnl*10000:.2f}"

    # 8) persist
    with open(args.statefile, "w") as f:
        json.dump({
            "prev_pos": int(next_pos),
            "bars_held": int(held),
            "bars_held_total": int(held_total),
            "entry_mid": entry_mid,
            "entry_ts": entry_ts
        }, f)

    # 9) log
    ts = now_utc_iso(args.ts_timespec)
    header = ("ts,prev_pos,next_pos,raw_action,buy_th,sell_th,hold_bars,hold_counter,"
              "mid,spread,vol3,entry_mid,entry_ts,upnl_bps,skipped_reason\n")
    line = (f"{ts},{prev_pos},{next_pos},{raw_action:.6f},{args.buy_th:.2f},{args.sell_th:.2f},"
            f"{args.hold_bars},{held},{feat['mid']:.2f},{feat['spread']:.6f},{feat['vol3']:.6f},"
            f"{'' if entry_mid is None else f'{entry_mid:.2f}'},"
            f"{'' if entry_ts is None else entry_ts},"
            f"{upnl_bps},{skipped_reason}")
    print(line)
    if not os.path.isfile(args.log):
        with open(args.log, "w") as f: f.write(header)
    with open(args.log, "a") as f: f.write(line + "\n")

if __name__ == "__main__":
    main()

=== ./export_scaler_from_states.py ===

#!/usr/bin/env python3
# export_scaler_from_states.py — reads states.csv, writes scaler.json with per-feature mean/std
import argparse, csv, json
import numpy as np

COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--state_csv", required=True)
    ap.add_argument("--out", default="scaler.json")
    args = ap.parse_args()

    X = []
    with open(args.state_csv, newline="") as f:
        r = csv.DictReader(f)
        hdr = [h.strip().lower() for h in (r.fieldnames or [])]
        if hdr != COLS:
            raise SystemExit(f"header mismatch; got {r.fieldnames}, want {COLS}")
        for row in r:
            try:
                X.append([float(row[c]) for c in COLS])
            except:
                pass
    if not X:
        raise SystemExit("no rows in states.csv")
    X = np.asarray(X, dtype=np.float64)
    mu = X.mean(axis=0)
    sd = X.std(axis=0)
    sd = np.where(sd < 1e-8, 1.0, sd)

    scaler = {"cols": COLS, "mean": mu.tolist(), "std": sd.tolist(), "type": "zscore"}
    with open(args.out, "w") as f:
        json.dump(scaler, f, indent=2)
    print(f"[ok] wrote {args.out}")
    print("[mean]", [round(x,4) for x in mu.tolist()])
    print("[std ]", [round(x,4) for x in sd.tolist()])

if __name__ == "__main__":
    main()

=== ./view_sweep_v2.py ===

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse, csv, os, sys, glob
from pathlib import Path

REQ_COLS = ["equity_end", "Sharpe_like", "MaxDD", "trades", "exposure",
            "min_agree_k", "agree_eps", "min_mag", "action_thresh", "linear_scale",
            "hold_bars", "mode", "gate_csv"]

def to_float(x, default=0.0):
    try:
        if x is None or x == "": return default
        return float(x)
    except Exception:
        return default

def to_int(x, default=0):
    try:
        if x is None or x == "": return default
        return int(float(x))
    except Exception:
        return default

def score(row):
    eq = to_float(row.get("equity_end"), 1.0)
    sh = to_float(row.get("Sharpe_like"), 0.0)
    dd = to_float(row.get("MaxDD"), 0.0)
    ex = to_float(row.get("exposure"), 0.0)
    tr = to_int(row.get("trades"), 0)
    if tr == 0:
        return -1e9
    # composite: emphasize equity, then Sharpe; penalize large DD or >95% exposure
    s = (eq - 1.0) * 100.0 + 0.5 * sh - 0.1 * dd - (1.0 if ex > 95.0 else 0.0)
    return s

def latest_sweep_dir(base="runs"):
    # prefer 'sweep_*' then newest lexicographically
    paths = sorted(glob.glob(os.path.join(base, "sweep_*")), reverse=True)
    return Path(paths[0]) if paths else None

def ensure_results_path(sweep_path: Path):
    res = sweep_path / "results.csv"
    if not res.exists():
        sys.exit(f"[error] results.csv not found: {res}")
    return res

def read_results(res_csv: Path):
    with res_csv.open(newline="") as f:
        rdr = csv.DictReader(f)
        rows = list(rdr)
    if not rows:
        sys.exit(f"[error] results.csv is empty: {res_csv}")
    # normalize keys (if someone wrote weird header casing)
    rows_norm = []
    for r in rows:
        nr = {k.strip(): v for k, v in r.items()}
        # Backward compatibility: accept alternative keys if present
        if "MaxDD" not in nr and "maxdd" in nr: nr["MaxDD"] = nr["maxdd"]
        if "Sharpe_like" not in nr and "Sharpe" in nr: nr["Sharpe_like"] = nr["Sharpe"]
        if "gate_csv" not in nr and "gate" in nr: nr["gate_csv"] = nr["gate"]
        rows_norm.append(nr)
    return rows_norm

def print_top(rows, n):
    ranked = sorted(rows, key=score, reverse=True)
    n = min(n, len(ranked))
    print(f"=== TOP {n} by composite score ===  ({len(rows)} total rows)\n")
    print(" #  score   eq_end  Sharpe  MaxDD  trades  k  eps    mag    th/ls      hold  gate_csv")
    print("-" * 130)
    for i, r in enumerate(ranked[:n], 1):
        eq = to_float(r.get("equity_end"), 1.0)
        sh = to_float(r.get("Sharpe_like"), 0.0)
        dd = to_float(r.get("MaxDD"), 0.0)
        tr = to_int(r.get("trades"), 0)
        k  = r.get("min_agree_k", "")
        eps = r.get("agree_eps", "")
        mag = r.get("min_mag", "")
        th  = r.get("action_thresh", "")
        ls  = r.get("linear_scale", "")
        tl  = f"th={th}" if (th not in (None, "", "0")) else f"ls={ls}" if ls not in (None, "") else "-"
        hold = r.get("hold_bars", "")
        gate = r.get("gate_csv", "")
        print(f"{i:2d}  {score(r):6.3f}  {eq:.4f}  {sh:6.2f}  {dd:5.2f}%  {tr:6d}  {k:>1}  {str(eps):<5}  {str(mag):<5}  {tl:<9} {str(hold):>4}  {gate}")
    print()

def saturate_counts(gate_csv: Path):
    neg = pos = mid = 0
    try:
        with gate_csv.open() as f:
            header = next(f, None)
            for line in f:
                parts = line.strip().split(",")
                if len(parts) < 3:
                    continue
                try:
                    a = float(parts[2])
                except Exception:
                    continue
                if a <= -0.99: neg += 1
                elif a >= 0.99: pos += 1
                else: mid += 1
    except FileNotFoundError:
        return None
    return neg, pos, mid

def list_gates(sweep_dir: Path, show_sat=False, limit=20):
    gdir = sweep_dir / "gates"
    gates = sorted(gdir.glob("gate_*.csv"))
    print(f"=== GATES ({len(gates)}) in {gdir} ===")
    for i, g in enumerate(gates[:limit], 1):
        if show_sat:
            sp = saturate_counts(g)
            if sp is None:
                print(f"{i:3d}. {g.name:<60}  [missing]")
            else:
                n1, p1, md = sp
                print(f"{i:3d}. {g.name:<60}  sat: neg≈-1 {n1:4d}  +1 {p1:4d}  mid {md:4d}")
        else:
            print(f"{i:3d}. {g.name}")
    if len(gates) > limit:
        print(f"... (+{len(gates) - limit} more)")
    print()

def find_log_for_gate(sweep_dir: Path, gate_name: str):
    tag = gate_name.replace("gate_", "").replace(".csv", "")
    # flexible matching
    cands = list(sweep_dir.glob(f"log_{tag}.txt"))
    if cands:
        return cands[0]
    # fallback: any log that contains all param tokens
    tokens = [t for t in tag.split("_") if t]
    for p in sorted(sweep_dir.glob("log_*.txt")):
        name = p.name.replace("log_", "").replace(".txt", "")
        if all(tok in name for tok in tokens):
            return p
    return None

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--sweep", default=None, help="path to runs/sweep_* folder; if omitted, auto-pick latest")
    ap.add_argument("--top", type=int, default=10, help="top K")
    ap.add_argument("--show-sat", action="store_true", help="show saturation for first 20 gates")
    ap.add_argument("--gate", default=None, help="gate file name (under sweep/gates) to inspect")
    ap.add_argument("--tail", type=int, default=30, help="tail lines for the gate log")
    args = ap.parse_args()

    sweep = Path(args.sweep).resolve() if args.sweep else latest_sweep_dir("runs")
    if not sweep or not sweep.exists():
        sys.exit("[error] could not locate a sweep folder; pass --sweep runs/sweep_* explicitly")

    res = ensure_results_path(sweep)
    rows = read_results(res)

    print(f"[info] sweep: {sweep}")
    print_top(rows, args.top)
    list_gates(sweep, show_sat=args.show_sat)

    if args.gate:
        g = (sweep / "gates" / args.gate)
        if not g.exists():
            sys.exit(f"[error] gate not found under {sweep/'gates'}: {args.gate}")
        sp = saturate_counts(g)
        if sp is None:
            print(f"[error] cannot open gate csv: {g}")
        else:
            n1, p1, md = sp
            print(f"=== {g.name} saturation ===")
            print(f"neg≈-1: {n1}  pos≈+1: {p1}  mid: {md}\n")
        log = find_log_for_gate(sweep, g.name)
        if log and log.exists():
            print(f"=== tail {args.tail} lines of {log.name} ===")
            with log.open() as f:
                lines = f.readlines()
            for ln in lines[-args.tail:]:
                sys.stdout.write(ln)
        else:
            print("[info] no matching log file found for that gate.")

=== ./backtest_gate_horizon_v2.py ===

#!/usr/bin/env python3
# backtest_gate_horizon_v2.py
# - Always prints a result block (or a clear error) and flushes stdout
# - Extra header tolerance for gate CSV
# - Verbose mode shows row counts and a few sample actions/returns

import argparse, csv, statistics as st, sys

def read_states(path):
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        hdr = [h.strip().lower() for h in (r.fieldnames or [])]
        need = ["mid","spread","imbalance","mom1","mom3","vol3"]
        if hdr != need:
            raise SystemExit(f"[error] states header mismatch: got {r.fieldnames}, want {need}")
        mids=[]
        ok=0; bad=0
        for row in r:
            try:
                mids.append(float(row["mid"])); ok+=1
            except:
                bad+=1
        if ok < 3:
            raise SystemExit(f"[error] states has too few usable rows: ok={ok}, bad={bad}")
        return mids, ok, bad

def read_gate(path):
    # Accept headers like: row_idx, agree, action, max_pair_diff, (min_q|details)
    with open(path, newline="") as f:
        r = csv.DictReader(f)
        if not r.fieldnames:
            raise SystemExit("[error] gate csv has no header")
        hdr = [h.strip().lower() for h in r.fieldnames]
        if "agree" not in hdr or "action" not in hdr:
            raise SystemExit(f"[error] gate header missing 'agree'/'action': {r.fieldnames}")
        acts=[]
        ok=0; bad=0
        for row in r:
            try:
                agree = int(row.get("agree", "0"))
                action = float(row.get("action","0"))
                acts.append(action if agree==1 else 0.0)
                ok+=1
            except:
                bad+=1
        if ok == 0:
            raise SystemExit("[error] gate has zero usable rows")
        return acts, ok, bad

def metrics(equity):
    rets=[equity[i]/equity[i-1]-1.0 for i in range(1,len(equity))]
    if not rets: return {"Sharpe_like": 0.0, "MaxDD": 0.0, "CAGR_like": 0.0}
    mu = st.mean(rets)
    sd = st.pstdev(rets) if len(rets)>1 else 0.0
    sharpe = (mu/sd) if sd>1e-12 else 0.0
    peak=equity[0]; maxdd=0.0
    for x in equity:
        if x>peak: peak=x
        dd = peak/x - 1.0
        if dd>maxdd: maxdd=dd
    return {"CAGR_like": equity[-1]/equity[0]-1.0, "Sharpe_like": sharpe, "MaxDD": maxdd}

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--action_thresh", type=float, default=0.08)
    ap.add_argument("--linear_scale", type=float, default=1.0)
    ap.add_argument("--hold_bars", type=int, default=8)
    ap.add_argument("--fee_bps", type=float, default=5.0)
    ap.add_argument("--verbose", action="store_true")
    args=ap.parse_args()

    mids, s_ok, s_bad = read_states(args.states)
    acts, g_ok, g_bad = read_gate(args.gate)

    n = min(len(acts), len(mids) - args.h)
    if n <= 0:
        print(f"[error] insufficient overlap: len(acts)={len(acts)} len(mids)={len(mids)} H={args.h}", flush=True)
        sys.exit(0)

    # H-step returns
    rets = [ (mids[t+args.h]-mids[t]) / mids[t] for t in range(n) ]

    # intended positions
    raw_pos=[0.0]*n
    if args.mode=="sign":
        th=args.action_thresh
        for t,a in enumerate(acts[:n]):
            raw_pos[t] = 1.0 if a>=th else (-1.0 if a<=-th else 0.0)
    else:
        sc=args.linear_scale
        for t,a in enumerate(acts[:n]):
            v = max(-1.0, min(1.0, a*sc))
            raw_pos[t]=v

    # min holding
    pos=[0.0]*n
    if n>0:
        pos[0]=raw_pos[0]
        last_change=0
        for t in range(1,n):
            if raw_pos[t] != pos[t-1] and (t - last_change) < args.hold_bars:
                pos[t] = pos[t-1]
            else:
                if raw_pos[t] != pos[t-1]:
                    last_change = t
                pos[t] = raw_pos[t]

    fee_rate = args.fee_bps/10000.0
    equity=[1.0]; trades=0; wins=0; prev_pos=0.0
    for t in range(n):
        turnover = abs(pos[t]-prev_pos)
        fee = turnover * fee_rate
        r = pos[t]*rets[t] - fee
        equity.append(equity[-1]*(1.0+r))
        if pos[t]!=prev_pos:
            trades += 1
        if pos[t]!=0.0 and r>0:
            wins += 1
        prev_pos = pos[t]

    m = metrics(equity)
    winrate = (wins/trades) if trades>0 else 0.0

    if args.verbose:
        print(f"[debug] states ok/bad={s_ok}/{s_bad}  gate ok/bad={g_ok}/{g_bad}  n={n}", flush=True)
        # sample few actions/returns
        import itertools
        aa = list(itertools.islice(acts, 0, 5))
        rr = list(itertools.islice(rets, 0, 5))
        print(f"[debug] sample acts={ [round(x,4) for x in aa] }  rets={ [round(x,6) for x in rr] }", flush=True)

    print("[backtest H]", flush=True)
    print(f" rows={n}  trades={trades}  winrate={winrate:.2%}", flush=True)
    print(f" H={args.h}  hold_bars={args.hold_bars}  mode={args.mode}  th={args.action_thresh if args.mode=='sign' else None}  scale={args.linear_scale if args.mode=='linear' else None}  fee_bps={args.fee_bps}", flush=True)
    print(f" equity_end={equity[-1]:.4f}  Sharpe_like={m.get('Sharpe_like',0):.2f}  MaxDD={m.get('MaxDD',0):.2%}  CAGR_like={m.get('CAGR_like',0):.2%}", flush=True)

if __name__=="__main__":
    main()

=== ./build_strict_gate_from_states.py ===

#!/usr/bin/env python3
import csv, subprocess, shlex, sys, os, json

GATE_JSON   = "consensus_gate_td3bc.json"
SCALER_JSON = "scaler.json"
LIVE_GATE   = "consensus_gate_live_plus.py"  # must be in current dir

def main():
    if not os.path.exists(GATE_JSON):
        sys.exit(f"[error] {GATE_JSON} not found")
    if not os.path.exists(SCALER_JSON):
        sys.exit(f"[error] {SCALER_JSON} not found")
    if not os.path.exists(LIVE_GATE):
        sys.exit(f"[error] {LIVE_GATE} not found in CWD")
    if len(sys.argv) != 3:
        print(f"usage: {sys.argv[0]} STATES_CSV OUT_CSV", file=sys.stderr)
        sys.exit(2)

    states_csv = sys.argv[1]
    out_csv    = sys.argv[2]

    # Read states.csv (expects header, e.g., mid,spread,imbalance,mom1,mom3,vol3)
    with open(states_csv, newline='') as f:
        rdr = csv.reader(f)
        cols = next(rdr)  # header
        rows = [r for r in rdr]

    # Build strict gate by invoking the live-gate once per row
    out_lines = ["row_idx,agree,action,max_pair_diff,min_q"]
    for i, r in enumerate(rows):
        state_row = ",".join(r)  # live gate expects values in the scaler's order; states.csv should already match
        cmd = [
            sys.executable, LIVE_GATE,
            "--gate", GATE_JSON,
            "--scaler", SCALER_JSON,
            "--state_row", state_row,
            "--device", "cpu",
            "--consensus-mode", "sign",
            "--min-agree-k", "3",
            "--agree-eps", "0.05",
            "--min-mag", "0.20",
        ]
        # Run and capture stdout
        res = subprocess.run(cmd, capture_output=True, text=True)
        if res.returncode != 0:
            print(res.stdout)
            print(res.stderr, file=sys.stderr)
            sys.exit(f"[error] live gate failed on row {i} (exit {res.returncode})")
        # Parse the last CSV line from output
        last = None
        for line in res.stdout.strip().splitlines():
            line = line.strip()
            if line.startswith("0,") or line.startswith("row_idx,"):
                last = line
        if not last:
            # Fallback: look for the standard header/data the script prints
            for line in res.stdout.strip().splitlines():
                if "," in line and line[0].isdigit():
                    last = line
                    break
        if not last:
            print(res.stdout)
            sys.exit(f"[error] could not parse gate output on row {i}")
        # Replace the '0,' with actual row index
        if last.startswith("0,"):
            last = f"{i}" + last[1:]
        out_lines.append(last)

    with open(out_csv, "w") as f:
        f.write("\n".join(out_lines) + "\n")

    print(f"[ok] wrote {out_csv} with {len(rows)} rows")

if __name__ == "__main__":
    main()

=== ./live_status.py ===

#!/usr/bin/env python3
# live_status.py — print a human summary of the current live state
import json, sys, pathlib, datetime as dt

def main():
    statefile = pathlib.Path("live_state.json")
    if not statefile.exists():
        print("state: not found (no position yet)"); return
    st = json.loads(statefile.read_text())

    pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))
    held_total = int(st.get("bars_held_total", 0))
    entry_mid = st.get("entry_mid", None)
    entry_ts  = st.get("entry_ts", None)

    pos_txt = "FLAT"
    if pos>0: pos_txt = "LONG"
    elif pos<0: pos_txt = "SHORT"

    print("=== LIVE STATUS ===")
    print(f"position     : {pos_txt} ({pos})")
    print(f"held (bars)  : {held}  | total: {held_total}")
    print(f"entry_mid    : {entry_mid if entry_mid is not None else '-'}")
    print(f"entry_ts     : {entry_ts if entry_ts else '-'}")

    # show last log line if available
    log = pathlib.Path("live_signals.csv")
    if log.exists():
        last = log.read_text().strip().splitlines()[-1]
        print(f"last_signal  : {last}")
    else:
        print("last_signal  : (no log yet)")

if __name__ == "__main__":
    main()

=== ./consensus_gate_td3bc.py ===

#!/usr/bin/env python3
# consensus_gate_td3bc.py
# Loads N actor .pt files, reads states.csv, computes per-actor actions,
# applies consensus (agree_eps) and optional q_min_thresh, prints final action per row.
import argparse, json, csv, os, sys
from typing import List, Tuple, Optional

import numpy as np

try:
    import torch
except ImportError:
    print("[error] Requires torch. Try: pip install torch --extra-index-url https://download.pytorch.org/whl/cpu", file=sys.stderr)
    sys.exit(2)

REQUIRED_COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

def load_model(path: str, device: str):
    # Try TorchScript first (jit), then regular torch.load
    m = None
    try:
        m = torch.jit.load(path, map_location=device)
    except Exception:
        try:
            m = torch.load(path, map_location=device)
        except Exception as e:
            raise RuntimeError(f"Failed to load model {path}: {e}")
    m.eval()
    return m

@torch.no_grad()
def model_forward(model, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    # Try a few common signatures: (s)->a ; (s)->(a,q)
    out = model(x)
    if isinstance(out, (tuple, list)) and len(out) >= 2:
        a, q = out[0], out[1]
        return a, q
    # if single tensor, assume it's the action; no Q available
    return out, None

def consensus_action(actions: np.ndarray, agree_eps: float, act_limit: float) -> Tuple[bool, float]:
    """
    actions: shape [n_actors] (1D)
    agree if max pairwise abs diff <= agree_eps * act_limit (scale by limit).
    return (agree, mean_action_clipped)
    """
    if actions.ndim != 1:
        actions = actions.reshape(-1)
    if len(actions) == 0:
        return False, 0.0
    max_diff = np.max(np.abs(actions[:, None] - actions[None, :]))
    agree = max_diff <= (agree_eps * act_limit)
    mean_a = float(np.mean(actions))
    # clip to act_limit
    mean_a = max(-act_limit, min(act_limit, mean_a))
    return agree, mean_a

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True, help="consensus_gate_td3bc.json")
    ap.add_argument("--state_csv", required=True, help="CSV with columns: mid,spread,imbalance,mom1,mom3,vol3")
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    ap.add_argument("--out_csv", default=None, help="optional write results to CSV")
    args = ap.parse_args()

    # Load gate config
    with open(args.gate, "r") as f:
        gate = json.load(f)
    if gate.get("type") != "td3bc_consensus_gate":
        print(f"[error] gate.type must be 'td3bc_consensus_gate' (got {gate.get('type')})", file=sys.stderr)
        sys.exit(2)

    actors: List[str] = gate["actors"]
    agree_eps: float = float(gate["agree_eps"])
    q_min_thresh: float = float(gate["q_min_thresh"])
    state_dim: int = int(gate["state_dim"])
    act_limit: float = float(gate.get("act_limit", 1.0))

    if state_dim != 6:
        print(f"[warn] gate.state_dim={state_dim} but this runner expects 6 features; continuing.", file=sys.stderr)

    # Load states
    with open(args.state_csv, newline="") as f:
        rdr = csv.DictReader(f)
        hdr = [h.strip().lower() for h in rdr.fieldnames or []]
        if hdr != REQUIRED_COLS:
            print(f"[error] state_csv header mismatch.\n"
                  f"  got : {rdr.fieldnames}\n"
                  f"  want: {REQUIRED_COLS}", file=sys.stderr)
            sys.exit(3)
        states = []
        for row in rdr:
            try:
                states.append([float(row[c]) for c in REQUIRED_COLS])
            except Exception as e:
                print(f"[warn] bad row in states.csv skipped: {e}", file=sys.stderr)
        X = np.array(states, dtype=np.float32)

    if X.size == 0:
        print("[error] no rows in state_csv after parsing.", file=sys.stderr)
        sys.exit(4)

    # Load models
    device = torch.device(args.device if (args.device != "cuda" or torch.cuda.is_available()) else "cpu")
    models = []
    for a in actors:
        if not os.path.isfile(a):
            print(f"[error] actor file not found: {a}", file=sys.stderr)
            sys.exit(5)
        models.append(load_model(a, device))

    # Inference
    T = X.shape[0]
    results = []
    print("row_idx,agree,action,details")
    for i in range(T):
        s = torch.from_numpy(X[i:i+1]).to(device)  # shape [1,6]
        acts = []
        qs   = []
        for m in models:
            a, q = model_forward(m, s)
            # force to scalar action (supports 1D action)
            a_np = a.detach().cpu().numpy().reshape(-1)
            if a_np.size == 0:
                print(f"[warn] empty action from a model at row {i}; treating as 0", file=sys.stderr)
                acts.append(0.0)
            else:
                acts.append(float(a_np[0]))
            if q is not None:
                q_np = q.detach().cpu().numpy().reshape(-1)
                if q_np.size > 0:
                    qs.append(float(q_np[0]))

        acts_arr = np.array(acts, dtype=np.float32)
        agree, mean_a = consensus_action(acts_arr, agree_eps, act_limit)

        # Optional Q filter: if any Qs exist, require min(Q) >= q_min_thresh
        q_ok = True
        min_q = None
        if qs:
            min_q = float(np.min(qs))
            q_ok = (min_q >= q_min_thresh)

        final_ok = (agree and q_ok)
        action_out = mean_a if final_ok else 0.0  # 0 = no-trade when not in consensus / Q too low

        details = {
            "actors": len(models),
            "agree_eps": agree_eps,
            "act_limit": act_limit,
            "max_pair_diff": float(np.max(np.abs(acts_arr[:,None]-acts_arr[None,:]))) if len(acts_arr)>1 else 0.0,
            "min_q": min_q
        }
        print(f"{i},{int(final_ok)},{action_out},{details}")
        results.append((i, int(final_ok), action_out, details))

    if args.out_csv:
        with open(args.out_csv, "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["row_idx","agree","action","min_q"])
            for i, ok, a, det in results:
                w.writerow([i, ok, a, (det.get("min_q") if isinstance(det, dict) else None)])

if __name__ == "__main__":
    main()

=== ./check_csv_sanity.py ===

#!/usr/bin/env python3
import argparse, os, csv, itertools, sys

def sniff(path, n=3):
    if not os.path.isfile(path):
        print(f"[error] missing file: {path}")
        return
    size = os.path.getsize(path)
    with open(path, 'r', newline='') as f:
        lines = f.readlines()
    print(f"[file] {path}  bytes={size}  lines={len(lines)}")
    if not lines:
        return
    print("[head]")
    for ln in lines[:n]:
        print(ln.rstrip("\n"))
    # header + minimal parse
    try:
        f = open(path, 'r', newline='')
        r = csv.DictReader(f)
        print("[header]", r.fieldnames)
        first = list(itertools.islice(r, 3))
        print("[sample rows]", len(first))
        for i,row in enumerate(first):
            print(f"  row{i} keys={list(row.keys())[:6]} ...")
    except Exception as e:
        print("[warn] csv parse error:", e)
    finally:
        try: f.close()
        except: pass

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    args = ap.parse_args()
    sniff(args.states)
    sniff(args.gate)

if __name__ == "__main__":
    main()

=== ./live_status_pretty.py ===

#!/usr/bin/env python3
# live_status_pretty.py
# Summarize live state in a single human-readable line.

import json, pathlib, csv, math, sys
from datetime import datetime, timezone

STATE = pathlib.Path("live_state.json")
LOG   = pathlib.Path("live_signals.csv")

def safe_float(x, default=None):
    try: return float(x)
    except: return default

def load_state():
    st = {"prev_pos":0,"bars_held":0,"bars_held_total":0,"entry_mid":None,"entry_ts":None}
    if STATE.exists():
        try: st.update(json.loads(STATE.read_text()))
        except: pass
    return st

def read_last_log_row():
    if not LOG.exists(): return None
    lines = LOG.read_text().strip().splitlines()
    if len(lines) <= 1: return None
    hdr = [h.strip() for h in lines[0].split(",")]
    row = [c.strip() for c in lines[-1].split(",")]
    obj = {k:(row[i] if i < len(row) else "") for i,k in enumerate(hdr)}
    # normalize fields possibly present
    mid = safe_float(obj.get("mid"))
    spread = safe_float(obj.get("spread"))
    vol3 = safe_float(obj.get("vol3"))
    raw_action = safe_float(obj.get("raw_action"), 0.0)
    buy_th = safe_float(obj.get("buy_th") or obj.get("th"), 0.10)
    sell_th = safe_float(obj.get("sell_th") or obj.get("th"), 0.10)
    hold_bars = int(float(obj.get("hold_bars", "12") or 12))
    hold_ctr  = int(float(obj.get("hold_counter", "0") or 0))
    ts = obj.get("ts","")
    return dict(ts=ts, mid=mid, spread=spread, vol3=vol3, raw_action=raw_action,
                buy_th=buy_th, sell_th=sell_th, hold_bars=hold_bars, hold_ctr=hold_ctr)

def fmt_bps(x):
    return ("+" if x>=0 else "") + f"{x:.2f} bps"

def main():
    st = load_state()
    last = read_last_log_row() or {}
    pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))
    held_total = int(st.get("bars_held_total", 0))
    entry_mid = st.get("entry_mid")
    entry_ts  = st.get("entry_ts")
    cur_mid   = last.get("mid")

    pos_txt = "FLAT"
    if pos>0: pos_txt = "LONG"
    elif pos<0: pos_txt = "SHORT"

    upnl_bps = None
    if entry_mid is not None and cur_mid is not None and pos != 0:
        ret = (cur_mid - float(entry_mid)) / float(entry_mid)
        upnl_bps = (ret * (1 if pos>0 else -1)) * 1e4

    # next flip earliest in:
    hold_bars = last.get("hold_bars", 12) or 12
    hold_ctr  = last.get("hold_ctr", held) or held
    flip_in   = max(0, int(hold_bars) - int(hold_ctr))

    # build one clean line
    ts = last.get("ts","")
    parts = [
        f"{ts}",
        f"{pos_txt}",
        f"held {held}/{hold_bars} bars (total {held_total})",
        f"flip in {flip_in} bars" if pos != 0 else "flip allowed now",
        f"entry_mid={entry_mid:.2f}" if isinstance(entry_mid,(int,float)) else "entry_mid=-",
        f"mid={cur_mid:.2f}" if isinstance(cur_mid,(int,float)) else "mid=-",
        f"uPnL={fmt_bps(upnl_bps)}" if upnl_bps is not None else "uPnL=-"
    ]
    print(" | ".join(parts))

if __name__ == "__main__":
    main()

=== ./trainer_heartbeat.py ===

import sys, os, time, json, math, signal
from pathlib import Path

class Heartbeat:
    def __init__(self, outdir: Path, total_steps: int, tag: str="train", refresh=2):
        self.outdir = Path(outdir)
        self.outdir.mkdir(parents=True, exist_ok=True)
        self.csv = (self.outdir / "train_log.csv").open("a", buffering=1)
        if self.csv.tell() == 0:
            self.csv.write("ts,step,total,eta_s,secs_per_k,seed,buf,loss_q,loss_pi,loss_bc,td_abs,pi_norm,q1,q2,reward,eps\n")
        self.now = (self.outdir / "now_train.csv")
        self.total = total_steps
        self.t0 = time.time()
        self._last = self.t0
        self.refresh = refresh
        self._last_print = 0
        self.tag = tag
        self._want_stop = False
        signal.signal(signal.SIGINT, self._sigint)

    def _sigint(self, *_):
        self._want_stop = True
        print("\n[heartbeat] SIGINT received — saving state then exiting cleanly...", file=sys.stderr)

    @property
    def want_stop(self): return self._want_stop

    def _eta(self, step):
        elapsed = max(1e-6, time.time() - self.t0)
        rate = step / elapsed
        rem = max(0.0, (self.total - step) / max(1e-6, rate))
        return rem, 1000.0 / max(1e-9, rate)  # secs_per_k

    def log(self, *, step:int, seed:int, buf:int, loss_q:float, loss_pi:float, loss_bc:float,
            td_abs:float, pi_norm:float, q1:float, q2:float, reward:float, eps:float):
        ts = time.time()
        eta_s, spk = self._eta(step)
        self.csv.write(f"{ts:.3f},{step},{self.total},{eta_s:.1f},{spk:.3f},{seed},{buf},{loss_q:.6f},{loss_pi:.6f},{loss_bc:.6f},{td_abs:.6f},{pi_norm:.6f},{q1:.6f},{q2:.6f},{reward:.6f},{eps:.6f}\n")
        # tiny “now” file for other UIs (tail -f)
        self.now.write_text(
            f"step={step}/{self.total}  eta={eta_s/60.0:,.1f}m  secs/1k={spk:.2f}  "
            f"loss_q={loss_q:.4f}  loss_pi={loss_pi:.4f}  bc={loss_bc:.4f}  "
            f"td|={td_abs:.4f}  |pi|={pi_norm:.3f}  q1={q1:.4f} q2={q2:.4f}  "
            f"buf={buf}  seed={seed}\n"
        )
        # periodic single-line console heartbeat (no spam)
        if ts - self._last_print >= self.refresh:
            pct = 100.0 * step / max(1, self.total)
            bar_w, filled = 28, int(28 * pct / 100.0)
            bar = "[" + "#"*filled + "-"*(bar_w-filled) + "]"
            print(
                f"\r{bar} {pct:5.1f}%  step {step}/{self.total}  eta {eta_s/60.0:5.1f}m  "
                f"q1={q1:7.4f} q2={q2:7.4f}  pi={pi_norm:6.3f}  "
                f"Q={loss_q:6.4f} Pi={loss_pi:6.4f} BC={loss_bc:6.4f}  buf={buf:7d}",
                end="",
                file=sys.stdout,
                flush=True
            )
            self._last_print = ts

    def close(self):
        try: self.csv.close()
        except: pass

=== ./inspect_parquet_orderbook.py ===

#!/usr/bin/env python3
"""
inspect_parquet_orderbook.py

One-shot scanner for L2/L1 orderbook Parquet trees.

- Recursively scans a Parquet glob (supports **/*.parquet)
- Reports schema and candidate columns for:
    * timestamp
    * bid/ask price
    * bid/ask size
- Builds robust, ready-to-use DuckDB expressions:
    * TS_EXPR (ISO string or epoch-ms auto-detect)
    * BID_PX_EX, ASK_PX_EX, BID_SZ_EX, ASK_SZ_EX
- Writes a JSON report: ob_mapping.json
- (Optional) emits a tailored rl_duckdb_data_layer.py when --emit-datalayer is given

Usage:
  python3 inspect_parquet_orderbook.py \
    --parquet "/abs/path/**/*.parquet" \
    --out ob_mapping.json \
    [--emit-datalayer rl_duckdb_data_layer.py]
"""

import argparse, json, os, re, sys
from typing import List, Tuple
import duckdb

# -------- helpers (version-safe) --------
def sql_lit(s: str) -> str:
    return "'" + s.replace("'", "''") + "'"

def qident(name: str) -> str:
    return '"' + name.replace('"','""') + '"'

def describe_schema(con: duckdb.DuckDBPyConnection, glob: str) -> List[Tuple[str,str]]:
    q = f"DESCRIBE SELECT * FROM read_parquet({sql_lit(glob)})"
    rows = con.execute(q).fetchall()
    return [(r[0], r[1]) for r in rows]

def list_files(con: duckdb.DuckDBPyConnection, glob: str) -> List[str]:
    # Use filename=true to expose the 'filename' pseudo-column
    q = f"SELECT DISTINCT filename FROM read_parquet({sql_lit(glob)}, filename=true)"
    try:
        return [r[0] for r in con.execute(q).fetchall()]
    except Exception:
        return []

def to_level(name: str):
    m = re.search(r'_(\d+)$', name)
    return int(m.group(1)) if m else None

def pick_candidates(cols: List[Tuple[str,str]], side: str, kind: str) -> List[str]:
    """
    side in {'bid','ask'}, kind in {'px','sz'}
    robust heuristics for names like: bid_px_0, best_ask_price, ask_qty_1, etc.
    """
    names = [c[0] for c in cols]
    cands = []
    for n in names:
        ln = n.lower()
        if side not in ln:
            continue
        if kind == "px":
            if ("px" in ln) or ("price" in ln) or re.search(r'(^|_)(p|prc|price)($|_)', ln):
                cands.append(n)
        else:  # sz
            if ("sz" in ln) or ("size" in ln) or ("qty" in ln) or ("quantity" in ln) or re.search(r'(^|_)(q|qty|size)($|_)', ln):
                cands.append(n)
    # prefer *_0, then ascending level, then name
    cands = list(set(cands))
    cands.sort(key=lambda x: (999999 if to_level(x) is None else to_level(x), x))
    return cands

def ts_candidates(cols: List[Tuple[str,str]]) -> List[str]:
    out = []
    for c,_ in cols:
        lc = c.lower()
        if ("ts" in lc) or ("time" in lc) or ("timestamp" in lc) or ("datetime" in lc):
            out.append(c)
    # unique but preserve order
    seen=set(); uniq=[]
    for c in out:
        if c not in seen:
            seen.add(c); uniq.append(c)
    return uniq or ["ts"]

def build_ts_expr(ts_cands: List[str]) -> str:
    parts = []
    for c in ts_cands:
        ci = qident(c)
        parts.append(f"TRY_CAST({ci} AS TIMESTAMP)")
    for c in ts_cands:
        ci = qident(c)
        parts.append(f"to_timestamp(CAST(TRY_CAST({ci} AS BIGINT) AS DOUBLE)/1000.0)")
    # common epoch-ms fallbacks
    for c in ["ts_ms","timestamp_ms","time_ms","epoch_ms","t_ms"]:
        parts.append(f"to_timestamp(CAST(TRY_CAST({c} AS BIGINT) AS DOUBLE)/1000.0)")
    # last resort: 'ts' cast via DOUBLE->BIGINT
    parts.append("to_timestamp(CAST(TRY_CAST(CAST(ts AS DOUBLE) AS BIGINT) AS DOUBLE)/1000.0)")
    return "COALESCE(" + ", ".join(parts) + ")"

def build_num_expr(cands: List[str], default: str=None) -> str:
    parts = [f"CAST({qident(c)} AS DOUBLE)" for c in cands]
    if default is not None:
        parts.append(default)
    if not parts:
        return "NULL"
    return "COALESCE(" + ", ".join(parts) + ")"

TAILORED_DATALAYER_TMPL = """#!/usr/bin/env python3
# Auto-generated by inspect_parquet_orderbook.py
# Tailored top-of-book mapping for your Parquet schema.

import argparse
from typing import List, Optional, Dict
import duckdb, numpy as np, pandas as pd

TS_EXPR   = \"\"\"{TS_EXPR}\"\"\"
BID_PX_EX = \"\"\"{BID_PX_EX}\"\"\"
ASK_PX_EX = \"\"\"{ASK_PX_EX}\"\"\"
BID_SZ_EX = \"\"\"{BID_SZ_EX}\"\"\"
ASK_SZ_EX = \"\"\"{ASK_SZ_EX}\"\"\"

class ReplayBuffer:
    def __init__(self, capacity:int, state_dim:int, act_dim:int=0):
        self.capacity=int(capacity); self.state_dim=int(state_dim); self.act_dim=int(act_dim)
        self.ptr=0; self.full=False
        self.s=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.sp=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.r=np.zeros((self.capacity,1),dtype=np.float32)
        self.d=np.zeros((self.capacity,1),dtype=np.float32)
        self.a=None
        if self.act_dim>0: self.a=np.zeros((self.capacity,self.act_dim),dtype=np.float32)
    def add(self,s,a,r,sp,d):
        i=self.ptr; self.s[i]=s; self.sp[i]=sp; self.r[i]=r; self.d[i]=d
        if self.a is not None and a is not None: self.a[i]=a
        self.ptr=(self.ptr+1)%self.capacity; self.full = self.full or (self.ptr==0)
    def size(self): return self.capacity if self.full else self.ptr
    def sample(self,batch_size:int,rng:np.random.Generator)->Dict[str,np.ndarray]:
        n=self.size(); idx=rng.integers(0,n,size=batch_size)
        out={{"s":self.s[idx],"r":self.r[idx],"sp":self.sp[idx],"d":self.d[idx]}}
        if self.a is not None: out["a"]=self.a[idx]
        return out

def build_sql(parquet_glob:str,start_ts:Optional[str],end_ts:Optional[str],horizon_rows:int,ret_threshold:float,extra_filters:Optional[str]=None)->str:
    where=[]
    if start_ts: where.append(f"ts_parsed >= TIMESTAMP '{{start_ts}}'")
    if end_ts:   where.append(f"ts_parsed <= TIMESTAMP '{{end_ts}}'")
    if extra_filters: where.append(f"({{extra_filters}})")
    where_clause = "WHERE " + " AND ".join(where) if where else ""
    sql = f\"\"\"
WITH raw AS (
  SELECT * FROM read_parquet('{{parquet_glob}}', filename=true)
),
proj AS (
  SELECT
    {{TS}} AS ts_parsed,
    {{BP}} AS bid_px_d,
    {{AP}} AS ask_px_d,
    {{BS}} AS bid_sz_d,
    {{AS}} AS ask_sz_d
  FROM raw
),
src AS ( SELECT * FROM proj {{where_clause}} ),
ord AS ( SELECT * FROM src ORDER BY ts_parsed ),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px_d + ask_px_d)*0.5 AS mid,
    (ask_px_d - bid_px_d)     AS spread,
    CASE WHEN (bid_sz_d + ask_sz_d)>0
         THEN (bid_sz_d - ask_sz_d)/NULLIF(bid_sz_d + ask_sz_d,0)
         ELSE 0 END           AS imbalance,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,1) OVER ()) AS mom1,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px_d + ask_px_d)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
),
lab AS (
  SELECT
    *,
    LEAD(mid, {{horizon_rows}}) OVER () AS mid_fwd,
    (LEAD(mid, {{horizon_rows}}) OVER () - mid) / NULLIF(mid,0) AS fwd_ret
  FROM feat
),
final AS (
  SELECT
    ts,
    mid, spread, imbalance, mom1, mom3, vol3,
    LEAD(mid, {{horizon_rows}}) OVER ()        AS mid_p,
    LEAD(spread, {{horizon_rows}}) OVER ()     AS spread_p,
    LEAD(imbalance, {{horizon_rows}}) OVER ()  AS imbalance_p,
    LEAD(mom1, {{horizon_rows}}) OVER ()       AS mom1_p,
    LEAD(mom3, {{horizon_rows}}) OVER ()       AS mom3_p,
    LEAD(vol3, {{horizon_rows}}) OVER ()       AS vol3_p,
    fwd_ret,
    CASE WHEN fwd_ret > {{ret_threshold}} THEN 1
         WHEN fwd_ret < -{{ret_threshold}} THEN -1
         ELSE 0 END AS label
  FROM lab
)
SELECT * FROM final
WHERE mid IS NOT NULL AND mid_p IS NOT NULL
\"\"\".format(parquet_glob=parquet_glob,start_ts=start_ts,end_ts=end_ts,extra_filters=extra_filters or "",
             horizon_rows=horizon_rows,ret_threshold=ret_threshold,where_clause=where_clause)\
             .replace("{{TS}}", TS_EXPR)\
             .replace("{{BP}}", BID_PX_EX)\
             .replace("{{AP}}", ASK_PX_EX)\
             .replace("{{BS}}", BID_SZ_EX)\
             .replace("{{AS}}", ASK_SZ_EX)
    return sql

def _ensure_float(df: pd.DataFrame, cols: List[str])->pd.DataFrame:
    for c in cols:
        if c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    return df

def stream_into_buffer(con:duckdb.DuckDBPyConnection, sql:str, buffer:ReplayBuffer,
                       chunk_rows:int=200_000, drop_noise_label:bool=False, derive_action:bool=False)->int:
    total=0; cur=con.execute(sql)
    state_cols=["mid","spread","imbalance","mom1","mom3","vol3"]
    next_cols=["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
    while True:
        df=cur.fetch_df_chunk(chunk_rows)
        if df is None or len(df)==0: break
        df=_ensure_float(df, list(df.columns))
        if drop_noise_label and "label" in df.columns: df=df[df["label"]!=0]
        df=df.dropna(subset=state_cols+next_cols+["fwd_ret"])
        S=df[state_cols].to_numpy(np.float32)
        SP=df[next_cols].to_numpy(np.float32)
        R=df[["fwd_ret"]].to_numpy(np.float32)
        D=np.zeros((len(df),1),dtype=np.float32)
        if derive_action:
            A=df[["label"]].to_numpy(np.float32) if "label" in df.columns else np.sign(R).astype(np.float32)
        else:
            A=None
        for i in range(len(df)):
            buffer.add(S[i], (A[i] if A is not None else None), R[i], SP[i], D[i])
        total+=len(df)
    return total

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    args=ap.parse_args()
    con=duckdb.connect(database=":memory:")
    sql=build_sql(args.parquet,args.start,args.end,args.h,args.th,None)
    plan=con.execute(f"EXPLAIN {sql}").fetchall()
    print("[DuckDB plan]"); [print(" ",r[0]) for r in plan]
"""

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Recursive glob, e.g. /path/**/*.parquet")
    ap.add_argument("--out", default="ob_mapping.json", help="Report JSON out path")
    ap.add_argument("--emit-datalayer", default=None, help="Optional: write tailored rl_duckdb_data_layer.py here")
    args = ap.parse_args()

    con = duckdb.connect()

    # gather info
    cols = describe_schema(con, args.parquet)
    if not cols:
        print("No columns found. Check your --parquet pattern.", file=sys.stderr)
        sys.exit(2)

    files = list_files(con, args.parquet)

    # detect candidates
    ts_cands = ts_candidates(cols)
    bid_px = pick_candidates(cols, "bid", "px")
    ask_px = pick_candidates(cols, "ask", "px")
    bid_sz = pick_candidates(cols, "bid", "sz")
    ask_sz = pick_candidates(cols, "ask", "sz")

    # build expressions
    TS_EXPR   = build_ts_expr(ts_cands)
    BID_PX_EX = build_num_expr(bid_px)
    ASK_PX_EX = build_num_expr(ask_px)
    BID_SZ_EX = build_num_expr(bid_sz, default="1.0")
    ASK_SZ_EX = build_num_expr(ask_sz, default="1.0")

    report = {
        "parquet_glob": args.parquet,
        "file_count_seen": len(files),
        "example_files": files[:10],
        "schema": [{ "name": n, "type": t } for n,t in cols],
        "detected": {
            "ts_candidates": ts_cands,
            "bid_px_candidates": bid_px,
            "ask_px_candidates": ask_px,
            "bid_sz_candidates": bid_sz,
            "ask_sz_candidates": ask_sz
        },
        "recommended_expressions": {
            "TS_EXPR": TS_EXPR,
            "BID_PX_EX": BID_PX_EX,
            "ASK_PX_EX": ASK_PX_EX,
            "BID_SZ_EX": BID_SZ_EX,
            "ASK_SZ_EX": ASK_SZ_EX
        }
    }

    with open(args.out, "w") as f:
        json.dump(report, f, indent=2)
    print(f"[ok] wrote mapping report: {os.path.abspath(args.out)}")

    if args.emit_datalayer:
        code = TAILORED_DATALAYER_TMPL.format(
            TS_EXPR=TS_EXPR,
            BID_PX_EX=BID_PX_EX,
            ASK_PX_EX=ASK_PX_EX,
            BID_SZ_EX=BID_SZ_EX,
            ASK_SZ_EX=ASK_SZ_EX
        )
        with open(args.emit_datalayer, "w") as f:
            f.write(code)
        print(f"[ok] wrote tailored data layer: {os.path.abspath(args.emit_datalayer)}")

if __name__ == "__main__":
    main()

=== ./tune_backtest.py ===

#!/usr/bin/env python3
# tune_backtest.py — grid search over action threshold (sign mode) or linear scale
import argparse, subprocess, sys, itertools, json

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        print(p.stderr.strip(), file=sys.stderr); sys.exit(p.returncode)
    return p.stdout.strip()

def parse_metrics(out):
    # expects backtest_gate_horizon.py stdout
    lines=[ln for ln in out.splitlines() if ln.startswith(" equity_end=") or ln.startswith("[backtest H]")]
    return "\n".join(lines[-2:])

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--fee_bps", type=float, default=5.0)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    args=ap.parse_args()

    if args.mode=="sign":
        th_grid=[0.03,0.05,0.08,0.10,0.15]
        hold_grid=[0,4,8,12]
        for th, hold in itertools.product(th_grid, hold_grid):
            out=run([sys.executable, "backtest_gate_horizon.py",
                     "--states", args.states, "--gate", args.gate,
                     "--h", str(args.h), "--mode", "sign",
                     "--action_thresh", str(th), "--hold_bars", str(hold),
                     "--fee_bps", str(args.fee_bps)])
            print(f"\n>>> th={th:.3f} hold={hold}\n{parse_metrics(out)}")
    else:
        scale_grid=[0.5,0.75,1.0,1.5,2.0]
        hold_grid=[0,4,8,12]
        for sc, hold in itertools.product(scale_grid, hold_grid):
            out=run([sys.executable, "backtest_gate_horizon.py",
                     "--states", args.states, "--gate", args.gate,
                     "--h", str(args.h), "--mode", "linear",
                     "--linear_scale", str(sc), "--hold_bars", str(hold),
                     "--fee_bps", str(args.fee_bps)])
            print(f"\n>>> scale={sc:.2f} hold={hold}\n{parse_metrics(out)}")
if __name__=="__main__":
    main()

=== ./live_executor_hold.py ===

#!/usr/bin/env python3
# live_executor_hold.py
# - Pull latest L2-derived state from your parquet glob
# - Run sign-consensus gate with min-agree-k=2, min-mag=TH
# - Enforce min-hold bars before flipping (state persisted in a json file)
# - Print a single CSV line with ts,pos,raw_action,dec_action,hold_left

import argparse, json, os, subprocess, sys, time, datetime as dt

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        raise RuntimeError(p.stderr.strip())
    return p.stdout.strip()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--th", type=float, default=0.10, help="min |action| to trade")
    ap.add_argument("--hold_bars", type=int, default=12, help="minimum bars to hold before flip")
    ap.add_argument("--statefile", default="live_executor_state.json")
    ap.add_argument("--log", default="live_signals.csv")
    args = ap.parse_args()

    # 1) get latest state row (6 features)
    row = run([sys.executable, "latest_state_from_parquet.py", "--parquet", args.parquet])

    # 2) run sign-consensus gate once
    out = run([
        sys.executable, "consensus_gate_live_plus.py",
        "--gate", args.gate,
        "--scaler", args.scaler,
        "--state_row", row,
        "--device", args.device,
        "--consensus-mode", "sign",
        "--min-agree-k", "2",
        "--min-mag", str(args.th)
    ])

    # parse last CSV line "row_idx,agree,action,max_pair_diff,min_q"
    last = [ln for ln in out.splitlines() if ln and ln[0].isdigit()][-1]
    parts = last.split(",")
    agree = int(parts[1])
    act = float(parts[2])  # already 0 if not agreed or fails q filter

    # 3) threshold to discrete intent (±1/0) using th; keep continuous for diagnostics
    intent = 1 if act >= args.th else (-1 if act <= -args.th else 0)

    # 4) load persistent state (prev_pos, bars_held)
    st = {"prev_pos": 0, "bars_held": 0}
    if os.path.isfile(args.statefile):
        try:
            with open(args.statefile, "r") as f: st.update(json.load(f))
        except: pass
    prev_pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))

    # 5) enforce min-hold: if wanting to flip but haven't held enough, keep prev_pos
    next_pos = intent
    if intent != prev_pos and held < args.hold_bars:
        next_pos = prev_pos
        held += 1
    else:
        # change allowed (or no change requested)
        held = 0 if next_pos != prev_pos else min(held+1, args.hold_bars)

    # 6) persist state
    with open(args.statefile, "w") as f:
        json.dump({"prev_pos": int(next_pos), "bars_held": int(held)}, f)

    # 7) print + log
    ts = dt.datetime.utcnow().isoformat()
    line = f"{ts},{prev_pos},{next_pos},{act:.6f},{args.th:.2f},{args.hold_bars},{held}"
    print(line)
    if not os.path.isfile(args.log):
        with open(args.log, "w") as f:
            f.write("ts,prev_pos,next_pos,raw_action,th,hold_bars,hold_counter\n")
    with open(args.log, "a") as f:
        f.write(line + "\n")

if __name__ == "__main__":
    main()

=== ./td3_bc_agent.py ===

#!/usr/bin/env python3
from typing import Dict, Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F
from rl_utils import MLP, polyak_update

class Actor(nn.Module):
    def __init__(self, s_dim: int, a_dim: int, hidden=(128,128), act_limit=1.0):
        super().__init__()
        self.pi = MLP(s_dim, a_dim, hidden, nn.ReLU, None)
        self.act_limit = float(act_limit)
    def forward(self, s):
        return torch.tanh(self.pi(s)) * self.act_limit

class CriticQ(nn.Module):
    def __init__(self, s_dim: int, a_dim: int, hidden=(128,128)):
        super().__init__()
        self.q = MLP(s_dim + a_dim, 1, hidden, nn.ReLU, None)
    def forward(self, s, a):
        x = torch.cat([s, a], dim=-1)
        return self.q(x)

class TD3BC:
    def __init__(
        self,
        s_dim: int,
        a_dim: int,
        act_limit: float = 1.0,
        actor_lr: float = 1e-3,
        critic_lr: float = 1e-3,
        gamma: float = 0.99,
        tau: float = 0.005,
        policy_delay: int = 2,
        bc_lambda: float = 0.1,
        target_noise: float = 0.2,
        noise_clip: float = 0.5,
        device: str = "cpu",
    ):
        self.device = device
        self.gamma = gamma
        self.tau = tau
        self.policy_delay = policy_delay
        self.bc_lambda = bc_lambda
        self.target_noise = target_noise
        self.noise_clip = noise_clip
        self.total_it = 0

        self.actor = Actor(s_dim, a_dim, act_limit=act_limit).to(device)
        self.actor_targ = Actor(s_dim, a_dim, act_limit=act_limit).to(device)
        self.actor_targ.load_state_dict(self.actor.state_dict())

        self.q1 = CriticQ(s_dim, a_dim).to(device)
        self.q1_targ = CriticQ(s_dim, a_dim).to(device)
        self.q1_targ.load_state_dict(self.q1.state_dict())

        self.q2 = CriticQ(s_dim, a_dim).to(device)
        self.q2_targ = CriticQ(s_dim, a_dim).to(device)
        self.q2_targ.load_state_dict(self.q2.state_dict())

        self.pi_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.q1_optim = torch.optim.Adam(self.q1.parameters(), lr=critic_lr)
        self.q2_optim = torch.optim.Adam(self.q2.parameters(), lr=critic_lr)

        self.act_limit = act_limit

=== ./auto_tune_gate.py ===

#!/usr/bin/env python3
import argparse, csv, itertools, os, re, subprocess, sys, time, json, shutil
from datetime import datetime
from pathlib import Path

# required files in CWD
GATE_JSON   = "consensus_gate_td3bc.json"
SCALER_JSON = "scaler.json"
LIVE_GATE   = "consensus_gate_live_plus.py"
BACKTEST    = "backtest_gate_horizon_v3.py"

RESULTS_HEADER = [
    "mode","H","fee_bps","min_agree_k","agree_eps","min_mag",
    "action_thresh","linear_scale","hold_bars",
    "rows","trades","trade_winrate","per_bar_hitrate",
    "equity_end","Sharpe_like","MaxDD","CAGR_like",
    "exposure","avg_turnover","avg_trade_pnl","median_trade_pnl",
    "saturation_neg1","saturation_pos1","saturation_mid",
    "gate_csv","score","trial","total"
]

BT_RX = {
    "rows":               re.compile(r"\brows=(\d+)"),
    "trades":             re.compile(r"\btrades=(\d+)"),
    "trade_winrate":      re.compile(r"\btrade_winrate=([0-9.]+)%"),
    "per_bar_hitrate":    re.compile(r"\bper_bar_hitrate=([0-9.]+)%"),
    "equity_end":         re.compile(r"\bequity_end=([0-9.]+)"),
    "Sharpe_like":        re.compile(r"\bSharpe_like=([\-0-9.]+)"),
    "MaxDD":              re.compile(r"\bMaxDD=([0-9.]+)%"),
    "CAGR_like":          re.compile(r"\bCAGR_like=([\-0-9.]+)%"),
    "exposure":           re.compile(r"\bexposure=([0-9.]+)%"),
    "avg_turnover":       re.compile(r"\bavg_turnover=([0-9.]+)\/bar"),
    "avg_trade_pnl":      re.compile(r"\bavg_trade_pnl=([\-0-9.eE]+)"),
    "median_trade_pnl":   re.compile(r"\bmedian_trade_pnl=([\-0-9.eE]+)")
}

def must_exist(path: str):
    if not Path(path).exists():
        sys.exit(f"[error] required file not found: {path}")

def read_states(states_csv: str):
    with open(states_csv, newline="") as f:
        rdr = csv.reader(f)
        header = next(rdr, None)
        rows = [r for r in rdr]
    if not rows:
        sys.exit("[error] states.csv has no rows")
    return header, rows

def run_live_gate_on_row(state_row: str, device: str, mode: str,
                         min_agree_k: int, agree_eps: float, min_mag: float) -> str:
    cmd = [
        sys.executable, LIVE_GATE,
        "--gate", GATE_JSON,
        "--scaler", SCALER_JSON,
        "--state_row", state_row,
        "--device", device,
        "--consensus-mode", mode,
        "--min-agree-k", str(min_agree_k),
        "--agree-eps", f"{agree_eps}",
        "--min-mag", f"{min_mag}",
    ]
    res = subprocess.run(cmd, capture_output=True, text=True)
    if res.returncode != 0:
        raise RuntimeError(f"[live_gate] rc={res.returncode}\nSTDOUT:\n{res.stdout}\nSTDERR:\n{res.stderr}")
    candidate = None
    for line in res.stdout.strip().splitlines():
        t = line.strip()
        if t.startswith("row_idx,") or (t and t[0].isdigit() and "," in t):
            candidate = t
    if not candidate:
        raise RuntimeError(f"[live_gate] could not parse output:\n{res.stdout}")
    return candidate

def build_gate_from_states(states_csv: str, out_csv: str, device: str, mode: str,
                           min_agree_k: int, agree_eps: float, min_mag: float,
                           progress_every: int = 25) -> dict:
    _, rows = read_states(states_csv)
    lines = ["row_idx,agree,action,max_pair_diff,min_q"]
    neg1 = pos1 = mid = 0
    n = len(rows)
    print(f"[gate] building {out_csv} rows={n} k={min_agree_k} eps={agree_eps} mag={min_mag}")
    last_flush = time.time()
    for i, r in enumerate(rows):
        state_row = ",".join(r)
        line = run_live_gate_on_row(state_row, device, mode, min_agree_k, agree_eps, min_mag)
        if line.startswith("row_idx,"):  # skip header from tool
            continue
        if line.startswith("0,"):
            line = f"{i}" + line[1:]
        lines.append(line)
        try:
            a = float(line.split(",")[2])
            if a <= -0.99: neg1 += 1
            elif a >= 0.99: pos1 += 1
            else: mid += 1
        except Exception:
            pass
        if (i+1) % progress_every == 0:
            pct = (i+1)*100.0/n
            print(f"[gate] {i+1}/{n} ({pct:5.1f}%)  neg≈-1:{neg1} pos≈+1:{pos1} mid:{mid}")
        else:
            now = time.time()
            if now - last_flush >= 0.75:
                print(".", end="", flush=True)
                last_flush = now
    Path(out_csv).write_text("\n".join(lines) + "\n", encoding="utf-8")
    print()
    return {"neg1": neg1, "pos1": pos1, "mid": mid, "n": n}

def _parse_bt_metrics(text: str) -> dict:
    def _find(rx): m = rx.search(text); return m and m.group(1)
    out = {
        "rows":             int(_find(BT_RX["rows"]) or 0),
        "trades":           int(_find(BT_RX["trades"]) or 0),
        "trade_winrate":    float(_find(BT_RX["trade_winrate"]) or 0.0),
        "per_bar_hitrate":  float(_find(BT_RX["per_bar_hitrate"]) or 0.0),
        "equity_end":       float(_find(BT_RX["equity_end"]) or 0.0),
        "Sharpe_like":      float(_find(BT_RX["Sharpe_like"]) or 0.0),
        "MaxDD":            float(_find(BT_RX["MaxDD"]) or 0.0),
        "CAGR_like":        float(_find(BT_RX["CAGR_like"]) or 0.0),
        "exposure":         float(_find(BT_RX["exposure"]) or 0.0),
        "avg_turnover":     float(_find(BT_RX["avg_turnover"]) or 0.0),
        "avg_trade_pnl":    float(_find(BT_RX["avg_trade_pnl"]) or 0.0),
        "median_trade_pnl": float(_find(BT_RX["median_trade_pnl"]) or 0.0),
    }
    return out

def run_bt(mode, states_csv, gate_csv, H, fee_bps, hold_bars, action_thresh, linear_scale, stream, log_path):
    cmd = [sys.executable, BACKTEST,
           "--states", states_csv, "--gate", gate_csv, "--h", str(H), "--fee_bps", str(fee_bps)]
    if mode == "sign":
        cmd += ["--mode","sign","--hold_bars",str(hold_bars),"--action_thresh",f"{action_thresh}","--verbose"]
    else:
        cmd += ["--mode","linear","--hold_bars",str(hold_bars),"--linear_scale",f"{linear_scale}","--verbose"]
    if stream:
        print(f"[bt] {' '.join(cmd)}")
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
        captured = []
        with open(log_path, "w", encoding="utf-8") as logf:
            for line in p.stdout:
                captured.append(line); logf.write(line); print(line.rstrip())
        rc = p.wait()
        if rc != 0: raise RuntimeError(f"[backtest] rc={rc}\n{''.join(captured)}")
        return _parse_bt_metrics("".join(captured))
    else:
        res = subprocess.run(cmd, capture_output=True, text=True)
        Path(log_path).write_text(res.stdout + "\n" + res.stderr, encoding="utf-8")
        if res.returncode != 0:
            raise RuntimeError(f"[backtest] rc={res.returncode}\nSTDOUT:\n{res.stdout}\nSTDERR:\n{res.stderr}")
        return _parse_bt_metrics(res.stdout + "\n" + res.stderr)

def score_result(m: dict) -> float:
    if m["trades"] == 0: return -1e9
    s = 0.0
    s += (m["equity_end"] - 1.0) * 100.0
    s += 0.5 * m["Sharpe_like"]
    s -= 0.1 * m["MaxDD"]
    if m["exposure"] > 95.0: s -= 1.0
    return s

def term_width():
    try:
        return shutil.get_terminal_size((120, 20)).columns
    except Exception:
        return 120

def oneline_status(trial, total, mode, k, eps, mag, th, ls, hold, gate_name, extra=""):
    w = term_width()
    parts = [
        f"[{trial}/{total}]",
        f"mode={mode}",
        f"k={k}",
        f"eps={eps:g}",
        f"mag={mag:g}",
        (f"th={th:g}" if th is not None else f"ls={ls:g}"),
        f"hold={hold}",
        f"gate={gate_name}",
        extra
    ]
    s = " ".join(p for p in parts if p)
    if len(s) > w-2: s = s[:w-5] + "..."
    print("\r" + s, end="", flush=True)

def write_live_files(prefix: Path, record: dict):
    # append JSONL
    with open(prefix.with_suffix(".now.jsonl"), "a", encoding="utf-8") as jf:
        jf.write(json.dumps(record, ensure_ascii=False) + "\n")
    # latest-only CSV (overwrite)
    fields = list(record.keys())
    csv_path = prefix.with_suffix(".now.csv")
    if not csv_path.exists():
        with open(csv_path, "w", newline="") as cf:
            csv.writer(cf).writerow(fields)
    with open(csv_path, "w", newline="") as cf:
        w = csv.DictWriter(cf, fieldnames=fields); w.writeheader(); w.writerow(record)

def main():
    for f in (GATE_JSON, SCALER_JSON, LIVE_GATE, BACKTEST):
        must_exist(f)

    ap = argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--device", choices=["cpu","cuda","mps"], default="cpu")
    ap.add_argument("--mode", choices=["sign","linear"], required=True)
    ap.add_argument("--H", type=int, required=True)
    ap.add_argument("--fee-bps", type=float, required=True)

    ap.add_argument("--min-agree-k", type=int, nargs="+", default=[2,3])
    ap.add_argument("--agree-eps", type=float, nargs="+", default=[0.05,0.10,0.15])
    ap.add_argument("--min-mag", type=float, nargs="+", default=[0.10,0.20])

    ap.add_argument("--action-th", type=float, nargs="+", default=[0.10])       # sign
    ap.add_argument("--linear-scale", type=float, nargs="+", default=[0.5,1.0]) # linear
    ap.add_argument("--hold-bars", type=int, nargs="+", default=[8,12])

    ap.add_argument("--outdir", default=None)
    ap.add_argument("--max-trials", type=int, default=None)

    ap.add_argument("--stream", action="store_true", help="stream backtest output live (tee to log)")
    ap.add_argument("--progress-every", type=int, default=25, help="print gate-build progress every N rows")

    # NEW: controlled noise for params
    ap.add_argument("--print-every", type=int, default=10, help="print a full summary line every N trials")
    ap.add_argument("--live-files-prefix", default="runs/live", help="prefix for live .now.jsonl/.now.csv outputs")
    args = ap.parse_args()

    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    outdir = Path(args.outdir or f"runs/sweep_{args.mode}_{ts}").resolve()
    (outdir / "gates").mkdir(parents=True, exist_ok=True)
    results_csv = outdir / "results.csv"
    live_prefix = Path(args.live_files_prefix)

    _, rows = read_states(args.states)
# === SAFE FOOTER (clean) =====================================================
# Self-contained footer that re-parses minimal args and prints a ready command.
import os, csv, glob, argparse

def _f(x, d=0.0):
    try: return float(x)
    except: return d

def _pick_best(results_csv):
    if not os.path.exists(results_csv): return None
    with open(results_csv, newline='') as f:
        rows = list(csv.DictReader(f))
    if not rows: return None
    # Rank by equity_end then Sharpe_like
    rows.sort(key=lambda r: (_f(r.get('equity_end', 0)), _f(r.get('Sharpe_like', 0))), reverse=True)
    # Ensure a tag exists
    for r in rows:
        if not r.get('tag'):
            r['tag'] = (
                f"k{r.get('min_agree_k','?')}"
                f"_eps{r.get('agree_eps','?')}"
                f"_mag{r.get('min_mag','?')}"
                f"_th{r.get('action_thresh','') or ''}"
                f"_hold{r.get('hold_bars','') or ''}"
            )
    return rows[0]

def _find_latest_sweep_dir(base="runs", mode="sign"):
    pattern = os.path.join(base, f"sweep_{mode}_*")
    cands = [d for d in glob.glob(pattern) if os.path.isdir(d)]
    cands = [d for d in cands if os.path.exists(os.path.join(d, "results.csv"))]
    if not cands: return None
    cands.sort(key=lambda d: os.path.getmtime(os.path.join(d, "results.csv")), reverse=True)
    return cands[0]

try:
    # Re-parse the minimal flags needed for the ready-to-run command
    _p = argparse.ArgumentParser(add_help=False)
    _p.add_argument("--states", default="states.csv")
    _p.add_argument("--mode", choices=["sign","linear"], default="sign")
    _p.add_argument("--H", type=int, default=40)
    _p.add_argument("--fee-bps", dest="fee_bps", type=float, default=5.0)
    _a, _ = _p.parse_known_args()

    latest = _find_latest_sweep_dir("runs", mode=_a.mode)
    if not latest:
        print("[warn] no sweep directory with results.csv found.")
    else:
        results_csv = os.path.join(latest, "results.csv")
        best = _pick_best(results_csv)
        if not best:
            print("[warn] results.csv empty — no best row to print.")
        else:
            gate_csv = best.get('gate_csv','')
            tag      = best.get('tag','')
            win      = best.get('trade_winrate','')
            hit      = best.get('per_bar_hitrate','')
            eq       = best.get('equity_end','')
            sh       = best.get('Sharpe_like','')
            dd       = best.get('MaxDD','')
            hold     = best.get('hold_bars','')
            th       = best.get('action_thresh','')
            ls       = best.get('linear_scale','')

            print("\n=== Recommended Params (no live execution) ===")
            print(f"Sweep  : {latest}")
            print(f"Gate CSV: {gate_csv}")
            print(f"Tag     : {tag}")
            print(f"KPI     : Win {win} | Hit {hit} | Eq {eq} | Sharpe {sh} | MaxDD {dd}")

            gates_dir = os.path.join(latest, "gates")
            gate_path = os.path.join(gates_dir, gate_csv) if gate_csv else "<missing_gate_csv>"

            print("\n# Backtest this exact gate again:")
            print("python3 backtest_gate_horizon_v3.py \\")
            print(f"  --states {_a.states} --gate {gate_path} \\")
            if _a.mode == "sign":
                print(f"  --h {_a.H} --mode sign --action_thresh {th} --hold_bars {hold} --fee_bps {_a.fee_bps}")
            else:
                print(f"  --h {_a.H} --mode linear --linear_scale {ls} --hold_bars {hold} --fee_bps {_a.fee_bps}")
except Exception as e:
    print(f"[warn] footer print failed: {e}")
# === END SAFE FOOTER =========================================================

=== ./adapters/mexc15s/rl_duckdb_data_layer_custom.py ===

#!/usr/bin/env python3
# DO NOT EDIT: generated from /home/bigdan7/Documents/TRADING/high_level_pipeline/ob_mapping.json
# Custom, non-destructive data layer (keeps originals intact).

import argparse
from typing import List, Optional, Dict
import duckdb, numpy as np, pandas as pd

TS_EXPR   = r"""COALESCE(TRY_CAST("ts" AS TIMESTAMP), to_timestamp(CAST(TRY_CAST("ts" AS BIGINT) AS DOUBLE)/1000.0), to_timestamp(CAST(TRY_CAST(ts_ms AS BIGINT) AS DOUBLE)/1000.0), to_timestamp(CAST(TRY_CAST(timestamp_ms AS BIGINT) AS DOUBLE)/1000.0), to_timestamp(CAST(TRY_CAST(time_ms AS BIGINT) AS DOUBLE)/1000.0))"""
BID_PX_EX = r"""COALESCE(CAST("bid_px_1" AS DOUBLE), CAST("bid_px_2" AS DOUBLE), CAST("bid_px_3" AS DOUBLE), CAST("bid_px_4" AS DOUBLE), CAST("bid_px_5" AS DOUBLE), CAST("bid_px_6" AS DOUBLE))"""
ASK_PX_EX = r"""COALESCE(CAST("ask_px_1" AS DOUBLE), CAST("ask_px_2" AS DOUBLE), CAST("ask_px_3" AS DOUBLE), CAST("ask_px_4" AS DOUBLE), CAST("ask_px_5" AS DOUBLE), CAST("ask_px_6" AS DOUBLE))"""
BID_SZ_EX = r"""COALESCE(CAST("bid_sz_1" AS DOUBLE), CAST("bid_sz_2" AS DOUBLE), CAST("bid_sz_3" AS DOUBLE), CAST("bid_sz_4" AS DOUBLE), CAST("bid_sz_5" AS DOUBLE), CAST("bid_sz_6" AS DOUBLE), 1.0)"""
ASK_SZ_EX = r"""COALESCE(CAST("ask_sz_1" AS DOUBLE), CAST("ask_sz_2" AS DOUBLE), CAST("ask_sz_3" AS DOUBLE), CAST("ask_sz_4" AS DOUBLE), CAST("ask_sz_5" AS DOUBLE), CAST("ask_sz_6" AS DOUBLE), 1.0)"""

class ReplayBuffer:
    def __init__(self, capacity:int, state_dim:int, act_dim:int=0):
        self.capacity=int(capacity); self.state_dim=int(state_dim); self.act_dim=int(act_dim)
        self.ptr=0; self.full=False
        self.s=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.sp=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.r=np.zeros((self.capacity,1),dtype=np.float32)
        self.d=np.zeros((self.capacity,1),dtype=np.float32)
        self.a=None
        if self.act_dim>0: self.a=np.zeros((self.capacity,self.act_dim),dtype=np.float32)
    def add(self,s,a,r,sp,d):
        i=self.ptr; self.s[i]=s; self.sp[i]=sp; self.r[i]=r; self.d[i]=d
        if self.a is not None and a is not None: self.a[i]=a
        self.ptr=(self.ptr+1)%self.capacity; self.full = self.full or (self.ptr==0)
    def size(self): return self.capacity if self.full else self.ptr
    def sample(self,batch_size:int,rng:np.random.Generator)->Dict[str,np.ndarray]:
        n=self.size(); idx=rng.integers(0,n,size=batch_size)
        out={"s":self.s[idx],"r":self.r[idx],"sp":self.sp[idx],"d":self.d[idx]}
        if self.a is not None: out["a"]=self.a[idx]
        return out

def build_sql(parquet_glob:str,start_ts:Optional[str],end_ts:Optional[str],horizon_rows:int,ret_threshold:float,extra_filters:Optional[str]=None)->str:
    where=[]
    if start_ts: where.append(f"ts_parsed >= TIMESTAMP '{start_ts}'")
    if end_ts:   where.append(f"ts_parsed <= TIMESTAMP '{end_ts}'")
    if extra_filters: where.append(f"({extra_filters})")
    where_clause = "WHERE " + " AND ".join(where) if where else ""
    sql = f"""
WITH raw AS (
  SELECT * FROM read_parquet('{parquet_glob}', filename=true)
),
proj AS (
  SELECT
    {TS_EXPR}   AS ts_parsed,
    {BID_PX_EX} AS bid_px_d,
    {ASK_PX_EX} AS ask_px_d,
    {BID_SZ_EX} AS bid_sz_d,
    {ASK_SZ_EX} AS ask_sz_d
  FROM raw
),
src AS ( SELECT * FROM proj {where_clause} ),
ord AS ( SELECT * FROM src ORDER BY ts_parsed ),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px_d + ask_px_d)*0.5 AS mid,
    (ask_px_d - bid_px_d)     AS spread,
    CASE WHEN (bid_sz_d + ask_sz_d)>0
         THEN (bid_sz_d - ask_sz_d)/NULLIF(bid_sz_d + ask_sz_d,0)
         ELSE 0 END           AS imbalance,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,1) OVER ()) AS mom1,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px_d + ask_px_d)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
),
lab AS (
  SELECT
    *,
    LEAD(mid, {horizon_rows}) OVER () AS mid_fwd,
    (LEAD(mid, {horizon_rows}) OVER () - mid) / NULLIF(mid,0) AS fwd_ret
  FROM feat
),
final AS (
  SELECT
    ts,
    mid, spread, imbalance, mom1, mom3, vol3,
    LEAD(mid, {horizon_rows}) OVER ()        AS mid_p,
    LEAD(spread, {horizon_rows}) OVER ()     AS spread_p,
    LEAD(imbalance, {horizon_rows}) OVER ()  AS imbalance_p,
    LEAD(mom1, {horizon_rows}) OVER ()       AS mom1_p,
    LEAD(mom3, {horizon_rows}) OVER ()       AS mom3_p,
    LEAD(vol3, {horizon_rows}) OVER ()       AS vol3_p,
    fwd_ret,
    CASE WHEN fwd_ret > {ret_threshold} THEN 1
         WHEN fwd_ret < -{ret_threshold} THEN -1
         ELSE 0 END AS label
  FROM lab
)
SELECT * FROM final
WHERE mid IS NOT NULL AND mid_p IS NOT NULL
"""
    return sql

def _ensure_float(df: pd.DataFrame, cols: List[str])->pd.DataFrame:
    for c in cols:
        if c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    return df

def stream_into_buffer(con:duckdb.DuckDBPyConnection, sql:str, buffer:ReplayBuffer,
                       chunk_rows:int=200_000, drop_noise_label:bool=False, derive_action:bool=False)->int:
    total=0; cur=con.execute(sql)
    state_cols=["mid","spread","imbalance","mom1","mom3","vol3"]
    next_cols=["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
    while True:
        df=cur.fetch_df_chunk(chunk_rows)
        if df is None or len(df)==0: break
        df=_ensure_float(df, list(df.columns))
        if drop_noise_label and "label" in df.columns: df=df[df["label"]!=0]
        df=df.dropna(subset=state_cols+next_cols+["fwd_ret"])
        S=df[state_cols].to_numpy(np.float32)
        SP=df[next_cols].to_numpy(np.float32)
        R=df[["fwd_ret"]].to_numpy(np.float32)
        D=np.zeros((len(df),1),dtype=np.float32)
        if derive_action:
            A=df[["label"]].to_numpy(np.float32) if "label" in df.columns else np.sign(R).astype(np.float32)
        else:
            A=None
        for i in range(len(df)):
            buffer.add(S[i], (A[i] if A is not None else None), R[i], SP[i], D[i])
        total+=len(df)
    return total

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    args=ap.parse_args()
    con=duckdb.connect(database=":memory:")
    sql=build_sql(args.parquet,args.start,args.end,args.h,args.th,None)
    plan=con.execute(f"EXPLAIN {sql}").fetchall()
    print("[DuckDB plan]"); [print(" ",r[0]) for r in plan]

=== ./pick_best_from_results.py ===

#!/usr/bin/env python3
import csv, sys, os, math

# usage: pick_best_from_results.py runs/<sweep_dir>/results.csv
# prints two lines to stdout:
# GATE=gate_....csv
# TAG=....

if len(sys.argv) != 2:
    print("usage: pick_best_from_results.py PATH_TO_results.csv", file=sys.stderr)
    sys.exit(2)

csv_path = sys.argv[1]
if not os.path.isfile(csv_path):
    print(f"[ERR] results.csv not found: {csv_path}", file=sys.stderr)
    sys.exit(1)

rows = []
with open(csv_path, newline="") as f:
    R = csv.DictReader(f)
    for r in R:
        # Skip partial/blank rows
        if not r.get("gate_csv"): 
            continue
        try:
            trades   = int(float(r.get("trades","0")))
            eq_end   = float(r.get("equity_end","nan"))
            sharpe   = float(r.get("Sharpe_like","nan"))
            maxdd    = float(r.get("MaxDD","nan"))
        except:
            continue

        # Filters (tune as you like)
        if trades < 3:                   # avoid too-few-sample mirages
            continue
        if math.isnan(eq_end) or math.isnan(sharpe) or math.isnan(maxdd):
            continue
        # be conservative on test: want >=1.0 eq, non-negative sharpe, moderate DD
        if eq_end < 1.0 or sharpe < 0.0 or maxdd > 1.5:
            continue

        rows.append(r)

if not rows:
    print("[WARN] No rows passed filters; falling back to 'best equity' among all rows.", file=sys.stderr)
    # fallback: just rank everything we can parse
    alt = []
    with open(csv_path, newline="") as f:
        R = csv.DictReader(f)
        for r in R:
            try:
                eq_end   = float(r.get("equity_end","nan"))
                sharpe   = float(r.get("Sharpe_like","nan"))
                maxdd    = float(r.get("MaxDD","inf"))
                trades   = int(float(r.get("trades","0")))
            except:
                continue
            if r.get("gate_csv"):
                alt.append((eq_end, sharpe, -maxdd, trades, r))
    if not alt:
        print("[ERR] results.csv has no usable rows.", file=sys.stderr)
        sys.exit(1)
    rows = [sorted(alt, key=lambda t:(t[0], t[1], t[2], t[3]), reverse=True)[0][-1]]

# Rank: equity_end desc, Sharpe desc, MaxDD asc, trades desc
def keyfn(r):
    return (
        float(r["equity_end"]),
        float(r["Sharpe_like"]),
        -float(r["MaxDD"]),           # smaller DD is better → negative for descending sort
        float(r["trades"])
    )

rows.sort(key=keyfn, reverse=True)
best = rows[0]
print(f"GATE={best['gate_csv']}")
print(f"TAG={best.get('tag','')}")

=== ./probe_td3bc_checkpoints.py ===

#!/usr/bin/env python3
# probe_td3bc_checkpoints.py
# Prints: format (jit/module/state_dict), md5 of each tensor, layer shapes order, quick stats
import argparse, hashlib, json, os, sys, re
from typing import Dict, Any, List, Tuple

try:
    import torch
    import torch.nn as nn
except ImportError:
    print("[error] torch required", file=sys.stderr); sys.exit(2)

def md5_tensor(t):
    b = t.detach().cpu().numpy().tobytes()
    return hashlib.md5(b).hexdigest()

def load_any(path, device="cpu"):
    # 1) jit
    try:
        m = torch.jit.load(path, map_location=device)
        return ("jit", m)
    except Exception:
        pass
    obj = torch.load(path, map_location=device)
    if isinstance(obj, nn.Module):
        return ("module", obj)
    if isinstance(obj, dict):
        sd = obj.get("state_dict", None) or obj
        if isinstance(sd, dict):
            return ("state_dict", sd)
    return ("unknown", obj)

def parse_layers_from_state_dict(sd: Dict[str, Any]) -> List[Tuple[str, Tuple[int,...], Tuple[int,...]]]:
    pairs = []
    for k in sd:
        if k.endswith(".weight"):
            p = k[:-7]
            bk = p + ".bias"
            if bk in sd:
                w = sd[k]; b = sd[bk]
                wshape = tuple(w.shape); bshape = tuple(b.shape)
                pairs.append((p, wshape, bshape))
    def idx(name: str) -> int:
        ms = list(re.finditer(r'(\d+)', name))
        return int(ms[-1].group(1)) if ms else 10**9
    pairs.sort(key=lambda x: (idx(x[0]), x[0]))
    return pairs

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("ckpts", nargs="+", help="*.pt files")
    args = ap.parse_args()
    rows = []
    for p in args.ckpts:
        if not os.path.isfile(p):
            print(f"[error] missing: {p}", file=sys.stderr); sys.exit(1)
        kind, obj = load_any(p)
        print(f"\n== {p} :: {kind} ==")
        if kind in ("jit","module"):
            # Try state_dict for checksum
            try:
                sd = obj.state_dict()
            except Exception:
                sd = {}
        elif kind == "state_dict":
            sd = obj
        else:
            print("[warn] unknown format; skipping details"); continue

        # layer order
        layers = parse_layers_from_state_dict(sd)
        for i,(name, wsh, bsh) in enumerate(layers):
            w = sd[name+".weight"]; b = sd[name+".bias"]
            print(f"  L{i:02d} {name}: W{wsh} B{bsh} | w(md5)={md5_tensor(w)[:8]} b(md5)={md5_tensor(b)[:8]} "
                  f"| w[min={float(w.min()):+.4f}, max={float(w.max()):+.4f}] b[min={float(b.min()):+.4f}, max={float(b.max()):+.4f}]")
        # global fingerprint
        h = hashlib.md5()
        for k in sorted(sd.keys()):
            if hasattr(sd[k], "shape"):
                h.update(sd[k].cpu().numpy().tobytes())
        print(f"  >> checkpoint_fingerprint: {h.hexdigest()}")

if __name__ == "__main__":
    main()

=== ./train_td3bc_per_ensemble.py ===

#!/usr/bin/env python3
# TD3+BC with PER + N-step replay and K-model ensemble training + consensus gate export.
# Robust to schema drift; logs ingest & training cleanly.

import argparse
import json
import time
from typing import List

import duckdb
import numpy as np
import torch
import torch.nn.functional as F
import pandas as pd

from rl_duckdb_data_layer import build_sql
from per_nstep_replay import PERNStepReplay
from td3_bc_agent import TD3BC

STATE_COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]
NEXT_COLS  = ["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
REQUIRED   = STATE_COLS + NEXT_COLS + ["fwd_ret"]

def td_error_min_q(agent: TD3BC, batch: dict, device="cpu"):
    with torch.no_grad():
        s  = torch.from_numpy(batch["s"]).to(device)
        a  = torch.from_numpy(batch["a"]).to(device)
        r  = torch.from_numpy(batch["r"]).to(device)
        sp = torch.from_numpy(batch["sp"]).to(device)
        d  = torch.from_numpy(batch["d"]).to(device)

        noise = (torch.randn_like(a) * agent.target_noise).clamp(-agent.noise_clip, agent.noise_clip)
        ap = (agent.actor_targ(sp) + noise).clamp(-agent.act_limit, agent.act_limit)

        q1_tp = agent.q1_targ(sp, ap)
        q2_tp = agent.q2_targ(sp, ap)
        q_tp_min = torch.minimum(q1_tp, q2_tp)
        y = r + (1.0 - d) * agent.gamma * q_tp_min

        q1 = agent.q1(s, a)
        q2 = agent.q2(s, a)
        qmin = torch.minimum(q1, q2)
        td = (y - qmin).squeeze(-1).cpu().numpy()
        return td

def train_one_model(
    per: PERNStepReplay,
    seed: int,
    steps: int,
    batch_size: int,
    device: str,
    bc_lambda: float,
    policy_delay: int,
    print_every: int = 200,
):
    np.random.seed(seed)
    torch.manual_seed(seed)

    agent = TD3BC(
        s_dim=6, a_dim=1, act_limit=1.0,
        actor_lr=1e-3, critic_lr=1e-3,
        gamma=0.99, tau=0.005, policy_delay=policy_delay,
        bc_lambda=bc_lambda,
        target_noise=0.2, noise_clip=0.5,
        device=device
    )

    it = 0
    while it < steps:
        batch = per.sample(batch_size)
        if "a" not in batch:
            raise RuntimeError("PER batch missing actions; ensure labels->actions were derived during ingest.")

        s  = torch.from_numpy(batch["s"]).to(device)
        a  = torch.from_numpy(batch["a"]).to(device)
        r  = torch.from_numpy(batch["r"]).to(device)
        sp = torch.from_numpy(batch["sp"]).to(device)
        d  = torch.from_numpy(batch["d"]).to(device)
        w  = torch.from_numpy(batch["w"]).to(device)  # importance weights

        # Critic targets (TD3 target policy smoothing)
        with torch.no_grad():
            noise = (torch.randn_like(a) * agent.target_noise).clamp(-agent.noise_clip, agent.noise_clip)
            ap = (agent.actor_targ(sp) + noise).clamp(-agent.act_limit, agent.act_limit)
            q1_tp = agent.q1_targ(sp, ap)
            q2_tp = agent.q2_targ(sp, ap)
            q_tp_min = torch.minimum(q1_tp, q2_tp)
            y = r + (1.0 - d) * agent.gamma * q_tp_min

        # Critic losses
        q1_pred = agent.q1(s, a)
        q2_pred = agent.q2(s, a)
        q1_loss = (w * (q1_pred - y).pow(2)).mean()
        q2_loss = (w * (q2_pred - y).pow(2)).mean()

        agent.q1_optim.zero_grad(set_to_none=True)
        q1_loss.backward()
        agent.q1_optim.step()

        agent.q2_optim.zero_grad(set_to_none=True)
        q2_loss.backward()
        agent.q2_optim.step()

        pi_loss_val = 0.0
        agent.total_it += 1

        # Delayed actor + Polyak
        if agent.total_it % agent.policy_delay == 0:
            pi = agent.actor(s)
            q_pi = agent.q1(s, pi)
            bc_loss = F.mse_loss(pi, a)
            pi_loss = (-q_pi.mean()) + bc_lambda * bc_loss

            agent.pi_optim.zero_grad(set_to_none=True)
            pi_loss.backward()
            agent.pi_optim.step()
            pi_loss_val = float(pi_loss.item())

            from rl_utils import polyak_update
            polyak_update(agent.actor_targ, agent.actor, agent.tau)
            polyak_update(agent.q1_targ, agent.q1, agent.tau)
            polyak_update(agent.q2_targ, agent.q2, agent.tau)

        # PER priority update via min-Q TD error
        td = td_error_min_q(agent, batch, device=device)
        per.update_priorities(batch["idx"], td)

        it += 1
        if it % print_every == 0 or it == steps:
            print(f"[seed {seed}] step {it}/{steps} | q1={q1_loss.item():.5f} q2={q2_loss.item():.5f} pi={{:.5f}}".format(pi_loss_val))

    # Save weights
    torch.save(agent.actor.state_dict(), f"td3bc_actor_seed{seed}.pt")
    torch.save(agent.q1.state_dict(),    f"td3bc_q1_seed{seed}.pt")
    torch.save(agent.q2.state_dict(),    f"td3bc_q2_seed{seed}.pt")
    return agent

def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    for c in df.columns:
        if c != "ts":
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    ap.add_argument("--buf", type=int, default=1_800_000)
    ap.add_argument("--chunk", type=int, default=200_000)
    ap.add_argument("--drop_noise", action="store_true")
    ap.add_argument("--nstep", type=int, default=3)
    ap.add_argument("--gamma", type=float, default=0.99)
    ap.add_argument("--alpha", type=float, default=0.6)
    ap.add_argument("--beta", type=float, default=0.4)
    ap.add_argument("--models", type=int, default=3, help="K models in ensemble")
    ap.add_argument("--steps", type=int, default=10000)
    ap.add_argument("--batch", type=int, default=512)
    ap.add_argument("--policy_delay", type=int, default=2)
    ap.add_argument("--bc_lambda", type=float, default=0.1)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--seeds", default="42,43,44", help="comma-separated seeds; auto-extends if needed")
    ap.add_argument("--agree_eps", type=float, default=0.15, help="agreement tolerance on actions")
    ap.add_argument("--q_min_thresh", type=float, default=-1e9, help="optional Q threshold for consensus")
    args = ap.parse_args()

    # PER buffer
    per = PERNStepReplay(
        capacity=args.buf,
        state_dim=6,
        act_dim=1,
        n_step=args.nstep,
        gamma=args.gamma,
        alpha=args.alpha,
        beta=args.beta,
        seed=7
    )

    # SQL that handles schema drift
    con = duckdb.connect(database=":memory:")
    sql = build_sql(args.parquet, args.start, args.end, args.h, args.th, None)

    try:
        plan = con.execute(f"EXPLAIN {sql}").fetchall()
        print("[DuckDB plan]")
        for row in plan:
            print(" ", row[0])
    except Exception as e:
        print(f"[DuckDB plan] skipped: {e}")

    total = 0
    have_cols_printed = False
    cur = con.execute(sql)
    while True:
        df = cur.fetch_df_chunk(args.chunk)
        if df is None or len(df) == 0:
            break

        df = coerce_numeric(df)
        if args.drop_noise and "label" in df.columns:
            before = len(df)
            df = df[df["label"] != 0]
            print(f"[ingest] dropped noise labels: {before - len(df)}")

        if not have_cols_printed:
            print(f"[ingest] columns: {list(df.columns)}")
            have_cols_printed = True

        missing = [c for c in REQUIRED if c not in df.columns]
        if missing:
            raise KeyError(f"Required columns missing from SQL result: {missing}")

        before = len(df)
        df = df.dropna(subset=REQUIRED)
        dropped = before - len(df)
        if dropped:
            print(f"[ingest] NaN-dropped rows: {dropped}")

        if len(df) == 0:
            continue

        S  = df[STATE_COLS].to_numpy(np.float32)
        SP = df[NEXT_COLS].to_numpy(np.float32)
        R  = df[["fwd_ret"]].to_numpy(np.float32)
        D  = np.zeros((len(df), 1), dtype=np.float32)

        if "label" in df.columns:
            A = df[["label"]].to_numpy(np.float32)
        else:
            A = np.sign(R).astype(np.float32)

        for i in range(len(df)):
            per.add(S[i], A[i], R[i], SP[i], D[i], priority=None)

        total += len(df)
        if total % (args.chunk * 2) < len(df):
            print(f"[ingest] total rows -> {total}")

    print(f"[ingest] transitions loaded: {total} | per.size={per.size()}")

    seeds = [int(x.strip()) for x in args.seeds.split(",") if x.strip()]
    while len(seeds) < args.models:
        seeds.append(seeds[-1] + 1 if seeds else 42)

    ensemble = []
    for k in range(args.models):
        print(f"\n=== Training model {k+1}/{args.models} (seed {seeds[k]}) ===")
        t0 = time.time()
        agent = train_one_model(
            per=per,
            seed=seeds[k],
            steps=args.steps,
            batch_size=args.batch,
            device=args.device,
            bc_lambda=args.bc_lambda,
            policy_delay=args.policy_delay,
        )
        dt = time.time() - t0
        print(f"[model {k+1}] done in {dt:.1f}s")
        ensemble.append({"seed": seeds[k], "actor": f"td3bc_actor_seed{seeds[k]}.pt"})

    cfg = {
        "type": "td3bc_consensus_gate",
        "actors": [e["actor"] for e in ensemble],
        "agree_eps": args.agree_eps,
        "q_min_thresh": args.q_min_thresh,
        "state_dim": 6,
        "act_limit": 1.0
    }
    with open("consensus_gate_td3bc.json", "w") as f:
        json.dump(cfg, f, indent=2)
    print("Saved consensus gate: consensus_gate_td3bc.json")

if __name__ == "__main__":
    main()

=== ./eval_actors_snapshot.py ===

#!/usr/bin/env python3
import argparse, json, os, time, subprocess, sys, glob, csv
import numpy as np
import pandas as pd
import torch

def find_actors(actors_dir):
    # pick three seeds if available; otherwise any *.pt
    pts = sorted(glob.glob(os.path.join(actors_dir, "td3bc_actor_seed*.pt")))
    if not pts:
        pts = sorted(glob.glob(os.path.join(actors_dir, "*.pt")))
    return pts[:3]

def run_gate(gate_json, state_csv, device, out_csv):
    # Use your flexible gate to produce actions for states
    cmd = [
        sys.executable, "consensus_gate_td3bc_flexible.py",
        "--gate", gate_json,
        "--state_csv", state_csv,
        "--device", device
    ]
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    rows = []
    for line in p.stdout:
        if line.startswith("row_idx,") or (len(line)>0 and line.split(",")[0].isdigit()):
            rows.append(line.strip())
    _, err = p.communicate()
    if p.returncode != 0:
        raise RuntimeError(f"gate failed: {err}")
    with open(out_csv, "w") as f:
        f.write("\n".join(rows) + "\n")
    return out_csv

def backtest(states_csv, gate_csv, H, mode, fee_bps, action_th=None, hold_bars=None, linear_scale=None):
    cmd = [sys.executable, "backtest_gate_horizon_v3.py",
           "--states", states_csv, "--gate", gate_csv,
           "--h", str(H), "--mode", mode, "--fee_bps", str(fee_bps)]
    if mode == "sign" and action_th is not None:
        cmd += ["--action_thresh", str(action_th)]
    if hold_bars is not None:
        cmd += ["--hold_bars", str(hold_bars)]
    if mode == "linear" and linear_scale is not None:
        cmd += ["--linear_scale", str(linear_scale)]
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    out = p.stdout
    if p.returncode != 0:
        raise RuntimeError(out)
    # parse KPI lines
    kpi = {"equity_end":None,"sharpe":None,"maxdd":None,"trades":None,"hitrate":None,"winrate":None}
    for ln in out.splitlines():
        if ln.strip().startswith("rows=") and "equity_end=" in ln:
            tok = ln.replace(",", " ").split()
            for t in tok:
                if t.startswith("equity_end="): kpi["equity_end"] = float(t.split("=")[1])
                if t.startswith("Sharpe_like="): kpi["sharpe"] = float(t.split("=")[1])
                if t.startswith("MaxDD="): kpi["maxdd"] = float(t.split("=")[1].replace("%",""))
                if t.startswith("trades="): kpi["trades"] = int(t.split("=")[1])
                if t.startswith("per_bar_hitrate="): kpi["hitrate"] = float(t.split("=")[1].replace("%",""))
                if t.startswith("trade_winrate="): kpi["winrate"] = float(t.split("=")[1].replace("%",""))
    return kpi, out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--actors-dir", default=".")
    ap.add_argument("--gate", default="consensus_gate_td3bc.json")
    ap.add_argument("--states", default="states.csv")
    ap.add_argument("--scaler", default="scaler.json")
    ap.add_argument("--device", choices=["cpu","cuda","mps"], default="cpu")
    ap.add_argument("--H", type=int, default=40)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--fee-bps", type=float, default=5.0)
    ap.add_argument("--action-th", type=float, default=0.05)
    ap.add_argument("--hold-bars", type=int, default=8)
    ap.add_argument("--linear-scale", type=float, default=0.5)
    ap.add_argument("--outdir", default="runs/eval_watch")
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)
    # Sanity
    if not os.path.exists(args.states):
        sys.exit(f"[error] states not found: {args.states}")
    if not os.path.exists(args.gate):
        sys.exit(f"[error] gate JSON not found: {args.gate}")

    # Discover actors (not used directly here; gate JSON already references names on disk)
    actors = find_actors(args.actors-dir if hasattr(args, "actors-dir") else args.actors_dir)

    # Build gate actions CSV
    gate_csv = os.path.join(args.outdir, f"gate_eval_{int(time.time())}.csv")
    run_gate(args.gate, args.states, args.device, gate_csv)

    # Backtest
    kpi, raw = backtest(args.states, gate_csv, args.H, args.mode, args.fee_bps,
                        action_th=(args.action_th if args.mode=="sign" else None),
                        hold_bars=args.hold_bars,
                        linear_scale=(args.linear_scale if args.mode=="linear" else None))

    # Append eval CSV
    eval_csv = os.path.join(args.outdir, "eval.csv")
    write_header = not os.path.exists(eval_csv)
    with open(eval_csv, "a", newline="") as f:
        w = csv.DictWriter(f, fieldnames=[
            "ts","H","mode","fee_bps","action_th","linear_scale","hold_bars",
            "equity_end","sharpe","maxdd","trades","hitrate","winrate","gate_csv"])
        if write_header: w.writeheader()
        w.writerow(dict(
            ts=int(time.time()), H=args.H, mode=args.mode, fee_bps=args.fee_bps,
            action_th=(args.action_th if args.mode=="sign" else ""),
            linear_scale=(args.linear_scale if args.mode=="linear" else ""),
            hold_bars=args.hold_bars,
            equity_end=kpi["equity_end"], sharpe=kpi["sharpe"], maxdd=kpi["maxdd"],
            trades=kpi["trades"], hitrate=kpi["hitrate"], winrate=kpi["winrate"],
            gate_csv=os.path.basename(gate_csv)
        ))

    # Human summary
    print("[eval]")
    print(f" gate_csv = {gate_csv}")
    print(f" equity_end={kpi['equity_end']}  Sharpe_like={kpi['sharpe']}  MaxDD={kpi['maxdd']}%")
    print(f" trades={kpi['trades']}  win%={kpi['winrate']}  hit%={kpi['hitrate']}")

=== ./latest_state_from_parquet.py ===

#!/usr/bin/env python3
# latest_state_from_parquet.py
# Reads your snaps parquet glob, builds features, prints the LAST row as:
# mid,spread,imbalance,mom1,mom3,vol3
import argparse, duckdb, sys

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help='e.g. "/path/**/*.parquet"')
    ap.add_argument("--mom1_lag", type=int, default=1)
    ap.add_argument("--mom3_lag", type=int, default=3)
    ap.add_argument("--vol_win",  type=int, default=10, help="stddev window (rows)")
    args = ap.parse_args()

    con = duckdb.connect(database=":memory:")
    sql = f"""
WITH raw AS (
  SELECT * FROM read_parquet('{args.parquet}', filename=true, union_by_name=true)
),
proj AS (
  SELECT
    COALESCE(TRY_CAST("ts" AS TIMESTAMP), TRY_CAST("timestamp" AS TIMESTAMP)) AS ts_parsed,
    CAST(bid_px_1 AS DOUBLE) AS bid_px,
    CAST(ask_px_1 AS DOUBLE) AS ask_px,
    CAST(bid_sz_1 AS DOUBLE) AS bid_sz,
    CAST(ask_sz_1 AS DOUBLE) AS ask_sz
  FROM raw
),
ord AS (
  SELECT * FROM proj WHERE ts_parsed IS NOT NULL ORDER BY ts_parsed
),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px + ask_px)*0.5 AS mid,
    (ask_px - bid_px)     AS spread,
    CASE WHEN (bid_sz + ask_sz) > 0
         THEN (bid_sz - ask_sz)/NULLIF(bid_sz + ask_sz,0)
         ELSE 0 END       AS imbalance,
    ((bid_px + ask_px)*0.5 - LAG((bid_px + ask_px)*0.5, {args.mom1_lag}) OVER ()) AS mom1,
    ((bid_px + ask_px)*0.5 - LAG((bid_px + ask_px)*0.5, {args.mom3_lag}) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px + ask_px)*0.5) OVER (ROWS BETWEEN {args.vol_win-1} PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
)
SELECT mid, spread, imbalance, mom1, mom3, vol3
FROM feat
WHERE mid IS NOT NULL
ORDER BY ts DESC
LIMIT 1
"""
    df = con.execute(sql).fetchdf()
    if df is None or len(df)==0:
        print("[error] no rows produced", file=sys.stderr); sys.exit(2)
    row = df.iloc[0].tolist()
    # print a single CSV line (no header)
    print(",".join(f"{float(x):.10f}" if x is not None else "0.0" for x in row))

if __name__ == "__main__":
    main()

=== ./compare_gates.py ===

#!/usr/bin/env python3
# compare_gates.py — grid search both gate files and print best config per file.
import argparse, subprocess, sys, itertools, re

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        print(p.stderr.strip(), file=sys.stderr); sys.exit(p.returncode)
    return p.stdout.strip()

def parse_summary(out):
    # Extract the last two lines from backtest_gate_horizon.py output
    lines = [ln for ln in out.splitlines() if ln.strip()]
    hdr = next((ln for ln in lines if ln.startswith("[backtest H]")), "")
    met = next((ln for ln in lines if ln.startswith(" equity_end=") or "equity_end=" in ln), "")
    return hdr, met

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate_a", required=True, help="first gate csv (e.g., gate_out.csv)")
    ap.add_argument("--gate_b", required=True, help="second gate csv (e.g., gate_out_sign.csv)")
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--fee_bps", type=float, default=5.0)
    args = ap.parse_args()

    th_grid = [0.03, 0.05, 0.08, 0.10, 0.15]
    hold_grid = [0, 4, 8, 12]

    best = {}
    for gate in [args.gate_a, args.gate_b]:
        best[gate] = None
        for th, hold in itertools.product(th_grid, hold_grid):
            out = run([sys.executable, "backtest_gate_horizon.py",
                      "--states", args.states, "--gate", gate,
                      "--h", str(args.h), "--mode", "sign",
                      "--action_thresh", str(th), "--hold_bars", str(hold),
                      "--fee_bps", str(args.fee_bps)])
            hdr, met = parse_summary(out)
            # crude score: prioritize Sharpe, then equity_end
            m = re.search(r"equity_end=([\d\.]+)\s+Sharpe_like=([-\d\.]+)\s+MaxDD=([\d\.%]+)", met)
            if not m: continue
            eq = float(m.group(1)); sh = float(m.group(2))
            score = (sh, eq)
            if best[gate] is None or score > best[gate][0]:
                best[gate] = ((sh, eq), th, hold, hdr, met)

    for gate in [args.gate_a, args.gate_b]:
        if best[gate] is None:
            print(f"[{gate}] no result"); continue
        (sh, eq), th, hold, hdr, met = best[gate]
        print(f"\n=== {gate} ===")
        print(f"best: th={th:.3f} hold={hold} | Sharpe_like={sh:.2f} equity_end={eq:.4f}")
        print(hdr); print(met)

if __name__ == "__main__":
    main()

=== ./scan_and_generate_ob_config.py ===

#!/usr/bin/env python3
"""
scan_and_generate_ob_config.py
One-shot: scan Parquet schema -> infer timestamp & top-of-book bid/ask px/sz -> emit:
  - ob_mapping.json   (what was detected)
  - rl_duckdb_data_layer.py (FULL, ready-to-run, tailored to your schema)

Usage:
  python3 scan_and_generate_ob_config.py \
    --parquet "/abs/path/**/*.parquet" \
    --out-config ob_mapping.json \
    --out-datalayer rl_duckdb_data_layer.py
"""

import argparse, json, re, os
import duckdb
import pandas as pd

# -------- Utilities (version-safe) --------
def quote_ident(c: str) -> str:
    # Double-quote SQL identifier, doubling any inner quotes
    return '"' + c.replace('"', '""') + '"'

def list_columns(con, glob):
    q = f"DESCRIBE SELECT * FROM read_parquet('{glob}')"
    rows = con.execute(q).fetchall()
    return [(r[0], r[1]) for r in rows]  # (name, type)

def sample_values(con, glob, cols, n=20):
    if not cols: return None
    sel = ", ".join([quote_ident(c) for c in cols])
    q = f"SELECT {sel} FROM read_parquet('{glob}') LIMIT {n}"
    try:
        return con.execute(q).fetchdf()
    except Exception:
        return None

def to_level(name: str):
    m = re.search(r'_(\d+)$', name)
    return int(m.group(1)) if m else None

def find_candidates(cols, side, kind):
    names = [c[0] for c in cols]
    cands = []
    for n in names:
        ln = n.lower()
        if side in ln:
            if kind == "px" and ("px" in ln or "price" in ln):
                cands.append(n)
            elif kind == "sz" and ("sz" in ln or "size" in ln or "qty" in ln or "quantity" in ln):
                cands.append(n)
    # fallback if strict missed
    if not cands:
        for n in names:
            ln = n.lower()
            if side in ln:
                if kind == "px" and ("p" in ln or "price" in ln):
                    cands.append(n)
                if kind == "sz" and ("size" in ln or "qty" in ln or "quantity" in ln):
                    cands.append(n)
    cands = list(set(cands))
    cands.sort(key=lambda x: (999999 if to_level(x) is None else to_level(x), x))
    return cands

def ts_candidates(cols):
    out = []
    for c,_ in cols:
        lc = c.lower()
        if "ts" in lc or "time" in lc or "timestamp" in lc or "datetime" in lc:
            out.append(c)
    # keep order but unique
    seen = set(); uniq=[]
    for c in out:
        if c not in seen:
            seen.add(c); uniq.append(c)
    return uniq or ["ts"]  # default guess

def build_ts_expr(cands):
    parts = []
    for c in cands:
        ci = quote_ident(c)
        parts.append(f"TRY_CAST({ci} AS TIMESTAMP)")
    for c in cands:
        ci = quote_ident(c)
        parts.append(f"to_timestamp(CAST(TRY_CAST({ci} AS BIGINT) AS DOUBLE)/1000.0)")
    # common fallbacks
    for c in ["ts_ms","timestamp_ms","time_ms","epoch_ms","t_ms"]:
        parts.append(f"to_timestamp(CAST(TRY_CAST({c} AS BIGINT) AS DOUBLE)/1000.0)")
    parts.append("to_timestamp(CAST(TRY_CAST(CAST(ts AS DOUBLE) AS BIGINT) AS DOUBLE)/1000.0)")
    return "COALESCE(" + ", ".join(parts) + ")"

def build_num_expr(cands, default=None):
    parts = [f"CAST({quote_ident(c)} AS DOUBLE)" for c in cands]
    if default is not None:
        parts.append(default)
    if not parts:
        return "NULL"
    return "COALESCE(" + ", ".join(parts) + ")"

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--out-config", default="ob_mapping.json")
    ap.add_argument("--out-datalayer", default="rl_duckdb_data_layer.py")
    ap.add_argument("--sample", type=int, default=10)
    args = ap.parse_args()

    con = duckdb.connect()

    cols = list_columns(con, args.parquet)
    if not cols:
        raise SystemExit("No columns found. Check your --parquet path/pattern.")

    # preview first few columns just to show something in mapping
    df_preview = sample_values(con, args.parquet, [c for c,_ in cols][:min(12,len(cols))], args.sample)

    # detect
    ts_cands = ts_candidates(cols)
    bid_px_c = find_candidates(cols, "bid", "px")
    ask_px_c = find_candidates(cols, "ask", "px")
    bid_sz_c = find_candidates(cols, "bid", "sz")
    ask_sz_c = find_candidates(cols, "ask", "sz")

    # expressions
    ts_expr   = build_ts_expr(ts_cands)
    bid_px_ex = build_num_expr(bid_px_c)
    ask_px_ex = build_num_expr(ask_px_c)
    bid_sz_ex = build_num_expr(bid_sz_c, default="1.0")
    ask_sz_ex = build_num_expr(ask_sz_c, default="1.0")

    # write mapping JSON
    mapping = {
        "ts_expr": ts_expr,
        "bid_px_expr": bid_px_ex,
        "ask_px_expr": ask_px_ex,
        "bid_sz_expr": bid_sz_ex,
        "ask_sz_expr": ask_sz_ex,
        "columns_detected": {k:v for k,v in cols},
        "ts_candidates": ts_cands,
        "bid_px_candidates": bid_px_c,
        "ask_px_candidates": ask_px_c,
        "bid_sz_candidates": bid_sz_c,
        "ask_sz_candidates": ask_sz_c,
        "preview": (df_preview.to_dict(orient="records") if df_preview is not None else [])
    }
    with open(args.out_config, "w") as f:
        json.dump(mapping, f, indent=2)
    print(f"[ok] wrote mapping {os.path.abspath(args.out_config)}")

    # emit FULL tailored rl_duckdb_data_layer.py
    code = f"""#!/usr/bin/env python3
# Auto-generated by scan_and_generate_ob_config.py
# Tailored top-of-book mapping for your Parquet schema.

import argparse
from typing import List, Optional, Dict
import duckdb, numpy as np, pandas as pd

class ReplayBuffer:
    def __init__(self, capacity:int, state_dim:int, act_dim:int=0):
        self.capacity=int(capacity); self.state_dim=int(state_dim); self.act_dim=int(act_dim)
        self.ptr=0; self.full=False
        self.s=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.sp=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.r=np.zeros((self.capacity,1),dtype=np.float32)
        self.d=np.zeros((self.capacity,1),dtype=np.float32)
        self.a=None
        if self.act_dim>0: self.a=np.zeros((self.capacity,self.act_dim),dtype=np.float32)
    def add(self,s,a,r,sp,d):
        i=self.ptr; self.s[i]=s; self.sp[i]=sp; self.r[i]=r; self.d[i]=d
        if self.a is not None and a is not None: self.a[i]=a
        self.ptr=(self.ptr+1)%self.capacity; self.full = self.full or (self.ptr==0)
    def size(self): return self.capacity if self.full else self.ptr
    def sample(self,batch_size:int,rng:np.random.Generator)->Dict[str,np.ndarray]:
        n=self.size(); idx=rng.integers(0,n,size=batch_size)
        out={{"s":self.s[idx],"r":self.r[idx],"sp":self.sp[idx],"d":self.d[idx]}}
        if self.a is not None: out["a"]=self.a[idx]
        return out

def build_sql(parquet_glob:str,start_ts:Optional[str],end_ts:Optional[str],horizon_rows:int,ret_threshold:float,extra_filters:Optional[str]=None)->str:
    where=[]
    if start_ts: where.append(f"ts_parsed >= TIMESTAMP '{{start_ts}}'")
    if end_ts:   where.append(f"ts_parsed <= TIMESTAMP '{{end_ts}}'")
    if extra_filters: where.append(f"({{extra_filters}})")
    where_clause = "WHERE " + " AND ".join(where) if where else ""
    sql = f\"\"\"
WITH raw AS (
  SELECT * FROM read_parquet('{{parquet_glob}}', filename=true)
),
proj AS (
  SELECT
    {ts_expr} AS ts_parsed,
    {bid_px_ex} AS bid_px_d,
    {ask_px_ex} AS ask_px_d,
    {bid_sz_ex} AS bid_sz_d,
    {ask_sz_ex} AS ask_sz_d
  FROM raw
),
src AS ( SELECT * FROM proj {{where_clause}} ),
ord AS ( SELECT * FROM src ORDER BY ts_parsed ),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px_d + ask_px_d)*0.5 AS mid,
    (ask_px_d - bid_px_d)     AS spread,
    CASE WHEN (bid_sz_d + ask_sz_d)>0
         THEN (bid_sz_d - ask_sz_d)/NULLIF(bid_sz_d + ask_sz_d,0)
         ELSE 0 END           AS imbalance,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,1) OVER ()) AS mom1,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px_d + ask_px_d)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
),
lab AS (
  SELECT
    *,
    LEAD(mid, {{horizon_rows}}) OVER () AS mid_fwd,
    (LEAD(mid, {{horizon_rows}}) OVER () - mid) / NULLIF(mid,0) AS fwd_ret
  FROM feat
),
final AS (
  SELECT
    ts,
    mid, spread, imbalance, mom1, mom3, vol3,
    LEAD(mid, {{horizon_rows}}) OVER ()        AS mid_p,
    LEAD(spread, {{horizon_rows}}) OVER ()     AS spread_p,
    LEAD(imbalance, {{horizon_rows}}) OVER ()  AS imbalance_p,
    LEAD(mom1, {{horizon_rows}}) OVER ()       AS mom1_p,
    LEAD(mom3, {{horizon_rows}}) OVER ()       AS mom3_p,
    LEAD(vol3, {{horizon_rows}}) OVER ()       AS vol3_p,
    fwd_ret,
    CASE WHEN fwd_ret > {{ret_threshold}} THEN 1
         WHEN fwd_ret < -{{ret_threshold}} THEN -1
         ELSE 0 END AS label
  FROM lab
)
SELECT * FROM final
WHERE mid IS NOT NULL AND mid_p IS NOT NULL
\"\"\".format(parquet_glob=parquet_glob,start_ts=start_ts,end_ts=end_ts,extra_filters=extra_filters or "",
             horizon_rows=horizon_rows,ret_threshold=ret_threshold,where_clause=where_clause)
    return sql

def _ensure_float(df: pd.DataFrame, cols: List[str])->pd.DataFrame:
    for c in cols:
        if c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    return df

def stream_into_buffer(con:duckdb.DuckDBPyConnection, sql:str, buffer:ReplayBuffer,
                       chunk_rows:int=200_000, drop_noise_label:bool=False, derive_action:bool=False)->int:
    total=0; cur=con.execute(sql)
    state_cols=["mid","spread","imbalance","mom1","mom3","vol3"]
    next_cols=["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
    while True:
        df=cur.fetch_df_chunk(chunk_rows)
        if df is None or len(df)==0: break
        df=_ensure_float(df, list(df.columns))
        if drop_noise_label and "label" in df.columns: df=df[df["label"]!=0]
        df=df.dropna(subset=state_cols+next_cols+["fwd_ret"])
        S=df[state_cols].to_numpy(np.float32)
        SP=df[next_cols].to_numpy(np.float32)
        R=df[["fwd_ret"]].to_numpy(np.float32)
        D=np.zeros((len(df),1),dtype=np.float32)
        if derive_action:
            A=df[["label"]].to_numpy(np.float32) if "label" in df.columns else np.sign(R).astype(np.float32)
        else:
            A=None
        for i in range(len(df)):
            buffer.add(S[i], (A[i] if A is not None else None), R[i], SP[i], D[i])
        total+=len(df)
    return total

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    args=ap.parse_args()
    con=duckdb.connect(database=":memory:")
    sql=build_sql(args.parquet,args.start,args.end,args.h,args.th,None)
    plan=con.execute(f"EXPLAIN {sql}").fetchall()
    print("[DuckDB plan]"); [print(" ",r[0]) for r in plan]
"""
    with open(args.out_datalayer, "w") as f:
        f.write(code)
    print(f"[ok] wrote data layer {os.path.abspath(args.out_datalayer)}")

    # quick EXPLAIN on the projection itself
    test_sql = f"""
WITH raw AS (SELECT * FROM read_parquet('{args.parquet}', filename=true)),
proj AS (
  SELECT
    {ts_expr} AS ts_parsed,
    {bid_px_ex} AS bid_px_d,
    {ask_px_ex} AS ask_px_d,
    {bid_sz_ex} AS bid_sz_d,
    {ask_sz_ex} AS ask_sz_d
  FROM raw
)
SELECT * FROM proj LIMIT 5
"""
    try:
        plan = con.execute(f"EXPLAIN {test_sql}").fetchall()
        print("[ok] projection EXPLAIN passed.")
    except Exception as e:
        print("[warn] projection EXPLAIN failed — check ob_mapping.json")
        print("      ", e)

if __name__ == "__main__":
    main()

=== ./backtest_gate_horizon.py ===

#!/usr/bin/env python3
# backtest_gate_horizon.py
# Aligns gate_out.csv with states.csv by row index and simulates PnL on H-step ahead returns.
# Adds holding constraint to cut churn and realistic fees.
#
# Example:
#   python3 backtest_gate_horizon.py --states states.csv --gate gate_out.csv \
#       --h 40 --mode sign --action_thresh 0.08 --hold_bars 8 --fee_bps 5.0
#
import argparse, csv, statistics as st

def read_states(path):
    mids=[]
    with open(path, newline="") as f:
        r=csv.DictReader(f)
        need=["mid","spread","imbalance","mom1","mom3","vol3"]
        hdr=[h.strip().lower() for h in (r.fieldnames or [])]
        if hdr!=need:
            raise SystemExit(f"states header mismatch: got {r.fieldnames}, want {need}")
        for row in r:
            try: mids.append(float(row["mid"]))
            except: pass
    if len(mids)<3: raise SystemExit("not enough rows in states.csv")
    return mids

def read_gate(path):
    acts=[]
    with open(path, newline="") as f:
        r=csv.DictReader(f)
        for row in r:
            try:
                agree=int(row["agree"])
                action=float(row["action"])
                acts.append(action if agree==1 else 0.0)
            except: pass
    if not acts: raise SystemExit("no rows in gate csv")
    return acts

def metrics(equity):
    rets=[equity[i]/equity[i-1]-1.0 for i in range(1,len(equity))]
    if not rets: return {}
    mu = st.mean(rets)
    sd = st.pstdev(rets) if len(rets)>1 else 0.0
    sharpe = (mu/sd) if sd>1e-12 else 0.0
    peak=equity[0]; maxdd=0.0
    for x in equity:
        if x>peak: peak=x
        dd = peak/x - 1.0
        if dd>maxdd: maxdd=dd
    return {"CAGR_like": equity[-1]/equity[0]-1.0, "Sharpe_like": sharpe, "MaxDD": maxdd, "Ret_mean": mu, "Ret_std": sd}

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40, help="forecast horizon (bars) to realize PnL")
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    ap.add_argument("--action_thresh", type=float, default=0.08, help="sign mode: trade if |a|>=th")
    ap.add_argument("--linear_scale", type=float, default=1.0, help="linear mode: pos = clip(a*scale, -1, 1)")
    ap.add_argument("--hold_bars", type=int, default=8, help="minimum holding period (bars) before switching/flattening")
    ap.add_argument("--fee_bps", type=float, default=5.0, help="per-change bps")
    args=ap.parse_args()

    mids=read_states(args.states)
    acts=read_gate(args.gate)

    # Need t and t+H → truncate
    n = min(len(acts), len(mids) - args.h)
    acts = acts[:n]; mids = mids[:n + args.h]

    # Compute H-step returns
    rets = [ (mids[t+args.h] - mids[t]) / mids[t] for t in range(n) ]

    # Build intended position (raw) from actions
    raw_pos=[0.0]*n
    if args.mode=="sign":
        th=args.action_thresh
        for t,a in enumerate(acts):
            raw_pos[t] = 1.0 if a>=th else (-1.0 if a<=-th else 0.0)
    else:
        for t,a in enumerate(acts):
            v = max(-1.0, min(1.0, a*args.linear_scale))
            raw_pos[t]=v

    # Enforce min holding
    pos=[0.0]*n
    if n>0:
        pos[0]=raw_pos[0]
        last_change=0
        for t in range(1,n):
            if raw_pos[t] != pos[t-1] and (t - last_change) < args.hold_bars:
                pos[t] = pos[t-1]   # defer change until holding satisfied
            else:
                if raw_pos[t] != pos[t-1]:
                    last_change = t
                pos[t] = raw_pos[t]

    # PnL with fees on position changes
    fee_rate = args.fee_bps / 10000.0
    equity=[1.0]
    trades=0; wins=0
    prev_pos=0.0
    for t in range(n):
        turnover = abs(pos[t] - prev_pos)
        fee = turnover * fee_rate
        r = pos[t]*rets[t] - fee
        equity.append(equity[-1]*(1.0 + r))
        if pos[t]!=0.0 and pos[t]!=prev_pos:
            trades += 1
        if pos[t]!=0.0 and r>0: wins += 1
        prev_pos = pos[t]

    m = metrics(equity)
    winrate = (wins/trades) if trades>0 else 0.0
    print("[backtest H]")
    print(f" rows={n}  trades={trades}  winrate={winrate:.2%}")
    print(f" H={args.h}  hold_bars={args.hold_bars}  mode={args.mode}  th={args.action_thresh if args.mode=='sign' else None}  scale={args.linear_scale if args.mode=='linear' else None}  fee_bps={args.fee_bps}")
    print(f" equity_end={equity[-1]:.4f}  Sharpe_like={m.get('Sharpe_like',0):.2f}  MaxDD={m.get('MaxDD',0):.2%}  CAGR_like={m.get('CAGR_like',0):.2%}")

=== ./tune_backtest_raw.py ===

#!/usr/bin/env python3
# tune_backtest_raw.py — robust grid search that prints FULL stdout from backtest for each combo.
import argparse, subprocess, sys, itertools

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        print("".join([
            "\n[CMD]\n", " ".join(cmd),
            "\n[STDERR]\n", p.stderr.strip(), "\n"
        ]), file=sys.stderr)
        sys.exit(p.returncode)
    return p.stdout.strip()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--states", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--h", type=int, default=40)
    ap.add_argument("--fee_bps", type=float, default=5.0)
    ap.add_argument("--mode", choices=["sign","linear"], default="sign")
    args = ap.parse_args()

    if args.mode == "sign":
        th_grid   = [0.03, 0.05, 0.08, 0.10, 0.15]
        hold_grid = [0, 4, 8, 12]
        grid = [(th, hold) for th in th_grid for hold in hold_grid]
        for th, hold in grid:
            cmd = [
                sys.executable, "backtest_gate_horizon.py",
                "--states", args.states,
                "--gate", args.gate,
                "--h", str(args.h),
                "--mode", "sign",
                "--action_thresh", str(th),
                "--hold_bars", str(hold),
                "--fee_bps", str(args.fee_bps),
            ]
            out = run(cmd)
            print(f"\n>>> th={th:.3f} hold={hold}")
            print(out)
    else:
        scale_grid = [0.5, 0.75, 1.0, 1.5, 2.0]
        hold_grid  = [0, 4, 8, 12]
        for sc, hold in [(s,h) for s in scale_grid for h in hold_grid]:
            cmd = [
                sys.executable, "backtest_gate_horizon.py",
                "--states", args.states,
                "--gate", args.gate,
                "--h", str(args.h),
                "--mode", "linear",
                "--linear_scale", str(sc),
                "--hold_bars", str(hold),
                "--fee_bps", str(args.fee_bps),
            ]
            out = run(cmd)
            print(f"\n>>> scale={sc:.2f} hold={hold}")
            print(out)

if __name__ == "__main__":
    main()

=== ./summarize_gate_out.py ===

#!/usr/bin/env python3
# summarize_gate_out.py — prints quick stats for gate_out.csv
# CSV format expected: row_idx,agree,action,min_q
import argparse, csv, math, statistics as st

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True)
    ap.add_argument("--bins", type=int, default=11, help="histogram bins")
    args = ap.parse_args()

    rows = []
    with open(args.csv, newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            try:
                rows.append({
                    "agree": int(row["agree"]),
                    "action": float(row["action"]),
                    "min_q": None if row.get("min_q","") in ("", "None") else float(row["min_q"]),
                })
            except Exception:
                pass

    n = len(rows)
    if n == 0:
        print("[error] no rows")
        return
    agree_cnt = sum(1 for x in rows if x["agree"] == 1)
    nonzero_cnt = sum(1 for x in rows if abs(x["action"]) > 1e-8)
    actions = [x["action"] for x in rows]
    act_min, act_max = min(actions), max(actions)
    act_mean = st.mean(actions)
    act_median = st.median(actions)

    print(f"[stats] rows={n}")
    print(f"[stats] agree_rate = {agree_cnt}/{n} = {agree_cnt/n:.3f}")
    print(f"[stats] nonzero_action_rate = {nonzero_cnt}/{n} = {nonzero_cnt/n:.3f}")
    print(f"[stats] action range = [{act_min:.4f}, {act_max:.4f}]")
    print(f"[stats] action mean/median = {act_mean:.4f} / {act_median:.4f}")

    # simple histogram
    lo, hi = -1.0, 1.0
    step = (hi - lo) / max(1, args.bins - 1)
    edges = [lo + i*step for i in range(args.bins)]
    counts = [0]* (len(edges)-1)
    for a in actions:
        k = min(len(edges)-2, max(0, int((a - lo) / step)))
        counts[k] += 1
    print("[hist] bins (approx):")
    for i in range(len(counts)):
        l, r = edges[i], edges[i+1]
        print(f"  [{l:+.2f},{r:+.2f}) : {counts[i]}")

=== ./train_td3bc_from_duckdb.py ===

#!/usr/bin/env python3
# train_td3bc_from_duckdb.py
# Offline TD3+BC training directly from Parquet via DuckDB feeder.

import argparse
import time
import numpy as np
import torch

from rl_duckdb_data_layer import build_sql, stream_into_buffer, ReplayBuffer
import duckdb
from td3_bc_agent import TD3BC

def to_torch(batch, device="cpu"):
    out = {}
    for k, v in batch.items():
        out[k] = torch.from_numpy(v).to(device)
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    ap.add_argument("--buf", type=int, default=1_500_000)
    ap.add_argument("--chunk", type=int, default=200_000)
    ap.add_argument("--drop_noise", action="store_true")
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--steps_per_epoch", type=int, default=5000)
    ap.add_argument("--batch", type=int, default=512)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--device", default="cpu")
    ap.add_argument("--bc_lambda", type=float, default=0.1, help="BC regularization weight")
    args = ap.parse_args()

    rng = np.random.default_rng(args.seed)
    torch.manual_seed(args.seed)

    con = duckdb.connect(database=":memory:")
    sql = build_sql(args.parquet, args.start, args.end, args.h, args.th, None)

    # derive_action=True so TD3+BC has actions from labels {-1,0,1}
    buf = ReplayBuffer(capacity=args.buf, state_dim=6, act_dim=1)

    total = stream_into_buffer(
        con, sql, buf,
        chunk_rows=args.chunk,
        drop_noise_label=args.drop_noise,
        derive_action=True
    )
    print(f"[ingest] transitions loaded: {total}, buffer.size={buf.size()}")

    assert buf.size() > args.batch, "Not enough samples to train."

    agent = TD3BC(
        s_dim=6, a_dim=1, act_limit=1.0,
        actor_lr=1e-3, critic_lr=1e-3,
        gamma=0.99, tau=0.005, policy_delay=2,
        bc_lambda=args.bc_lambda,
        target_noise=0.2, noise_clip=0.5,
        device=args.device
    )

    for ep in range(1, args.epochs + 1):
        t0 = time.time()
        for t in range(1, args.steps_per_epoch + 1):
            batch_np = buf.sample(args.batch, rng)
            # require actions
            assert "a" in batch_np, "Actions missing. Use derive_action=True in feeder."
            batch = {
                "s":  batch_np["s"],
                "a":  batch_np["a"],
                "r":  batch_np["r"],
                "sp": batch_np["sp"],
                "d":  batch_np["d"],
            }
            for k in batch:
                batch[k] = torch.from_numpy(batch[k]).to(args.device)

            stats = agent.update(batch)

            if t % 200 == 0 or t == args.steps_per_epoch:
                print(f"[ep {ep}/{args.epochs}] step {t}/{args.steps_per_epoch} "
                      f"| q1={stats['q1_loss']:.5f} q2={stats['q2_loss']:.5f} pi={stats['pi_loss']:.5f}")

        dur = time.time() - t0
        print(f"[ep {ep}] done in {dur:.1f}s")

    # Save final actor (deterministic policy)
    torch.save(agent.actor.state_dict(), "td3bc_actor.pt")
    print("Saved: td3bc_actor.pt")

if __name__ == "__main__":
    main()

=== ./view_sweep.py ===

#!/usr/bin/env python3
import argparse, csv, glob, os, sys
from typing import List

ANSI = {"reset":"\033[0m","bd":"\033[1m","r":"\033[31m","g":"\033[32m","y":"\033[33m","b":"\033[34m","m":"\033[35m","c":"\033[36m","w":"\033[37m"}
def use_color(enabled: bool) -> bool: return enabled and sys.stdout.isatty()
def C(txt, code, enabled=True): return f"{ANSI.get(code,'')}{txt}{ANSI['reset']}" if enabled else str(txt)

COLS = ["mode","H","fee_bps","min_agree_k","agree_eps","min_mag","action_thresh","linear_scale","hold_bars",
        "rows","trades","trade_winrate","per_bar_hitrate","equity_end","Sharpe_like","MaxDD","CAGR_like",
        "exposure","avg_turnover","avg_trade_pnl","median_trade_pnl","saturation_neg1","saturation_pos1",
        "saturation_mid","gate_csv"]

def count_data_rows(path:str)->int:
    try:
        with open(path, 'r', newline='') as f:
            n=0
            for i,_ in enumerate(f): n=i+1
        return max(0, n-1)  # minus header
    except: return 0

def resolve_candidates(sweep_arg: str):
    """Return list of candidate results.csv paths, newest first."""
    cands=[]
    def add(p):
        if os.path.isfile(p): cands.append((os.path.getmtime(p), p))
    if sweep_arg.endswith(".csv") and os.path.isfile(sweep_arg):
        add(sweep_arg)
    elif os.path.isdir(sweep_arg):
        p=os.path.join(sweep_arg,"results.csv"); 
        if os.path.isfile(p): add(p)
    else:
        for m in glob.glob(sweep_arg):
            if os.path.isdir(m):
                p=os.path.join(m,"results.csv"); 
                if os.path.isfile(p): add(p)
            elif m.endswith(".csv") and os.path.isfile(m):
                add(m)
    cands.sort(reverse=True)
    return [p for _,p in cands]

def resolve_results_csv(sweep_arg: str, prefer_non_empty=True):
    cands = resolve_candidates(sweep_arg)
    if not cands:
        raise FileNotFoundError(f"No results.csv found under: {sweep_arg}")
    if not prefer_non_empty:
        return cands[0]
    for p in cands:
        if count_data_rows(p) > 0:
            return p
    # none had data; return newest anyway
    return cands[0]

def read_results(results_csv: str) -> List[dict]:
    with open(results_csv, newline="") as f:
        rdr = csv.DictReader(f); rows=[]
        for r in rdr:
            for k in COLS:
                if k not in r: r[k]=""
            rows.append(r)
        return rows

def ffloat(x, dflt=None):
    try: return float(x)
    except: return dflt

def highlight_kpis(r: dict):
    tw=ffloat(r.get("trade_winrate")); ph=ffloat(r.get("per_bar_hitrate"))
    eq=ffloat(r.get("equity_end")); sh=ffloat(r.get("Sharpe_like")); dd=ffloat(r.get("MaxDD"))
    tw_c="r";   tw_c="y" if tw is not None and tw>=50 else tw_c;   tw_c="g" if tw is not None and tw>=60 else tw_c
    ph_c="r";   ph_c="y" if ph is not None and ph>=50 else ph_c;   ph_c="g" if ph is not None and ph>=55 else ph_c
    eq_c="r";   eq_c="y" if eq is not None and eq>=1.00 else eq_c; eq_c="g" if eq is not None and eq>=1.01 else eq_c
    sh_c="r";   sh_c="y" if sh is not None and sh>=0.1 else sh_c;  sh_c="g" if sh is not None and sh>=0.5 else sh_c
    dd_c="r";   dd_c="y" if dd is not None and dd<=1.0 else dd_c;  dd_c="g" if dd is not None and dd<=0.5 else dd_c
    return {"tw":tw_c,"ph":ph_c,"eq":eq_c,"sh":sh_c,"dd":dd_c}

def fmt_pct(x, p=2):
    v=ffloat(x); return f"{v:.{p}f}%" if v is not None else "-"

def fmt_num(x, p=4):
    v=ffloat(x); return f"{v:.{p}f}" if v is not None else "-"

def print_table(rows: List[dict], top: int, color_on: bool):
    print(C("=== Top Results ===","bd",color_on))
    hdr=["Tag","Trades","Winrate","HitRate","EquityEnd","Sharpe","MaxDD","H","k","eps","mag","th","hold"]
    print(" | ".join(hdr)); print("-"* (len(" | ".join(hdr))))
    for r in rows[:top]:
        tag=f"k{r['min_agree_k']}_eps{r['agree_eps']}_mag{r['min_mag']}_th{r['action_thresh']}_hold{r['hold_bars']}"
        K=highlight_kpis(r)
        line=" | ".join([
            tag, f"{r['trades']}",
            C(fmt_pct(r['trade_winrate']),K["tw"],color_on),
            C(fmt_pct(r['per_bar_hitrate']),K["ph"],color_on),
            C(fmt_num(r['equity_end'],4),K["eq"],color_on),
            C(fmt_num(r['Sharpe_like'],2),K["sh"],color_on),
            C(fmt_num(r['MaxDD'],2)+"%",K["dd"],color_on),
            r["H"], r["min_agree_k"], r["agree_eps"], r["min_mag"], r["action_thresh"], r["hold_bars"]
        ])
        print(line)
    print()

def print_gate_detail(results_csv: str, gate_name: str, color_on: bool):
    rows=read_results(results_csv); m=None
    for r in rows:
        if r.get("gate_csv")==gate_name: m=r; break
    if not m:
        print(C(f"[warn] gate not found: {gate_name}","y",color_on)); return
    K=highlight_kpis(m)
    print(C("=== Backtest KPI (selected gate) ===","bd",color_on))
    print(f"Gate           : {m['gate_csv']}")
    print(f"Mode/H/Fee     : {m['mode']}/H={m['H']}/fee={m['fee_bps']} bps")
    print(f"Consensus      : k={m['min_agree_k']} eps={m['agree_eps']} mag={m['min_mag']}")
    print(f"Exec           : th={m['action_thresh']} hold={m['hold_bars']}")
    print(f"Rows/Trades    : {m['rows']}/{m['trades']}")
    print("KPI            : "
          + f"Win {C(fmt_pct(m['trade_winrate']),K['tw'],color_on)} | "
          + f"Hit {C(fmt_pct(m['per_bar_hitrate']),K['ph'],color_on)} | "
          + f"Eq {C(fmt_num(m['equity_end']),K['eq'],color_on)} | "
          + f"Sharpe {C(fmt_num(m['Sharpe_like'],2),K['sh'],color_on)} | "
          + f"MaxDD {C(fmt_num(m['MaxDD'],2)+'%',K['dd'],color_on)}")
    print()

def print_saturation(results_csv: str, color_on: bool):
    rows=read_results(results_csv)
    if not rows:
        print(C("[empty results]","y",color_on)); return
    r=rows[-1]
    print(C("=== Latest Gate Saturation Snapshot ===","bd",color_on))
    print(f"Gate: {r.get('gate_csv','-')}")
    print(f"neg≈-1: {r.get('saturation_neg1','-')} | pos≈+1: {r.get('saturation_pos1','-')} | mid: {r.get('saturation_mid','-')}")
    print()

def main():
    ap=argparse.ArgumentParser(description="View sweep results (colored KPIs) with empty-safe fallback.")
    ap.add_argument("--sweep", required=True, help="Sweep dir, results.csv, or glob (e.g. 'runs/sweep_sign_*')")
    ap.add_argument("--top", type=int, default=12)
    ap.add_argument("--show-sat", action="store_true")
    ap.add_argument("--gate", help="Show KPI for a specific gate CSV")
    ap.add_argument("--force-color", action="store_true")
    ap.add_argument("--no-color", action="store_true")
    ap.add_argument("--allow-empty", action="store_true", help="Allow showing an empty results.csv")
    args=ap.parse_args()

    try:
        results_csv = resolve_results_csv(args.sweep, prefer_non_empty=not args.allow_empty)
    except FileNotFoundError as e:
        print(f"[error] {e}"); sys.exit(1)

    nrows = count_data_rows(results_csv)
    color_on = use_color(not args.no_color or args.force_color)

    print(C(f"[sweep] {os.path.dirname(results_csv) or '.'}  (rows={nrows})","bd",color_on))
    rows = read_results(results_csv)
    if nrows <= 0:
        print(C("[info] results.csv has no data rows yet.","y",color_on))
        print(C("      Tip: run another sweep, or pass a glob (e.g. --sweep 'runs/sweep_sign_*') and I’ll pick the latest non-empty.","y",color_on))
        sys.exit(0)

    # sort by equity_end desc, then Sharpe desc
    rows_sorted = sorted(rows, key=lambda r: (ffloat(r.get("equity_end"),1.0), ffloat(r.get("Sharpe_like"),0.0)), reverse=True)

    print_table(rows_sorted, args.top, color_on)
    if args.gate: print_gate_detail(results_csv, args.gate, color_on)
    if args.show_sat: print_saturation(results_csv, color_on)

if __name__=="__main__": main()

=== ./sweep_now_writer.py ===

#!/usr/bin/env python3
import argparse, time, re
from pathlib import Path

def latest_log(sweep: Path):
    logs = sorted(sweep.glob("log_*.txt"), key=lambda p: p.stat().st_mtime, reverse=True)
    return logs[0] if logs else None

def parse_line(ln: str):
    # Accept a few formats we printed during sweep/backtests
    # Example lines (be tolerant):
    # [bt] backtest -> gate_k2_eps0.05_mag0.1_th0.05_hold8.csv
    # [backtest H] rows=160 trades=8 ... equity_end=1.0124 Sharpe_like=0.15 MaxDD=1.03% ...
    # params might appear in file name as k2_eps0.05_mag0.1_th0.05_hold8
    info = {}
    gate_m = re.search(r'gate_([^.\s]+)\.csv', ln)
    if gate_m:
        info["gate_tag"] = gate_m.group(1)
        # extract fields in tag if present
        m = re.match(r'k(?P<k>\d+)_eps(?P<eps>[0-9.]+)_mag(?P<mag>[0-9.]+)_(?:th(?P<th>[0-9.]+)|ls(?P<ls>[0-9.]+))?_hold(?P<hold>\d+)', info["gate_tag"])
        if m:
            d = m.groupdict()
            if d.get("k"):   info["min_agree_k"] = d["k"]
            if d.get("eps"): info["agree_eps"]   = d["eps"]
            if d.get("mag"): info["min_mag"]     = d["mag"]
            if d.get("th"):  info["action_thresh"] = d["th"]
            if d.get("ls"):  info["linear_scale"]  = d["ls"]
            if d.get("hold"):info["hold_bars"]   = d["hold"]
    # backtest summary values (be permissive)
    kvs = re.findall(r'(\w+)=([-\w\.%]+)', ln)
    for k,v in kvs:
        info[k] = v
    return info

def write_now_csv(path: Path, info: dict):
    # Define a stable header
    header = ["mode","H","fee_bps","min_agree_k","agree_eps","min_mag","action_thresh","linear_scale","hold_bars","rows","trades","equity_end","Sharpe_like","MaxDD","exposure","gate_tag"]
    # normalize values if we saw them
    row = [
        info.get("mode",""),
        info.get("H",""),
        info.get("fee_bps",""),
        info.get("min_agree_k",""),
        info.get("agree_eps",""),
        info.get("min_mag",""),
        info.get("action_thresh",""),
        info.get("linear_scale",""),
        info.get("hold_bars",""),
        info.get("rows",""),
        info.get("trades",""),
        info.get("equity_end",""),
        info.get("Sharpe_like",""),
        info.get("MaxDD",""),
        info.get("exposure",""),
        info.get("gate_tag",""),
    ]
    txt = ",".join(header) + "\n" + ",".join(map(str,row)) + "\n"
    path.write_text(txt)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--sweep", required=True, help="runs/sweep_* folder")
    ap.add_argument("--every", type=float, default=1.0)
    args = ap.parse_args()

    sweep = Path(args.sweep).resolve()
    out = sweep / "live.now.csv"

    last_written = ""
    while True:
        lg = latest_log(sweep)
        if lg and lg.exists():
            lines = lg.read_text(errors="ignore").splitlines()
            # scan from the end for a line with richest info
            info = {}
            for ln in reversed(lines[-200:]):  # look at last 200 lines
                data = parse_line(ln)
                if data:
                    info.update(data)
                    # stop if we already captured a gate & equity/Sharpe
                    if "gate_tag" in info and ("equity_end" in info or "Sharpe_like" in info):
                        break
            if info:
                snapshot = repr(sorted(info.items()))
                if snapshot != last_written:
                    write_now_csv(out, info)
                    last_written = snapshot
        time.sleep(args.every)

if __name__ == "__main__":
    main()

=== ./live_top.py ===

#!/usr/bin/env python3
import argparse, csv, os, time
from math import isnan

def score(row):
    try:
        equity_end  = float(row["equity_end"] or 1.0)
        sharpe      = float(row["Sharpe_like"] or 0.0)
        maxdd       = float(row["MaxDD"] or 0.0)
        exposure    = float(row["exposure"] or 0.0)
        trades      = int(float(row["trades"] or 0))
    except Exception:
        return -1e9
    if trades == 0: return -1e9
    s  = (equity_end - 1.0) * 100.0
    s += 0.5 * sharpe
    s -= 0.1 * maxdd
    if exposure > 95.0: s -= 1.0
    return s

def read_rows(path):
    if not os.path.exists(path): return []
    with open(path, newline="") as f:
        rdr = csv.DictReader(f)
        return list(rdr)

def fmt(r):
    mode = r["mode"]; gate = r["gate_csv"]
    return (f"{mode:5}   {float(r['equity_end']):.4f}   {float(r['Sharpe_like']):6.2f}   "
            f"{float(r['MaxDD']):6.2f}%   {int(float(r['trades'])):4d}   "
            f"k={r['min_agree_k']} eps={r['agree_eps']} mag={r['min_mag']}  "
            f"{('th='+r['action_thresh']) if r['action_thresh'] else ('ls='+r['linear_scale'])}  "
            f"hold={r['hold_bars']}   {gate}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--results", required=True, help="path to sweep .../results.csv")
    ap.add_argument("--n", type=int, default=10, help="top K")
    ap.add_argument("--every", type=float, default=1.0, help="refresh seconds")
    args = ap.parse_args()

    while True:
        rows = read_rows(args.results)
        ranked = sorted([(score(r), r) for r in rows], key=lambda x: x[0], reverse=True)
        os.system("clear")
        print(f"=== LIVE TOP {args.n} ===  ({len(rows)} rows)  {args.results}\n")
        print("mode   equity_end   Sharpe   MaxDD    trds   params...   gate")
        print("-"*120)
        for i, (sc, r) in enumerate(ranked[:args.n]):
            print(f"#{i+1:02}  {fmt(r)}   | score={sc:.3f}")
        print("\n[press Ctrl+C to exit]")
        time.sleep(args.every)

if __name__ == "__main__":
    main()

=== ./live_executor_hold_v2.py ===

#!/usr/bin/env python3
# live_executor_hold_v2.py (v2.1)
# - Fixes UTC deprecation (uses timezone-aware UTC timestamps)
# - Adds --ts_timespec (seconds|milliseconds) for timestamp precision
# - Consistent float formatting in CSV
# - Same trading logic/flags as v2

import argparse, json, os, subprocess, sys
import datetime as dt

def run(cmd):
    p = subprocess.run(cmd, capture_output=True, text=True)
    if p.returncode != 0:
        raise RuntimeError(p.stderr.strip())
    return p.stdout.strip()

def parse_state_row(row_csv):
    # "mid,spread,imbalance,mom1,mom3,vol3" as a single CSV row (no header)
    vals = [float(x.strip()) for x in row_csv.split(",")]
    if len(vals) != 6:
        raise ValueError("state_row must have 6 numeric fields")
    return dict(mid=vals[0], spread=vals[1], imbalance=vals[2], mom1=vals[3], mom3=vals[4], vol3=vals[5])

def now_utc_iso(timespec: str = "seconds") -> str:
    # timespec: "seconds" or "milliseconds"
    t = dt.datetime.now(dt.timezone.utc)
    if timespec == "milliseconds":
        # emulate ms: ISO without microseconds, then add .mmmZ
        return t.isoformat(timespec="milliseconds")
    return t.isoformat(timespec="seconds")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    ap.add_argument("--device", default="cpu")

    # thresholds & holding
    ap.add_argument("--buy_th", type=float, default=0.10, help="min +action to go/stay long")
    ap.add_argument("--sell_th", type=float, default=0.10, help="min -action (abs) to go/stay short")
    ap.add_argument("--hold_bars", type=int, default=12, help="minimum bars to hold before flipping")
    ap.add_argument("--max_hold_bars", type=int, default=160, help="hard cap on holding duration; force flat after this many bars")

    # microstructure guards
    ap.add_argument("--spread_cap", type=float, default=5.0, help="skip new entries if spread > cap (price units)")
    ap.add_argument("--min_vol3", type=float, default=0.0, help="skip new entries if vol3 < min_vol3")

    # persistence / logging
    ap.add_argument("--statefile", default="live_state.json")
    ap.add_argument("--log", default="live_signals.csv")

    # formatting
    ap.add_argument("--ts_timespec", choices=["seconds","milliseconds"], default="seconds")
    ap.add_argument("--fmt_action", default=".6f")
    ap.add_argument("--fmt_spread", default=".6f")
    ap.add_argument("--fmt_vol3", default=".6f")

    args = ap.parse_args()

    # 1) Pull latest features (6-tuple)
    row = run([sys.executable, "latest_state_from_parquet.py", "--parquet", args.parquet])
    feat = parse_state_row(row)

    # 2) Run sign consensus gate ON THIS ROW (min-agree-k=2, min-mag = min(buy_th, sell_th))
    min_mag = min(args.buy_th, args.sell_th)
    out = run([
        sys.executable, "consensus_gate_live_plus.py",
        "--gate", args.gate,
        "--scaler", args.scaler,
        "--state_row", row,
        "--device", args.device,
        "--consensus-mode", "sign",
        "--min-agree-k", "2",
        "--min-mag", f"{min_mag}"
    ])

    # parse last CSV line "row_idx,agree,action,max_pair_diff,min_q"
    last = [ln for ln in out.splitlines() if ln and ln[0].isdigit()][-1]
    parts = last.split(",")
    agree = int(parts[1]); raw_action = float(parts[2])

    # 3) Discrete intent with asymmetric thresholds
    intent = 0
    if raw_action >= args.buy_th: intent = +1
    elif raw_action <= -args.sell_th: intent = -1

    # 4) Load persistent state
    st = {"prev_pos": 0, "bars_held": 0, "bars_held_total": 0}
    if os.path.isfile(args.statefile):
        try:
            with open(args.statefile, "r") as f: st.update(json.load(f))
        except: pass
    prev_pos = int(st.get("prev_pos", 0))
    held = int(st.get("bars_held", 0))
    held_total = int(st.get("bars_held_total", 0))

    # 5) Microstructure guards (apply to NEW ENTRIES only; allow exits always)
    skipped_reason = ""
    if prev_pos == 0 and intent != 0:
        if feat["spread"] > args.spread_cap:
            skipped_reason = f"spread>{args.spread_cap}"
            intent = 0
        elif feat["vol3"] < args.min_vol3:
            skipped_reason = f"vol3<{args.min_vol3}"
            intent = 0

    # 6) Enforce min-hold + max-hold
    next_pos = intent
    if next_pos != prev_pos:
        # candidate change
        if prev_pos != 0 and next_pos != 0 and held < args.hold_bars:
            # flip too soon -> defer
            next_pos = prev_pos
            held += 1; held_total += 1
        else:
            # allowed change
            if next_pos == 0:
                held = 0; held_total = 0
            else:
                held = 0; held_total = 0
    else:
        # no change, increment timers if in position
        if next_pos != 0:
            held = min(held + 1, args.hold_bars)
            held_total += 1
            if held_total >= args.max_hold_bars:
                next_pos = 0
                skipped_reason = (skipped_reason + "; " if skipped_reason else "") + "max_hold_cap"
                held = 0; held_total = 0

    # 7) Persist
    with open(args.statefile, "w") as f:
        json.dump({"prev_pos": int(next_pos), "bars_held": int(held), "bars_held_total": int(held_total)}, f)

    # 8) Log/print (UTC, timezone-aware)
    ts = now_utc_iso(args.ts_timespec)
    # format floats
    fmt_a = f"{raw_action:{args.fmt_action}}"
    fmt_s = f"{feat['spread']:{args.fmt_spread}}"
    fmt_v = f"{feat['vol3']:{args.fmt_vol3}}"

    header = "ts,prev_pos,next_pos,raw_action,buy_th,sell_th,hold_bars,hold_counter,spread,vol3,skipped_reason\n"
    line = f"{ts},{prev_pos},{next_pos},{fmt_a},{args.buy_th:.2f},{args.sell_th:.2f},{args.hold_bars},{held},{fmt_s},{fmt_v},{skipped_reason}"
    print(line)
    if not os.path.isfile(args.log):
        with open(args.log, "w") as f: f.write(header)
    with open(args.log, "a") as f: f.write(line + "\n")

if __name__ == "__main__":
    main()

=== ./interpret_gate_output.py ===

#!/usr/bin/env python3
# interpret_gate_output.py — turns gate_out.csv into an interpretable summary
# Usage: python3 interpret_gate_output.py --csv gate_out.csv --pos_bins 0.05 0.15 0.30
import argparse, csv, statistics as st, math

def bucket_action(a, bins):
    ab = abs(a)
    for i, b in enumerate(bins):
        if ab < b:
            return i
    return len(bins)

def label_for(i, bins):
    if i==0: return "Tiny (|a| < %.2f)" % bins[0]
    if i==1: return "Small [%.2f, %.2f)" % (bins[0], bins[1])
    if i==2: return "Medium [%.2f, %.2f)" % (bins[1], bins[2])
    return "Large (>= %.2f)" % bins[-1]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True)
    ap.add_argument("--pos_bins", nargs="+", type=float, default=[0.05,0.15,0.30],
                    help="abs(action) bucket edges")
    args = ap.parse_args()

    rows=[]
    with open(args.csv, newline="") as f:
        r=csv.DictReader(f)
        for row in r:
            try:
                agree=int(row["agree"])
                action=float(row["action"])
                maxdiff=float(row.get("max_pair_diff") or row.get("details","{}").split("max_pair_diff': ")[-1].split(",")[0]) if "max_pair_diff" in row or "details" in row else float("nan")
                rows.append((agree, action, maxdiff))
            except:
                pass

    if not rows:
        print("[error] no rows"); return
    n=len(rows)
    agree_n=sum(1 for a,_,__ in rows if a==1)
    print(f"[rows] {n}   [agree=1] {agree_n}  ({agree_n/n:.2%})")

    acts=[a for g,a,_ in rows if g==1]
    if not acts:
        print("[info] no agreed trades → loosen gate or try sign-consensus")
        return

    print("\n=== Action stats (agreed trades only) ===")
    print(f"mean={st.mean(acts):+.4f}  median={st.median(acts):+.4f}  min={min(acts):+.4f}  max={max(acts):+.4f}")

    # Buckets
    bins = sorted(args.pos_bins)
    buckets = {}
    for g,a,_ in rows:
        if g!=1: continue
        k = bucket_action(a, bins)
        buckets.setdefault(k, []).append(a)

    total_agreed=len(acts)
    for k in sorted(buckets):
        arr=buckets[k]
        print(f"- {label_for(k,bins)}: {len(arr)} ({len(arr)/total_agreed:.1%}) | buy={sum(1 for x in arr if x>0)} sell={sum(1 for x in arr if x<0)}")

    # sample most confident buys/sells
    buys=sorted([x for x in acts if x>0], reverse=True)[:5]
    sells=sorted([x for x in acts if x<0])[:5]
    print("\nTop buys:", [round(x,4) for x in buys])
    print("Top sells:", [round(x,4) for x in sells])

    # disagreement snapshot
    diffs=[d for g,_,d in rows if g==1 and not math.isnan(d)]
    if diffs:
        print(f"\nmax_pair_diff: mean={st.mean(diffs):.3f}, 90p={sorted(diffs)[int(0.9*len(diffs))]:.3f}, max={max(diffs):.3f}")
    print("\nLegend:")
    print("  action: signed size in [-1,1] (magnitude ~ conviction).")
    print("  Tiny/Small/Medium/Large buckets help you see if signals are mostly noise or have teeth.")
    print("  If you mostly see Tiny, raise the min magnitude you act on (e.g., trade only if |a|>=0.05).")

if __name__ == "__main__":
    main()

=== ./human_status.py ===

#!/usr/bin/env python3
import argparse, csv, time, os, sys

ANSI = {
    "reset":"\033[0m","r":"\033[31m","g":"\033[32m","y":"\033[33m",
    "b":"\033[34m","m":"\033[35m","c":"\033[36m","bd":"\033[1m"
}
def C(txt, c, use=True): 
    s = ANSI.get(c,"") + str(txt) + ANSI["reset"]
    return s if use else str(txt)

def pos_name(x):
    try: x = int(float(x))
    except: return f"{x}"
    return {-1:"SHORT (-1)", 0:"FLAT (0)", 1:"LONG (+1)"}.get(x, f"{x:+d}")

def conf_level(a):
    if a is None: return "n/a"
    aa = abs(a)
    if aa >= 0.75: return "very strong"
    if aa >= 0.50: return "strong"
    if aa >= 0.25: return "medium"
    if aa >= 0.10: return "weak"
    return "very weak"

def safe_float(x, default=None):
    try:
        return float(x)
    except:
        return default

def read_last_row(path):
    """
    Robust reader:
    - Finds the first header (row[0] == 'ts')
    - Ignores any repeated headers later in the file
    - Returns dict of last data row keyed by header names
    """
    if not os.path.exists(path):
        return None, "not_found"
    with open(path, newline="") as f:
        rows = list(csv.reader(f))
    if not rows:
        return None, "empty"

    # find a header row: must start with 'ts'
    header_idx = None
    for i, r in enumerate(rows):
        if r and r[0].strip().lower() == "ts":
            header_idx = i
            break
    if header_idx is None:
        return None, "no_header"

    hdr = rows[header_idx]
    # gather only data rows after the header, skipping any repeated header lines
    data = [r for r in rows[header_idx+1:] if r and r[0].strip().lower() != "ts"]
    if not data:
        return None, "no_data"

    last = data[-1]
    # allow for trailing commas / short lines by padding
    if len(last) < len(hdr):
        last = last + [""] * (len(hdr) - len(last))

    idx = {k: i for i, k in enumerate(hdr)}

    def get(k, default=None):
        i = idx.get(k)
        if i is None or i >= len(last):
            return default
        val = last[i]
        return val if val != "" else default

    # read fields (support either buy_th/sell_th or a single th)
    ts = get("ts", "?")
    prev_pos = safe_float(get("prev_pos"), 0)
    next_pos = safe_float(get("next_pos"), 0)
    raw_action = safe_float(get("raw_action"), None)

    th = safe_float(get("th"), None)
    buy_th = safe_float(get("buy_th"), th if th is not None else 0.10)
    sell_th = safe_float(get("sell_th"), th if th is not None else 0.10)

    hold_bars = int(safe_float(get("hold_bars"), 0) or 0)
    hold_counter = int(safe_float(get("hold_counter"), 0) or 0)

    spread = safe_float(get("spread"), None)
    vol3   = safe_float(get("vol3"), None)

    return {
        "ts": ts,
        "prev_pos": prev_pos,
        "next_pos": next_pos,
        "raw_action": raw_action,
        "buy_th": buy_th,
        "sell_th": sell_th,
        "hold_bars": hold_bars,
        "hold_counter": hold_counter,
        "spread": spread,
        "vol3": vol3,
    }, None

def decide(action, bth, sth):
    if action is None:
        return "NO SIGNAL", "y"
    if action >= bth:
        return "BUY (LONG)", "g"
    if action <= -sth:
        return "SELL (SHORT)", "r"
    return "HOLD / NO TRADE", "y"

def print_once(path, color=True):
    d, err = read_last_row(path)
    if err == "not_found":
        print(C(f"[error] {path} not found", "r", color)); return
    if err in ("empty", "no_data"):
        print(C("[info] waiting for first signal...", "y", color)); return
    if err == "no_header":
        print(C("[error] CSV missing header row starting with 'ts'", "r", color)); return

    act_txt, col = decide(d["raw_action"], d["buy_th"], d["sell_th"])

    print(C("=== LATEST SIGNAL ===", "bd", color))
    print(f"time: {d['ts']}")
    print(C(
        f"decision: {act_txt}   (action={d['raw_action']:+.3f}  buy_th={d['buy_th']:.2f}  sell_th={d['sell_th']:.2f})",
        col, color
    ))
    print(f"position: {pos_name(d['prev_pos'])}  →  {pos_name(d['next_pos'])}")

    if d["hold_bars"] > 0:
        pct = 100 * min(max(d["hold_counter"]/max(d["hold_bars"],1), 0), 1)
        print(f"hold: {d['hold_counter']}/{d['hold_bars']} bars ({pct:.0f}% done)")
    else:
        print("hold: none")

    print(f"confidence: {conf_level(d['raw_action'])}")
    extras = []
    if d["spread"] is not None: extras.append(f"spread={d['spread']}")
    if d["vol3"]   is not None: extras.append(f"vol3={d['vol3']}")
    if extras:
        print("market:", ", ".join(extras))

def watch(path, sec, color=True):
    try:
        last_size = -1
        while True:
            try:
                s = os.path.getsize(path)
                if s != last_size:
                    os.system("clear")
                    print_once(path, color)
                    last_size = s
            except FileNotFoundError:
                os.system("clear")
                print(C(f"[error] {path} not found", "r", color))
            time.sleep(sec)
    except KeyboardInterrupt:
        pass

def main():
    ap = argparse.ArgumentParser(description="Human-readable view of live_signals.csv")
    ap.add_argument("csv", help="Path to live_signals.csv")
    ap.add_argument("--watch", type=int, default=0, help="Refresh seconds (0 = print once)")
    ap.add_argument("--no-color", action="store_true", help="Disable ANSI colors")
    a = ap.parse_args()
    if a.watch > 0:
        watch(a.csv, a.watch, color=not a.no_color)
    else:
        print_once(a.csv, color=not a.no_color)

if __name__ == "__main__":
    main()

=== ./consensus_gate_live_plus.py ===

#!/usr/bin/env python3
# consensus_gate_live_plus.py
# Live gate with scaler.json; supports:
#  - value-consensus (pairwise diff within eps)
#  - sign-consensus (same sign, optional min magnitude)
#  - k-of-n agreement (min_agree_k)
#  - CLI overrides for agree_eps, act_limit
#  - float32-safe for models and inputs
import argparse, json, csv, os, sys, re
from typing import List, Tuple
import numpy as np

try:
    import torch
    import torch.nn as nn
except ImportError:
    print("[error] Requires torch.", file=sys.stderr); sys.exit(2)

COLS = ["mid","spread","imbalance","mom1","mom3","vol3"]

def _extract_layers_from_state_dict(sd: "OrderedDict"):
    pairs = {}
    for k in sd.keys():
        if k.endswith(".weight"):
            pre = k[:-7]; bk = pre + ".bias"
            if bk in sd: pairs[pre] = (k, bk)
    def get_idx(name: str) -> int:
        ms = list(re.finditer(r'(\d+)', name))
        return int(ms[-1].group(1)) if ms else 10**9
    items = [(get_idx(pre), pre, wk, bk) for pre,(wk,bk) in pairs.items()]
    items.sort(key=lambda x: (x[0], x[1]))
    layers=[]
    for _, pre, wk, bk in items:
        W = sd[wk].detach().clone().float()
        B = sd[bk].detach().clone().float()
        layers.append((W, B))
    return layers

class StateDictMLPRunner(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.W = nn.ParameterList([nn.Parameter(W) for (W,B) in layers])
        self.B = nn.ParameterList([nn.Parameter(B) for (W,B) in layers])
        self.activ_last_tanh = (self.W[-1].shape[0] == 1)
        self.eval()
    def forward(self, x):
        h = x
        for i in range(len(self.W)):
            h = torch.addmm(self.B[i], h, self.W[i].t())
            if i < len(self.W)-1: h = torch.relu(h)
            else:
                if self.activ_last_tanh: h = torch.tanh(h)
        return h

def load_flexible_model(path: str, device: str):
    # TorchScript
    try:
        m = torch.jit.load(path, map_location=device)
        m = m.float(); m.eval(); return m, "jit"
    except Exception:
        pass
    # torch.load
    obj = torch.load(path, map_location=device)
    if isinstance(obj, nn.Module):
        obj = obj.float(); obj.eval(); return obj, "module"
    if isinstance(obj, dict):
        sd = obj.get("state_dict", None) or obj
        if isinstance(sd, dict):
            layers = _extract_layers_from_state_dict(sd)
            if not layers: raise RuntimeError("No linear layers parsed from state_dict.")
            runner = StateDictMLPRunner(layers).to(device).float()
            return runner, "state_dict"
    raise RuntimeError(f"Unsupported checkpoint format: {path}")

@torch.no_grad()
def model_forward(model, x):
    out = model(x)
    if isinstance(out, (tuple,list)) and len(out) >= 2:
        return out[0], out[1]
    return out, None

def load_scaler(path: str):
    with open(path, "r") as f:
        S = json.load(f)
    if S.get("type") != "zscore": raise SystemExit("scaler.json type must be 'zscore'")
    cols = S["cols"]
    if [c.lower() for c in cols] != COLS: raise SystemExit(f"scaler cols mismatch: {cols}")
    mean = np.asarray(S["mean"], dtype=np.float32)
    std  = np.asarray(S["std"], dtype=np.float32)
    std  = np.where(std < 1e-8, 1.0, std).astype(np.float32)
    return mean, std

def consensus_value(actions: np.ndarray, agree_eps: float, act_limit: float, min_agree_k: int):
    # pick the largest cluster where |ai - aj| <= agree_eps*act_limit
    if actions.size == 0: return False, 0.0, 0.0
    tol = agree_eps * act_limit
    acts = np.sort(actions)
    best_idx = (0,1)  # [start, end) window
    i = 0
    for j in range(1, len(acts)+1):
        while acts[j-1] - acts[i] > tol:
            i += 1
        if j - i > best_idx[1] - best_idx[0]:
            best_idx = (i, j)
    cluster = acts[best_idx[0]:best_idx[1]]
    ok = (len(cluster) >= min_agree_k)
    mean_a = float(np.mean(cluster)) if ok else 0.0
    mean_a = max(-act_limit, min(act_limit, mean_a))
    maxdiff = float(np.max(np.abs(actions[:,None]-actions[None,:]))) if len(actions)>1 else 0.0
    return ok, mean_a, maxdiff

def consensus_sign(actions: np.ndarray, min_agree_k: int, act_limit: float, min_mag: float = 0.0):
    if actions.size == 0: return False, 0.0, 0.0
    pos = actions[actions >= +min_mag]
    neg = actions[actions <= -min_mag]
    # choose larger agreeing set
    cluster = pos if len(pos) >= len(neg) else neg
    ok = (len(cluster) >= min_agree_k)
    mean_a = float(np.mean(cluster)) if ok else 0.0
    mean_a = max(-act_limit, min(act_limit, mean_a))
    maxdiff = float(np.max(np.abs(actions[:,None]-actions[None,:]))) if len(actions)>1 else 0.0
    return ok, mean_a, maxdiff

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--gate", required=True)
    ap.add_argument("--scaler", required=True)
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--state_csv")
    g.add_argument("--state_row")
    ap.add_argument("--device", default="cpu", choices=["cpu","cuda","mps"])
    ap.add_argument("--out_csv", default=None)

    # consensus controls
    ap.add_argument("--consensus-mode", default="value", choices=["value","sign"],
                    help="value: pairwise diff within eps; sign: same sign with optional min magnitude")
    ap.add_argument("--min-agree-k", type=int, default=None, help="require at least k actors to agree (default: all actors)")
    ap.add_argument("--min-mag", type=float, default=0.0, help="sign mode: minimum |action| to count as a vote")

    # overrides (optional; if omitted, read from JSON)
    ap.add_argument("--agree-eps", type=float, default=None, help="override agree_eps")
    ap.add_argument("--act-limit", type=float, default=None, help="override act_limit")

    args = ap.parse_args()

    with open(args.gate,"r") as f: gate = json.load(f)
    if gate.get("type") != "td3bc_consensus_gate":
        raise SystemExit("gate.type must be td3bc_consensus_gate")
    actors: List[str] = gate["actors"]
    agree_eps = float(args.agree_eps if args.agree_eps is not None else gate["agree_eps"])
    act_limit = float(args.act_limit if args.act_limit is not None else gate.get("act_limit", 1.0))
    q_min_thresh: float = float(gate.get("q_min_thresh", -1e9))  # ignored if no Q
    min_agree_k = int(args.min_agree_k) if args.min_agree_k is not None else len(actors)

    mu, sd = load_scaler(args.scaler)  # float32

    # states
    X=[]
    if args.state_csv:
        with open(args.state_csv, newline="") as f:
            rdr = csv.DictReader(f)
            hdr = [h.strip().lower() for h in (rdr.fieldnames or [])]
            if hdr != COLS: raise SystemExit(f"state_csv header mismatch: got {rdr.fieldnames}, want {COLS}")
            for row in rdr:
                try:
                    X.append([float(row[c]) for c in COLS])
                except: pass
    else:
        vals = [float(x.strip()) for x in args.state_row.split(",")]
        if len(vals) != 6: raise SystemExit("state_row must have 6 comma-separated values")
        X.append(vals)
    if not X: raise SystemExit("no usable rows")

    X = np.asarray(X, dtype=np.float32)
    Xn = ((X - mu) / sd).astype(np.float32)

    device = torch.device(args.device if (args.device != "cuda" or torch.cuda.is_available()) else "cpu")
    models=[]; kinds=[]
    for a in actors:
        if not os.path.isfile(a): raise SystemExit(f"actor not found: {a}")
        m, k = load_flexible_model(a, device); models.append(m); kinds.append(k)
    print("[info] loaded actors:", list(zip(actors, kinds)))
    print(f"[info] mode={args.consensus_mode} agree_eps={agree_eps} act_limit={act_limit} min_agree_k={min_agree_k} min_mag={args.min_mag}")

    # infer
    T = Xn.shape[0]; out=[]
    print("row_idx,agree,action,max_pair_diff,min_q")
    for i in range(T):
        s = torch.from_numpy(Xn[i:i+1].astype(np.float32)).to(device).to(torch.float32)
        acts=[]; qs=[]
        for m in models:
            a, q = model_forward(m, s)
            a_np = a.detach().cpu().numpy().astype(np.float32).reshape(-1)
            acts.append(float(a_np[0]) if a_np.size>0 else 0.0)
            if q is not None:
                q_np = q.detach().cpu().numpy().astype(np.float32).reshape(-1)
                if q_np.size>0: qs.append(float(q_np[0]))
        actions = np.array(acts, dtype=np.float32)

        if args.consensus_mode == "value":
            ok, mean_a, maxdiff = consensus_value(actions, agree_eps, act_limit, min_agree_k)
        else:
            ok, mean_a, maxdiff = consensus_sign(actions, min_agree_k, act_limit, args.min_mag)

        min_q = float(np.min(qs)) if qs else None
        q_ok = (min_q is None) or (min_q >= q_min_thresh)
        ok = ok and q_ok
        action_out = mean_a if ok else 0.0
        print(f"{i},{int(ok)},{action_out},{maxdiff},{min_q}")
        out.append((i,int(ok),action_out,min_q))

    if args.out_csv:
        with open(args.out_csv, "w", newline="") as f:
            w = csv.writer(f); w.writerow(["row_idx","agree","action","min_q"])
            for i,ok,a,mq in out: w.writerow([i,ok,a,mq])

if __name__ == "__main__":
    main()

=== ./peek_parquet.py ===

#!/usr/bin/env python3
# peek_parquet.py
# Minimal, version-safe Parquet inspector using DuckDB.
# - Lists how many files match your glob and prints a few example paths
# - Prints the unified schema (column name + DuckDB type)
# - Prints the first N rows (all columns) so we can see real values
# Nothing else. No guesses. No transformations.

import argparse
import sys
import duckdb

def sql_lit(path: str) -> str:
    # Safe single-quoted SQL literal
    return "'" + path.replace("'", "''") + "'"

def main():
    ap = argparse.ArgumentParser(description="Minimal Parquet inspector (DuckDB).")
    ap.add_argument("--parquet", required=True, help='Recursive glob, e.g. /path/**/*.parquet')
    ap.add_argument("--limit", type=int, default=10, help="Number of preview rows to print (default: 10)")
    ap.add_argument("--files", type=int, default=10, help="Show up to this many example file paths (default: 10)")
    args = ap.parse_args()

    con = duckdb.connect(database=":memory:")

    # 1) Count files and show a few paths
    try:
        q_files = f"SELECT DISTINCT filename FROM read_parquet({sql_lit(args.parquet)}, filename=true)"
        files = [r[0] for r in con.execute(q_files).fetchall()]
    except Exception as e:
        print(f"[error] Could not enumerate files for pattern: {args.parquet}")
        print(f"        {e}", file=sys.stderr)
        sys.exit(2)

    if not files:
        print(f"[info] No files matched pattern: {args.parquet}")
        sys.exit(0)

    print(f"[ok] Matched files: {len(files)}")
    for p in files[:args.files]:
        print("  -", p)
    if len(files) > args.files:
        print(f"  ... (+{len(files)-args.files} more)")

    # 2) Unified schema across the match
    try:
        q_schema = f"DESCRIBE SELECT * FROM read_parquet({sql_lit(args.parquet)})"
        schema_rows = con.execute(q_schema).fetchall()
    except Exception as e:
        print(f"[error] Could not DESCRIBE schema: {e}", file=sys.stderr)
        sys.exit(3)

    print("\n[Schema]")
    # DuckDB DESCRIBE returns: name, type, null, key, default, extra
    # We'll print name and type.
    for row in schema_rows:
        col_name, col_type = row[0], row[1]
        print(f"  {col_name}: {col_type}")

    # 3) Preview first N rows (all columns)
    try:
        q_preview = f"SELECT * FROM read_parquet({sql_lit(args.parquet)}) LIMIT {int(args.limit)}"
        df = con.execute(q_preview).fetchdf()
    except Exception as e:
        print(f"\n[error] Could not fetch preview rows: {e}", file=sys.stderr)
        sys.exit(4)

    print(f"\n[Preview: first {len(df)} row(s)]")
    # Pretty print as CSV-ish for easy eyeballing
    # (Avoid Pandas repr width truncation)
    if df.empty:
        print("  (no rows)")
        sys.exit(0)

    # Print header
    cols = list(df.columns)
    print(",".join(str(c) for c in cols))
    # Print rows
    for _, r in df.iterrows():
        vals = []
        for c in cols:
            v = r[c]
            # Keep it simple; stringify None/NaN clearly
            if v is None:
                vals.append("NULL")
            else:
                s = str(v)
                # replace newlines/commas to keep one-line per row readable
                s = s.replace("\n", " ").replace(",", ";")
                vals.append(s)
        print(",".join(vals))

if __name__ == "__main__":
    main()

=== ./baseline_random_sign.py ===

#!/usr/bin/env python3
import argparse, csv, random, time, subprocess, sys, os, tempfile

ap = argparse.ArgumentParser()
ap.add_argument("--states", default="states.csv")
ap.add_argument("--H", type=int, default=40)
ap.add_argument("--fee-bps", type=float, default=5.0)
ap.add_argument("--hold-bars", type=int, default=8)
args = ap.parse_args()

tmp = tempfile.NamedTemporaryFile("w", delete=False, suffix=".csv")
tmp.write("row_idx,agree,action,details\n")
with open(args.states) as f:
    n = sum(1 for _ in f) - 1
for i in range(n):
    a = random.choice([-1.0, 0.0, 1.0])
    tmp.write(f"{i},1,{a},{{}}\n")
tmp.close()

cmd = [sys.executable, "backtest_gate_horizon_v3.py",
       "--states", args.states, "--gate", tmp.name,
       "--h", str(args.H), "--mode", "sign",
       "--action_thresh", "0.05",
       "--hold_bars", str(args.hold_bars),
       "--fee_bps", str(args.fee_bps)]
out = subprocess.check_output(cmd, text=True)
print(out)
os.unlink(tmp.name)

=== ./emit_datalayer_from_ob_json.py ===

#!/usr/bin/env python3
# Reads ob_mapping.json and writes adapters/mexc15s/rl_duckdb_data_layer_custom.py
# No overwrites of your originals. No nested .format() traps.

import json, os, sys

OUT_DIR = os.path.join("adapters", "mexc15s")
OUT_PY  = os.path.join(OUT_DIR, "rl_duckdb_data_layer_custom.py")

TEMPLATE = r'''#!/usr/bin/env python3
# DO NOT EDIT: generated from __JSON_PATH__
# Custom, non-destructive data layer (keeps originals intact).

import argparse
from typing import List, Optional, Dict
import duckdb, numpy as np, pandas as pd

TS_EXPR   = r"""__TS_EXPR__"""
BID_PX_EX = r"""__BID_PX_EX__"""
ASK_PX_EX = r"""__ASK_PX_EX__"""
BID_SZ_EX = r"""__BID_SZ_EX__"""
ASK_SZ_EX = r"""__ASK_SZ_EX__"""

class ReplayBuffer:
    def __init__(self, capacity:int, state_dim:int, act_dim:int=0):
        self.capacity=int(capacity); self.state_dim=int(state_dim); self.act_dim=int(act_dim)
        self.ptr=0; self.full=False
        self.s=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.sp=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.r=np.zeros((self.capacity,1),dtype=np.float32)
        self.d=np.zeros((self.capacity,1),dtype=np.float32)
        self.a=None
        if self.act_dim>0: self.a=np.zeros((self.capacity,self.act_dim),dtype=np.float32)
    def add(self,s,a,r,sp,d):
        i=self.ptr; self.s[i]=s; self.sp[i]=sp; self.r[i]=r; self.d[i]=d
        if self.a is not None and a is not None: self.a[i]=a
        self.ptr=(self.ptr+1)%self.capacity; self.full = self.full or (self.ptr==0)
    def size(self): return self.capacity if self.full else self.ptr
    def sample(self,batch_size:int,rng:np.random.Generator)->Dict[str,np.ndarray]:
        n=self.size(); idx=rng.integers(0,n,size=batch_size)
        out={"s":self.s[idx],"r":self.r[idx],"sp":self.sp[idx],"d":self.d[idx]}
        if self.a is not None: out["a"]=self.a[idx]
        return out

def build_sql(parquet_glob:str,start_ts:Optional[str],end_ts:Optional[str],horizon_rows:int,ret_threshold:float,extra_filters:Optional[str]=None)->str:
    where=[]
    if start_ts: where.append(f"ts_parsed >= TIMESTAMP '{start_ts}'")
    if end_ts:   where.append(f"ts_parsed <= TIMESTAMP '{end_ts}'")
    if extra_filters: where.append(f"({extra_filters})")
    where_clause = "WHERE " + " AND ".join(where) if where else ""
    sql = f"""
WITH raw AS (
  SELECT * FROM read_parquet('{parquet_glob}', filename=true)
),
proj AS (
  SELECT
    {TS_EXPR}   AS ts_parsed,
    {BID_PX_EX} AS bid_px_d,
    {ASK_PX_EX} AS ask_px_d,
    {BID_SZ_EX} AS bid_sz_d,
    {ASK_SZ_EX} AS ask_sz_d
  FROM raw
),
src AS ( SELECT * FROM proj {where_clause} ),
ord AS ( SELECT * FROM src ORDER BY ts_parsed ),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px_d + ask_px_d)*0.5 AS mid,
    (ask_px_d - bid_px_d)     AS spread,
    CASE WHEN (bid_sz_d + ask_sz_d)>0
         THEN (bid_sz_d - ask_sz_d)/NULLIF(bid_sz_d + ask_sz_d,0)
         ELSE 0 END           AS imbalance,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,1) OVER ()) AS mom1,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px_d + ask_px_d)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
),
lab AS (
  SELECT
    *,
    LEAD(mid, {horizon_rows}) OVER () AS mid_fwd,
    (LEAD(mid, {horizon_rows}) OVER () - mid) / NULLIF(mid,0) AS fwd_ret
  FROM feat
),
final AS (
  SELECT
    ts,
    mid, spread, imbalance, mom1, mom3, vol3,
    LEAD(mid, {horizon_rows}) OVER ()        AS mid_p,
    LEAD(spread, {horizon_rows}) OVER ()     AS spread_p,
    LEAD(imbalance, {horizon_rows}) OVER ()  AS imbalance_p,
    LEAD(mom1, {horizon_rows}) OVER ()       AS mom1_p,
    LEAD(mom3, {horizon_rows}) OVER ()       AS mom3_p,
    LEAD(vol3, {horizon_rows}) OVER ()       AS vol3_p,
    fwd_ret,
    CASE WHEN fwd_ret > {ret_threshold} THEN 1
         WHEN fwd_ret < -{ret_threshold} THEN -1
         ELSE 0 END AS label
  FROM lab
)
SELECT * FROM final
WHERE mid IS NOT NULL AND mid_p IS NOT NULL
"""
    return sql

def _ensure_float(df: pd.DataFrame, cols: List[str])->pd.DataFrame:
    for c in cols:
        if c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    return df

def stream_into_buffer(con:duckdb.DuckDBPyConnection, sql:str, buffer:ReplayBuffer,
                       chunk_rows:int=200_000, drop_noise_label:bool=False, derive_action:bool=False)->int:
    total=0; cur=con.execute(sql)
    state_cols=["mid","spread","imbalance","mom1","mom3","vol3"]
    next_cols=["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
    while True:
        df=cur.fetch_df_chunk(chunk_rows)
        if df is None or len(df)==0: break
        df=_ensure_float(df, list(df.columns))
        if drop_noise_label and "label" in df.columns: df=df[df["label"]!=0]
        df=df.dropna(subset=state_cols+next_cols+["fwd_ret"])
        S=df[state_cols].to_numpy(np.float32)
        SP=df[next_cols].to_numpy(np.float32)
        R=df[["fwd_ret"]].to_numpy(np.float32)
        D=np.zeros((len(df),1),dtype=np.float32)
        if derive_action:
            A=df[["label"]].to_numpy(np.float32) if "label" in df.columns else np.sign(R).astype(np.float32)
        else:
            A=None
        for i in range(len(df)):
            buffer.add(S[i], (A[i] if A is not None else None), R[i], SP[i], D[i])
        total+=len(df)
    return total

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    args=ap.parse_args()
    con=duckdb.connect(database=":memory:")
    sql=build_sql(args.parquet,args.start,args.end,args.h,args.th,None)
    plan=con.execute(f"EXPLAIN {sql}").fetchall()
    print("[DuckDB plan]"); [print(" ",r[0]) for r in plan]
'''

def main():
    jpath = "ob_mapping.json" if len(sys.argv) <= 1 else sys.argv[1]
    with open(jpath,"r") as f:
        data = json.load(f)

    ex = data.get("recommended_expressions") or data.get("expressions") or {}
    TS = ex.get("TS_EXPR")
    BP = ex.get("BID_PX_EX")
    AP = ex.get("ASK_PX_EX")
    BS = ex.get("BID_SZ_EX")
    AS = ex.get("ASK_SZ_EX")
    miss = [k for k,v in [("TS_EXPR",TS),("BID_PX_EX",BP),("ASK_PX_EX",AP),("BID_SZ_EX",BS),("ASK_SZ_EX",AS)] if not v]
    if miss:
        print(f"[error] Missing keys in {jpath}: {miss}")
        sys.exit(2)

    os.makedirs(OUT_DIR, exist_ok=True)

    code = TEMPLATE.replace("__JSON_PATH__", os.path.abspath(jpath))\
                   .replace("__TS_EXPR__", TS)\
                   .replace("__BID_PX_EX__", BP)\
                   .replace("__ASK_PX_EX__", AP)\
                   .replace("__BID_SZ_EX__", BS)\
                   .replace("__ASK_SZ_EX__", AS)

    with open(OUT_PY, "w") as f:
        f.write(code)
    print(f"[ok] wrote {OUT_PY}")

if __name__=="__main__":
    main()

#!/usr/bin/env python3
"""
inspect_parquet_orderbook.py

One-shot scanner for L2/L1 orderbook Parquet trees.

- Recursively scans a Parquet glob (supports **/*.parquet)
- Reports schema and candidate columns for:
    * timestamp
    * bid/ask price
    * bid/ask size
- Builds robust, ready-to-use DuckDB expressions:
    * TS_EXPR (ISO string or epoch-ms auto-detect)
    * BID_PX_EX, ASK_PX_EX, BID_SZ_EX, ASK_SZ_EX
- Writes a JSON report: ob_mapping.json
- (Optional) emits a tailored rl_duckdb_data_layer.py when --emit-datalayer is given

Usage:
  python3 inspect_parquet_orderbook.py \
    --parquet "/abs/path/**/*.parquet" \
    --out ob_mapping.json \
    [--emit-datalayer rl_duckdb_data_layer.py]
"""

import argparse, json, os, re, sys
from typing import List, Tuple
import duckdb

# -------- helpers (version-safe) --------
def sql_lit(s: str) -> str:
    return "'" + s.replace("'", "''") + "'"

def qident(name: str) -> str:
    return '"' + name.replace('"','""') + '"'

def describe_schema(con: duckdb.DuckDBPyConnection, glob: str) -> List[Tuple[str,str]]:
    q = f"DESCRIBE SELECT * FROM read_parquet({sql_lit(glob)})"
    rows = con.execute(q).fetchall()
    return [(r[0], r[1]) for r in rows]

def list_files(con: duckdb.DuckDBPyConnection, glob: str) -> List[str]:
    # Use filename=true to expose the 'filename' pseudo-column
    q = f"SELECT DISTINCT filename FROM read_parquet({sql_lit(glob)}, filename=true)"
    try:
        return [r[0] for r in con.execute(q).fetchall()]
    except Exception:
        return []

def to_level(name: str):
    m = re.search(r'_(\d+)$', name)
    return int(m.group(1)) if m else None

def pick_candidates(cols: List[Tuple[str,str]], side: str, kind: str) -> List[str]:
    """
    side in {'bid','ask'}, kind in {'px','sz'}
    robust heuristics for names like: bid_px_0, best_ask_price, ask_qty_1, etc.
    """
    names = [c[0] for c in cols]
    cands = []
    for n in names:
        ln = n.lower()
        if side not in ln:
            continue
        if kind == "px":
            if ("px" in ln) or ("price" in ln) or re.search(r'(^|_)(p|prc|price)($|_)', ln):
                cands.append(n)
        else:  # sz
            if ("sz" in ln) or ("size" in ln) or ("qty" in ln) or ("quantity" in ln) or re.search(r'(^|_)(q|qty|size)($|_)', ln):
                cands.append(n)
    # prefer *_0, then ascending level, then name
    cands = list(set(cands))
    cands.sort(key=lambda x: (999999 if to_level(x) is None else to_level(x), x))
    return cands

def ts_candidates(cols: List[Tuple[str,str]]) -> List[str]:
    out = []
    for c,_ in cols:
        lc = c.lower()
        if ("ts" in lc) or ("time" in lc) or ("timestamp" in lc) or ("datetime" in lc):
            out.append(c)
    # unique but preserve order
    seen=set(); uniq=[]
    for c in out:
        if c not in seen:
            seen.add(c); uniq.append(c)
    return uniq or ["ts"]

def build_ts_expr(ts_cands: List[str]) -> str:
    parts = []
    for c in ts_cands:
        ci = qident(c)
        parts.append(f"TRY_CAST({ci} AS TIMESTAMP)")
    for c in ts_cands:
        ci = qident(c)
        parts.append(f"to_timestamp(CAST(TRY_CAST({ci} AS BIGINT) AS DOUBLE)/1000.0)")
    # common epoch-ms fallbacks
    for c in ["ts_ms","timestamp_ms","time_ms","epoch_ms","t_ms"]:
        parts.append(f"to_timestamp(CAST(TRY_CAST({c} AS BIGINT) AS DOUBLE)/1000.0)")
    # last resort: 'ts' cast via DOUBLE->BIGINT
    parts.append("to_timestamp(CAST(TRY_CAST(CAST(ts AS DOUBLE) AS BIGINT) AS DOUBLE)/1000.0)")
    return "COALESCE(" + ", ".join(parts) + ")"

def build_num_expr(cands: List[str], default: str=None) -> str:
    parts = [f"CAST({qident(c)} AS DOUBLE)" for c in cands]
    if default is not None:
        parts.append(default)
    if not parts:
        return "NULL"
    return "COALESCE(" + ", ".join(parts) + ")"

TAILORED_DATALAYER_TMPL = """#!/usr/bin/env python3
# Auto-generated by inspect_parquet_orderbook.py
# Tailored top-of-book mapping for your Parquet schema.

import argparse
from typing import List, Optional, Dict
import duckdb, numpy as np, pandas as pd

TS_EXPR   = \"\"\"{TS_EXPR}\"\"\"
BID_PX_EX = \"\"\"{BID_PX_EX}\"\"\"
ASK_PX_EX = \"\"\"{ASK_PX_EX}\"\"\"
BID_SZ_EX = \"\"\"{BID_SZ_EX}\"\"\"
ASK_SZ_EX = \"\"\"{ASK_SZ_EX}\"\"\"

class ReplayBuffer:
    def __init__(self, capacity:int, state_dim:int, act_dim:int=0):
        self.capacity=int(capacity); self.state_dim=int(state_dim); self.act_dim=int(act_dim)
        self.ptr=0; self.full=False
        self.s=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.sp=np.zeros((self.capacity,self.state_dim),dtype=np.float32)
        self.r=np.zeros((self.capacity,1),dtype=np.float32)
        self.d=np.zeros((self.capacity,1),dtype=np.float32)
        self.a=None
        if self.act_dim>0: self.a=np.zeros((self.capacity,self.act_dim),dtype=np.float32)
    def add(self,s,a,r,sp,d):
        i=self.ptr; self.s[i]=s; self.sp[i]=sp; self.r[i]=r; self.d[i]=d
        if self.a is not None and a is not None: self.a[i]=a
        self.ptr=(self.ptr+1)%self.capacity; self.full = self.full or (self.ptr==0)
    def size(self): return self.capacity if self.full else self.ptr
    def sample(self,batch_size:int,rng:np.random.Generator)->Dict[str,np.ndarray]:
        n=self.size(); idx=rng.integers(0,n,size=batch_size)
        out={{"s":self.s[idx],"r":self.r[idx],"sp":self.sp[idx],"d":self.d[idx]}}
        if self.a is not None: out["a"]=self.a[idx]
        return out

def build_sql(parquet_glob:str,start_ts:Optional[str],end_ts:Optional[str],horizon_rows:int,ret_threshold:float,extra_filters:Optional[str]=None)->str:
    where=[]
    if start_ts: where.append(f"ts_parsed >= TIMESTAMP '{{start_ts}}'")
    if end_ts:   where.append(f"ts_parsed <= TIMESTAMP '{{end_ts}}'")
    if extra_filters: where.append(f"({{extra_filters}})")
    where_clause = "WHERE " + " AND ".join(where) if where else ""
    sql = f\"\"\"
WITH raw AS (
  SELECT * FROM read_parquet('{{parquet_glob}}', filename=true)
),
proj AS (
  SELECT
    {{TS}} AS ts_parsed,
    {{BP}} AS bid_px_d,
    {{AP}} AS ask_px_d,
    {{BS}} AS bid_sz_d,
    {{AS}} AS ask_sz_d
  FROM raw
),
src AS ( SELECT * FROM proj {{where_clause}} ),
ord AS ( SELECT * FROM src ORDER BY ts_parsed ),
feat AS (
  SELECT
    ts_parsed AS ts,
    (bid_px_d + ask_px_d)*0.5 AS mid,
    (ask_px_d - bid_px_d)     AS spread,
    CASE WHEN (bid_sz_d + ask_sz_d)>0
         THEN (bid_sz_d - ask_sz_d)/NULLIF(bid_sz_d + ask_sz_d,0)
         ELSE 0 END           AS imbalance,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,1) OVER ()) AS mom1,
    ((bid_px_d + ask_px_d)*0.5 - LAG((bid_px_d + ask_px_d)*0.5,3) OVER ()) AS mom3,
    STDDEV_SAMP((bid_px_d + ask_px_d)*0.5) OVER (ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS vol3
  FROM ord
),
lab AS (
  SELECT
    *,
    LEAD(mid, {{horizon_rows}}) OVER () AS mid_fwd,
    (LEAD(mid, {{horizon_rows}}) OVER () - mid) / NULLIF(mid,0) AS fwd_ret
  FROM feat
),
final AS (
  SELECT
    ts,
    mid, spread, imbalance, mom1, mom3, vol3,
    LEAD(mid, {{horizon_rows}}) OVER ()        AS mid_p,
    LEAD(spread, {{horizon_rows}}) OVER ()     AS spread_p,
    LEAD(imbalance, {{horizon_rows}}) OVER ()  AS imbalance_p,
    LEAD(mom1, {{horizon_rows}}) OVER ()       AS mom1_p,
    LEAD(mom3, {{horizon_rows}}) OVER ()       AS mom3_p,
    LEAD(vol3, {{horizon_rows}}) OVER ()       AS vol3_p,
    fwd_ret,
    CASE WHEN fwd_ret > {{ret_threshold}} THEN 1
         WHEN fwd_ret < -{{ret_threshold}} THEN -1
         ELSE 0 END AS label
  FROM lab
)
SELECT * FROM final
WHERE mid IS NOT NULL AND mid_p IS NOT NULL
\"\"\".format(parquet_glob=parquet_glob,start_ts=start_ts,end_ts=end_ts,extra_filters=extra_filters or "",
             horizon_rows=horizon_rows,ret_threshold=ret_threshold,where_clause=where_clause)\
             .replace("{{TS}}", TS_EXPR)\
             .replace("{{BP}}", BID_PX_EX)\
             .replace("{{AP}}", ASK_PX_EX)\
             .replace("{{BS}}", BID_SZ_EX)\
             .replace("{{AS}}", ASK_SZ_EX)
    return sql

def _ensure_float(df: pd.DataFrame, cols: List[str])->pd.DataFrame:
    for c in cols:
        if c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    return df

def stream_into_buffer(con:duckdb.DuckDBPyConnection, sql:str, buffer:ReplayBuffer,
                       chunk_rows:int=200_000, drop_noise_label:bool=False, derive_action:bool=False)->int:
    total=0; cur=con.execute(sql)
    state_cols=["mid","spread","imbalance","mom1","mom3","vol3"]
    next_cols=["mid_p","spread_p","imbalance_p","mom1_p","mom3_p","vol3_p"]
    while True:
        df=cur.fetch_df_chunk(chunk_rows)
        if df is None or len(df)==0: break
        df=_ensure_float(df, list(df.columns))
        if drop_noise_label and "label" in df.columns: df=df[df["label"]!=0]
        df=df.dropna(subset=state_cols+next_cols+["fwd_ret"])
        S=df[state_cols].to_numpy(np.float32)
        SP=df[next_cols].to_numpy(np.float32)
        R=df[["fwd_ret"]].to_numpy(np.float32)
        D=np.zeros((len(df),1),dtype=np.float32)
        if derive_action:
            A=df[["label"]].to_numpy(np.float32) if "label" in df.columns else np.sign(R).astype(np.float32)
        else:
            A=None
        for i in range(len(df)):
            buffer.add(S[i], (A[i] if A is not None else None), R[i], SP[i], D[i])
        total+=len(df)
    return total

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True)
    ap.add_argument("--start", default=None)
    ap.add_argument("--end", default=None)
    ap.add_argument("--h", type=int, default=20)
    ap.add_argument("--th", type=float, default=0.0002)
    args=ap.parse_args()
    con=duckdb.connect(database=":memory:")
    sql=build_sql(args.parquet,args.start,args.end,args.h,args.th,None)
    plan=con.execute(f"EXPLAIN {sql}").fetchall()
    print("[DuckDB plan]"); [print(" ",r[0]) for r in plan]
"""

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Recursive glob, e.g. /path/**/*.parquet")
    ap.add_argument("--out", default="ob_mapping.json", help="Report JSON out path")
    ap.add_argument("--emit-datalayer", default=None, help="Optional: write tailored rl_duckdb_data_layer.py here")
    args = ap.parse_args()

    con = duckdb.connect()

    # gather info
    cols = describe_schema(con, args.parquet)
    if not cols:
        print("No columns found. Check your --parquet pattern.", file=sys.stderr)
        sys.exit(2)

    files = list_files(con, args.parquet)

    # detect candidates
    ts_cands = ts_candidates(cols)
    bid_px = pick_candidates(cols, "bid", "px")
    ask_px = pick_candidates(cols, "ask", "px")
    bid_sz = pick_candidates(cols, "bid", "sz")
    ask_sz = pick_candidates(cols, "ask", "sz")

    # build expressions
    TS_EXPR   = build_ts_expr(ts_cands)
    BID_PX_EX = build_num_expr(bid_px)
    ASK_PX_EX = build_num_expr(ask_px)
    BID_SZ_EX = build_num_expr(bid_sz, default="1.0")
    ASK_SZ_EX = build_num_expr(ask_sz, default="1.0")

    report = {
        "parquet_glob": args.parquet,
        "file_count_seen": len(files),
        "example_files": files[:10],
        "schema": [{ "name": n, "type": t } for n,t in cols],
        "detected": {
            "ts_candidates": ts_cands,
            "bid_px_candidates": bid_px,
            "ask_px_candidates": ask_px,
            "bid_sz_candidates": bid_sz,
            "ask_sz_candidates": ask_sz
        },
        "recommended_expressions": {
            "TS_EXPR": TS_EXPR,
            "BID_PX_EX": BID_PX_EX,
            "ASK_PX_EX": ASK_PX_EX,
            "BID_SZ_EX": BID_SZ_EX,
            "ASK_SZ_EX": ASK_SZ_EX
        }
    }

    with open(args.out, "w") as f:
        json.dump(report, f, indent=2)
    print(f"[ok] wrote mapping report: {os.path.abspath(args.out)}")

    if args.emit_datalayer:
        code = TAILORED_DATALAYER_TMPL.format(
            TS_EXPR=TS_EXPR,
            BID_PX_EX=BID_PX_EX,
            ASK_PX_EX=ASK_PX_EX,
            BID_SZ_EX=BID_SZ_EX,
            ASK_SZ_EX=ASK_SZ_EX
        )
        with open(args.emit_datalayer, "w") as f:
            f.write(code)
        print(f"[ok] wrote tailored data layer: {os.path.abspath(args.emit_datalayer)}")

if __name__ == "__main__":
    main()
